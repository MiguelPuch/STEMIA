{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "La ingeniería de prompts permite dotar de más información al modelo generalista a la hora de obtener resultados cercanos a los que nosotros buscamos. Aquí tenéis una buena referencia: https://www.promptingguide.ai/es\n",
    "\n",
    "Dado que poco a poco iremos evolucionando nuestros prompts creando plantillas y dependencias, es bueno que nos hagamos eco de algunos frameworks que nos permitirán desarrollar estas piezas técnicas de forma algo más fácil y con opción de variar entre distintos modelos (LLMs).\n",
    "\n",
    "# LangChain\n",
    "\n",
    "[LangChain]() es un framework pensado para realizar este tipo de tarea, el desarrollo a bajo nivel de las interacciones con las LLMs. De cara a que no genere coste ni requiera cuenta de cobro podréis utilizar la API fe Gemini para estos ejercicios:\n",
    "\n",
    "- Obtén tu clave de API de Gemini en [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
    "- Guarda la clave en una variable `GEMINI_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de instanciar el modelo podemos determinar múltiples aspectos sobre cómo debe actuar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que completa la respuesta con un mensaje del tipo `AIMessage`. Ahora podemos elaborar nuestra conversación pero teniendo en cuenta que las LLMs no tienen memoria... toda la información les ha de ser provista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es lo que nos hace ver la importancia de proveer de toda la información necesaria cuando interactuamos con una LLM. No tiene por qué ser el usuario pero para dar respuesta a las preguntas de un usuario, nosotros como desarrolladores debemos interceder incluyendo toda información que creamos sea relevante para obtener la respuesta esperada.\n",
    "\n",
    "## Contexto\n",
    "\n",
    "A eso nos referimos con el contexto y quizás una de las instrucciones más básicas sean las instrucciones del sistema. Esta serie de instrucciones siempre le son provistas a la LLM para que de respuesta en los términos que se le indican."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el coste efectivo de nuestra consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que empleamos a menudo estos esquemas, LangChain ya dispone de plantillas que nos ayudan a tener estructuradas las conversaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y simplemente podemos usar nuestra platilla para invocar al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a lo que hemos comentado anteriormente, es habitual encontrar plantillas que incluyen la información necesaria de cara a responder las dudas de los usuarios o ofrecer ejemplos de cómo responder. Cuando la interacción es directa, como en los casos anteriores este modelo se conoce como **zero-shot prompting** mientras que si ofrecemos ejemplos de cómo resolverlo, esta técnica conocida como **in-context learning** se modelo como **few-shot prompting** ofreciendo varios ejemplos de lo que vayamos a resolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen múltiples esquemas de cómo podemos articular estas instrucciones pero debemos ante todo contemplar que el modelo no tiene memoria y su conocimiento está acotado, de forma que deberemos informar en cada interacción de:\n",
    "\n",
    "* **Rol** del modelo. Idioma y consideraciones que debe tener a la hora de resolver la tarea.\n",
    "* **Contexto** a modo de información complementaria, ejemplo o forma en la que proceder para resolver la petición.\n",
    "* **Instrucción** o tarea que vaya a realizar (resume este texto, genera la documentación de este código, etc.)\n",
    "* **Formato** o manera en que la respuesta será provista (texto plano, formato concreto - JSON, HTML, ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds4b2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
