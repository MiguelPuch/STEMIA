{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6db8dd1",
   "metadata": {},
   "source": [
    "# 1. Modelos de difusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b1bae",
   "metadata": {},
   "source": [
    "Los modelos de difusión heredan de varias técnicas anteriormente vistas haciendo evolucionar una conjunto de ruido hacia una distribución conocida y acotada. Se trata de iniciar el proceso con algo aleatorio pero hacerlo converger a muestras de una distribución conocida como pudieran ser:\n",
    "\n",
    "* Dígitos escritos a mano\n",
    "* Ropa de un catalogo de moda\n",
    "\n",
    "Aunque su aplicación inicial se dió en el contexto de la generación de imágenes, han sido utilizados para la creación de textos, particularmente código, con bastante éxito. Su fortaleza está en que el modelo no intenta hacer una generación en un único paso, lo cual facilita el entrenamiento pero también permite una mejor generación de imágenes con buena resolución.\n",
    "\n",
    "Tenéis una muy buena referencia sobre esta temática en https://huggingface.co/learn/diffusion-course/unit0/1 aunque nosotros emplearemos un ejercicio simplificado que podéis extender con conceptos que van más en profundidad.\n",
    "\n",
    "## Difusión\n",
    "\n",
    "Se trata del proceso por el que podemos iniciar nuestro objeto de datos de forma aleatoria e ir regenerando una estructura sobre esa base que represente una distribución concreta.\n",
    "\n",
    "![difusión](https://user-images.githubusercontent.com/10695622/174349667-04e9e485-793b-429a-affe-096e8199ad5b.png)\n",
    "\n",
    "_Del artículo https://arxiv.org/abs/2006.11239_\n",
    "\n",
    "La base consiste en la bidireccionalidad del proceso. Podemos empezar con imágenes de alta definición y calcular la forma en la que el ruido les es añadida. Esto nos permite generar un buen predictor de ruido que aplicado de manera inversa nos permite predecir que partes de una imagen son ruido, substraerlas y por pura iteración generar finalmente imágenes definidas de la distribución de las muestras originalmente presentadas.\n",
    "\n",
    "![difusion](https://cdn.prod.website-files.com/614c82ed388d53640613982e/66acbdfb02fc228862686191_65608be75079ab1a96ef681b_reverse-sde.webp)\n",
    "\n",
    "_Referencia_ https://arxiv.org/pdf/2011.13456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff41d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d17ed7",
   "metadata": {},
   "source": [
    "Trabajaremos con el conjunto de datos del CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e1bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bdf12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cvtImg(img):\n",
    "    img = img - img.min()\n",
    "    img = (img / img.max())\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "def show_examples(x):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        img = cvtImg(x[i])\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "show_examples(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2280c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[y_train.squeeze() == 1]\n",
    "X_train = (X_train / 127.5) - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b44db",
   "metadata": {},
   "source": [
    "Existe un profeso de secuenciado de la difusión que se conoce como planificador o _scheduler_ que es el encargado de hacer los pases por los bloques de difusión, creando la estructura arriba indicada que genera imágenes nítidas a partir del ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c56b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 32     # input image size, CIFAR-10 is 32x32\n",
    "BATCH_SIZE = 128  # for training batch size\n",
    "timesteps = 16    # how many steps for a noisy image into clear\n",
    "time_bar = 1 - np.linspace(0, 1.0, timesteps + 1) # linspace for timesteps\n",
    "\n",
    "plt.plot(time_bar, label='Noise')\n",
    "plt.plot(1 - time_bar, label='Clarity')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d311467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_noise(x, t):\n",
    "    a = time_bar[t]      # base on t\n",
    "    b = time_bar[t + 1]  # image for t + 1\n",
    "    \n",
    "    noise = np.random.normal(size=x.shape)  # noise mask\n",
    "    a = a.reshape((-1, 1, 1, 1))\n",
    "    b = b.reshape((-1, 1, 1, 1))\n",
    "    img_a = x * (1 - a) + noise * a\n",
    "    img_b = x * (1 - b) + noise * b\n",
    "    return img_a, img_b\n",
    "    \n",
    "def generate_ts(num):\n",
    "    return np.random.randint(0, timesteps, size=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.full((25,), 0) # El punto de partida\n",
    "print(t)\n",
    "\n",
    "a, b = forward_noise(X_train[:25], t)\n",
    "show_examples(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.full((25,), timesteps - 1) # Damos todos los pasos, y generamos imágenes claras\n",
    "print(t)\n",
    "\n",
    "a, b = forward_noise(X_train[:25], t)\n",
    "show_examples(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = generate_ts(25)                 # intercalado\n",
    "print(t)\n",
    "\n",
    "a, b = forward_noise(X_train[:25], t)\n",
    "show_examples(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63039c1",
   "metadata": {},
   "source": [
    "Se observó que usar interpolaciones no lineales tiene sus ventajas, es quizás una de las grandes aportaciones del artículo de Stable Diffusion, y aunque el foco era la eficiencia, podemos ver cómo cambiar la forma en la que interpolamos ruido y señal también afecta al resultado:\n",
    "\n",
    "![caras](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*CTM_483g1nWQLs8TGio6UQ.png)\n",
    "\n",
    "https://arxiv.org/pdf/2112.10752"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f64f4cb",
   "metadata": {},
   "source": [
    "Con esto ahora nos toca diseñar nuestra red que tendrá una pinta similar a \n",
    "\n",
    "![Arquitectura de red](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "def block(x_img, x_ts):\n",
    "    x_parameter = layers.Conv2D(128, kernel_size=3, padding='same')(x_img)\n",
    "    x_parameter = layers.Activation('relu')(x_parameter)\n",
    "\n",
    "    time_parameter = layers.Dense(128)(x_ts)\n",
    "    time_parameter = layers.Activation('relu')(time_parameter)\n",
    "    time_parameter = layers.Reshape((1, 1, 128))(time_parameter)\n",
    "    x_parameter = x_parameter * time_parameter\n",
    "    \n",
    "    x_out = layers.Conv2D(128, kernel_size=3, padding='same')(x_img)\n",
    "    x_out = x_out + x_parameter\n",
    "    x_out = layers.LayerNormalization()(x_out)\n",
    "    x_out = layers.Activation('relu')(x_out)\n",
    "    \n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    x_input = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='x_input')\n",
    "    \n",
    "    x_ts_input = layers.Input(shape=(1,), name='x_ts_input')\n",
    "    \n",
    "    x_ts = x_ts_input\n",
    "    x_ts = layers.Dense(192)(x_ts)\n",
    "    x_ts = layers.LayerNormalization()(x_ts)\n",
    "    x_ts = layers.Activation('relu')(x_ts)\n",
    "    \n",
    "    x = x_input\n",
    "    # ----- Hacia abajo -----\n",
    "    x = x32 = block(x, x_ts)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    \n",
    "    x = x16 = block(x, x_ts)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    \n",
    "    x = x8 = block(x, x_ts)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    \n",
    "    x = x4 = block(x, x_ts)\n",
    "    \n",
    "    # ----- MLP -----\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Concatenate()([x, x_ts])\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Dense(4 * 4 * 32)(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Reshape((4, 4, 32))(x)\n",
    "    \n",
    "    # ----- Hacia arriba -----\n",
    "    x = layers.Concatenate()([x, x4])\n",
    "    x = block(x, x_ts)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "    \n",
    "    x = layers.Concatenate()([x, x8])\n",
    "    x = block(x, x_ts)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "    \n",
    "    x = layers.Concatenate()([x, x16])\n",
    "    x = block(x, x_ts)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "    \n",
    "    x = layers.Concatenate()([x, x32])\n",
    "    x = block(x, x_ts)\n",
    "    \n",
    "    # ----- Salida -----\n",
    "    x = layers.Conv2D(3, kernel_size=1, padding='same')(x)\n",
    "    model = tf.keras.models.Model([x_input, x_ts_input], x)\n",
    "    return model\n",
    "\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a48be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0008)\n",
    "loss_func = tf.keras.losses.MeanAbsoluteError()\n",
    "model.compile(loss=loss_func, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step():\n",
    "    xs = []\n",
    "    x = np.random.normal(size=(8, IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "    for i in range(timesteps):\n",
    "        t = i\n",
    "        x = model.predict([x, np.full((8),  t)], verbose=0)\n",
    "        if i % 2 == 0:\n",
    "            xs.append(x[0])\n",
    "\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    for i in range(len(xs)):\n",
    "        plt.subplot(1, len(xs), i+1)\n",
    "        plt.imshow(cvtImg(xs[i]))\n",
    "        plt.title(f'{i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "predict_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(x_img):\n",
    "    x_ts = generate_ts(len(x_img))\n",
    "    x_a, x_b = forward_noise(x_img, x_ts)\n",
    "    loss = model.train_on_batch([x_a, x_ts], x_b)\n",
    "    return loss\n",
    "\n",
    "def train(R=50):\n",
    "    total = 100\n",
    "    for i in range(R):\n",
    "        for j in range(total):\n",
    "            x_img = X_train[np.random.randint(len(X_train), size=BATCH_SIZE)]\n",
    "            loss = train_one(x_img)\n",
    "            pg = (j / total) * 100\n",
    "            if j % 20 == 0:\n",
    "                print(f'iteration: {i} loss: {loss:.5f}, p: {pg:.2f}%')\n",
    "\n",
    "def predict(x_idx=None):\n",
    "    x = np.random.normal(size=(32, IMG_SIZE, IMG_SIZE, 3))\n",
    "    for i in range(timesteps):\n",
    "        t = i\n",
    "        x = model.predict([x, np.full((32), t)], verbose=0)\n",
    "    show_examples(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8993f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()\n",
    "\n",
    "# show result \n",
    "predict()\n",
    "predict_step()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f831073",
   "metadata": {},
   "source": [
    "Podemos ajustar los parámetros de aprendizaje en cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd03db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.learning_rate = max(0.000001, model.optimizer.learning_rate * 0.9)\n",
    "\n",
    "train(2)\n",
    "predict()\n",
    "predict_step()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelos/difusion.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41958f99",
   "metadata": {},
   "source": [
    "Algunos recursos adicionales:\n",
    "\n",
    "* https://github.com/CompVis/latent-diffusion\n",
    "* https://huggingface.co/learn/diffusion-course/unit0/1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
