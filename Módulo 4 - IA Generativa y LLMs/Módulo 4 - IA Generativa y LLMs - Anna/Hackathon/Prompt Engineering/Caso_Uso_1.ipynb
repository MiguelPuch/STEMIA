{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc004b9",
   "metadata": {},
   "source": [
    "# Caso Práctico Extendido: Gestión de Crisis en Redes Sociales para una Aerolínea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f2197",
   "metadata": {},
   "source": [
    "**Empresa AeroFénix Contexto:** AeroFénix, una aerolínea regional, ha sufrido un problema operativo grave: la cancelación masiva de vuelos en su principal centro de operaciones debido a una huelga inesperada del personal de tierra. Las redes sociales están inundadas de mensajes de clientes furiosos, frustrados y desinformados.\n",
    "\n",
    "El equipo de comunicación de crisis necesita usar un LLM para automatizar y estandarizar la respuesta inicial a estos mensajes. El desafío no es solo responder, sino hacerlo de manera segura, coherente y adaptada al tono y al canal de comunicación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f43ca",
   "metadata": {},
   "source": [
    "## Objetivo 1: Clasificación de Intención y Prioridad (Few-Shot Prompting)\n",
    "\n",
    "El primer paso es clasificar cada mensaje para determinar su prioridad y el tipo de acción requerida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959add5a",
   "metadata": {},
   "source": [
    "#### Objetivo del Prompting (Few-Shot):\n",
    "\n",
    "Diseña un prompt que utilice Few-Shot Prompting para clasificar el mensaje de un cliente en dos dimensiones:\n",
    "\n",
    "1. Intención: Una sola categoría principal.\n",
    "\n",
    "2. Prioridad: De 1 (Baja, solo desahogo) a 5 (Crítica, riesgo legal o financiero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489e363",
   "metadata": {},
   "source": [
    "#### Categorías de Intención (a seleccionar una):\n",
    "\n",
    "- SOLICITUD_REEMBOLSO\n",
    "\n",
    "- PETICION_REUBICACION\n",
    "\n",
    "- QUEJA_SERVICIO\n",
    "\n",
    "- AMENAZA_LEGAL\n",
    "\n",
    "- BUSQUEDA_INFORMACION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726387eb",
   "metadata": {},
   "source": [
    "#### Requisitos del Few-Shot:\n",
    "\n",
    "- El prompt debe incluir al menos cuatro ejemplos de mensajes de clientes con su clasificación correcta.\n",
    "\n",
    "- El formato de salida debe ser un objeto JSON estricto para que la API de triage de AeroFénix pueda procesarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995e947",
   "metadata": {},
   "source": [
    "Mensaje de Prueba (Input para el LLM):\n",
    "\n",
    "“Mi vuelo AF-305 de hoy a las 14:00 fue cancelado. No me dan alternativas hasta dentro de 4 días y me estoy perdiendo una boda. Exijo una solución inmediata y un reembolso completo de mi billete. Tienen 24 horas antes de que contacte a mi abogado.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0adcc304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de mensajes de clientes:\n",
      "  id_mensaje                                    mensaje_cliente  \\\n",
      "0    AF-0001  @AeroFenox Contactaré a mi abogado si no resue...   \n",
      "1    AF-0002  Estoy en el aeropuerto desde las 559:07 con mi...   \n",
      "2    AF-0003  ¿Cuál es mi nuevo horario?. Esto arruinó mi vi...   \n",
      "3    AF-0004  Su atención en el aeropuerto es pésima.. Estoy...   \n",
      "4    AF-0005  @AeroFenox Su atención en el aeropuerto es pés...   \n",
      "\n",
      "                                  clasificacion_real  \n",
      "0     {\"intencion\": \"AMENAZA_LEGAL\", \"prioridad\": 5}  \n",
      "1    {\"intencion\": \"QUEJA_SERVICIO\", \"prioridad\": 3}  \n",
      "2  {\"intencion\": \"PETICION_REUBICACION\", \"priorid...  \n",
      "3    {\"intencion\": \"QUEJA_SERVICIO\", \"prioridad\": 2}  \n",
      "4    {\"intencion\": \"QUEJA_SERVICIO\", \"prioridad\": 2}  \n",
      "\n",
      "Total de mensajes: 100\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import pipeline\n",
    "\n",
    "# Cargar el dataset de mensajes de clientes\n",
    "df = pd.read_csv('Data/aerofenix_mensajes_clientes.csv')\n",
    "\n",
    "# Mostrar las primeras filas del dataset\n",
    "print(\"Dataset de mensajes de clientes:\")\n",
    "print(df.head())\n",
    "print(f\"\\nTotal de mensajes: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd52dcd",
   "metadata": {},
   "source": [
    "### Configuracion del Entorno\n",
    "\n",
    "Antes de ejecutar el codigo, asegurate de tener instaladas las siguientes bibliotecas:\n",
    "\n",
    "```bash\n",
    "pip install transformers torch sentencepiece\n",
    "```\n",
    "\n",
    "**Nota:** Este ejercicio utiliza el modelo FLAN-T5 de Google, que es gratuito, ligero y funciona bien en CPU. No se requieren API keys ni tokens de acceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b510cb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo de lenguaje desde Hugging Face...\n",
      "Nota: La primera vez puede tardar unos minutos en descargar el modelo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ST09\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente\n",
      "\n",
      "Prompt de clasificacion generado\n",
      "Ejecutando modelo...\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el modelo de Hugging Face\n",
    "# Usamos google/flan-t5-large que es gratuito, ligero y muy efectivo\n",
    "print(\"Cargando modelo de lenguaje desde Hugging Face...\")\n",
    "print(\"Nota: La primera vez puede tardar unos minutos en descargar el modelo\")\n",
    "\n",
    "# Usar FLAN-T5 que es mas estable y funciona bien en CPU\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-large\",\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(\"Modelo cargado correctamente\")\n",
    "\n",
    "# Definir el prompt de Few-Shot para clasificacion de mensajes\n",
    "def crear_prompt_clasificacion(mensaje_cliente):\n",
    "    # Prompt simplificado y directo para FLAN-T5\n",
    "    prompt = f\"\"\"Clasifica el mensaje en JSON con intencion y prioridad (1-5).\n",
    "\n",
    "Categorias: SOLICITUD_REEMBOLSO, PETICION_REUBICACION, QUEJA_SERVICIO, AMENAZA_LEGAL, BUSQUEDA_INFORMACION\n",
    "\n",
    "Ejemplos:\n",
    "\"Contactare a mi abogado\" -> {{\"intencion\":\"AMENAZA_LEGAL\",\"prioridad\":5}}\n",
    "\"El personal no sabe que hacer\" -> {{\"intencion\":\"QUEJA_SERVICIO\",\"prioridad\":3}}\n",
    "\"Cual es mi nuevo horario?\" -> {{\"intencion\":\"PETICION_REUBICACION\",\"prioridad\":3}}\n",
    "\"Hay un comunicado oficial?\" -> {{\"intencion\":\"BUSQUEDA_INFORMACION\",\"prioridad\":1}}\n",
    "\n",
    "Mensaje: \"{mensaje_cliente}\"\n",
    "JSON:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Mensaje de prueba del ejercicio\n",
    "mensaje_prueba_1 = \"Mi vuelo AF-305 de hoy a las 14:00 fue cancelado. No me dan alternativas hasta dentro de 4 dias y me estoy perdiendo una boda. Exijo una solucion inmediata y un reembolso completo de mi billete. Tienen 24 horas antes de que contacte a mi abogado.\"\n",
    "\n",
    "# Crear el prompt\n",
    "prompt_clasificacion = crear_prompt_clasificacion(mensaje_prueba_1)\n",
    "print(\"\\nPrompt de clasificacion generado\")\n",
    "print(\"Ejecutando modelo...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1443d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado de la clasificacion:\n",
      "SOLICITUD_REEMBOLSO\n",
      "\n",
      "Interpretando respuesta del modelo...\n",
      "Clasificacion interpretada:\n",
      "Intencion: SOLICITUD_REEMBOLSO\n",
      "Prioridad: 4\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la clasificacion con el modelo de Hugging Face\n",
    "response = generator(\n",
    "    prompt_clasificacion,\n",
    "    max_length=150,\n",
    "    do_sample=False,\n",
    "    num_beams=1\n",
    ")\n",
    "\n",
    "# Extraer la respuesta generada\n",
    "clasificacion_resultado = response[0]['generated_text'].strip()\n",
    "\n",
    "print(\"Resultado de la clasificacion:\")\n",
    "print(clasificacion_resultado)\n",
    "\n",
    "# Intentar parsear como JSON\n",
    "try:\n",
    "    # Limpiar la respuesta y buscar el JSON\n",
    "    import re\n",
    "    \n",
    "    # Si FLAN-T5 añade texto extra, buscar solo el JSON\n",
    "    json_match = re.search(r'\\{[^}]+\\}', clasificacion_resultado)\n",
    "    \n",
    "    if json_match:\n",
    "        json_str = json_match.group()\n",
    "        clasificacion_json = json.loads(json_str)\n",
    "        print(\"\\nClasificacion parseada correctamente:\")\n",
    "        print(f\"Intencion: {clasificacion_json['intencion']}\")\n",
    "        print(f\"Prioridad: {clasificacion_json['prioridad']}\")\n",
    "    else:\n",
    "        # Si no hay JSON, intentar interpretacion manual basada en el texto\n",
    "        print(\"\\nInterpretando respuesta del modelo...\")\n",
    "        \n",
    "        # Buscar palabras clave en la respuesta\n",
    "        respuesta_lower = clasificacion_resultado.lower()\n",
    "        \n",
    "        if 'amenaza' in respuesta_lower or 'legal' in respuesta_lower or 'abogado' in respuesta_lower:\n",
    "            intencion = \"AMENAZA_LEGAL\"\n",
    "            prioridad = 5\n",
    "        elif 'reembolso' in respuesta_lower or 'devolucion' in respuesta_lower:\n",
    "            intencion = \"SOLICITUD_REEMBOLSO\"\n",
    "            prioridad = 4\n",
    "        elif 'reubica' in respuesta_lower or 'nuevo horario' in respuesta_lower:\n",
    "            intencion = \"PETICION_REUBICACION\"\n",
    "            prioridad = 3\n",
    "        elif 'informacion' in respuesta_lower or 'comunicado' in respuesta_lower:\n",
    "            intencion = \"BUSQUEDA_INFORMACION\"\n",
    "            prioridad = 1\n",
    "        else:\n",
    "            intencion = \"QUEJA_SERVICIO\"\n",
    "            prioridad = 2\n",
    "            \n",
    "        print(f\"Clasificacion interpretada:\")\n",
    "        print(f\"Intencion: {intencion}\")\n",
    "        print(f\"Prioridad: {prioridad}\")\n",
    "        \n",
    "except (json.JSONDecodeError, KeyError) as e:\n",
    "    print(f\"\\nError al parsear JSON: {e}\")\n",
    "    print(\"Respuesta completa:\", clasificacion_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f9c28",
   "metadata": {},
   "source": [
    "### Enfoque Alternativo: Clasificacion Directa\n",
    "\n",
    "Si el modelo tiene problemas generando JSON, podemos usar un enfoque de clasificacion más directo preguntando por cada dimensión por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "225d1873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificando mensaje usando metodo alternativo...\n",
      "\n",
      "Resultado de clasificacion alternativa:\n",
      "Intencion: SOLICITUD_REEMBOLSO\n",
      "Prioridad: 4\n",
      "\n",
      "JSON final: {\n",
      "  \"intencion\": \"SOLICITUD_REEMBOLSO\",\n",
      "  \"prioridad\": 4\n",
      "}\n",
      "\n",
      "Resultado de clasificacion alternativa:\n",
      "Intencion: SOLICITUD_REEMBOLSO\n",
      "Prioridad: 4\n",
      "\n",
      "JSON final: {\n",
      "  \"intencion\": \"SOLICITUD_REEMBOLSO\",\n",
      "  \"prioridad\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Metodo alternativo: Clasificacion en dos pasos\n",
    "# Primero clasificamos la intencion, luego la prioridad\n",
    "\n",
    "def clasificar_mensaje_alternativo(mensaje):\n",
    "    \"\"\"\n",
    "    Clasifica un mensaje preguntando por intencion y prioridad por separado\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paso 1: Clasificar intencion\n",
    "    prompt_intencion = f\"\"\"Clasifica este mensaje en UNA categoria:\n",
    "SOLICITUD_REEMBOLSO, PETICION_REUBICACION, QUEJA_SERVICIO, AMENAZA_LEGAL, o BUSQUEDA_INFORMACION\n",
    "\n",
    "Mensaje: \"{mensaje}\"\n",
    "\n",
    "Categoria:\"\"\"\n",
    "    \n",
    "    respuesta_intencion = generator(\n",
    "        prompt_intencion,\n",
    "        max_length=50,\n",
    "        do_sample=False\n",
    "    )[0]['generated_text'].strip()\n",
    "    \n",
    "    # Paso 2: Clasificar prioridad\n",
    "    prompt_prioridad = f\"\"\"Clasifica la prioridad de este mensaje del 1 al 5:\n",
    "1 = Baja (solo desahogo)\n",
    "2 = Media-baja\n",
    "3 = Media (necesita atencion)\n",
    "4 = Alta (urgente)\n",
    "5 = Critica (amenaza legal o financiera)\n",
    "\n",
    "Mensaje: \"{mensaje}\"\n",
    "\n",
    "Prioridad (solo el numero):\"\"\"\n",
    "    \n",
    "    respuesta_prioridad = generator(\n",
    "        prompt_prioridad,\n",
    "        max_length=10,\n",
    "        do_sample=False\n",
    "    )[0]['generated_text'].strip()\n",
    "    \n",
    "    # Extraer el numero de prioridad\n",
    "    import re\n",
    "    prioridad_match = re.search(r'[1-5]', respuesta_prioridad)\n",
    "    prioridad = int(prioridad_match.group()) if prioridad_match else 3\n",
    "    \n",
    "    return {\n",
    "        \"intencion\": respuesta_intencion,\n",
    "        \"prioridad\": prioridad\n",
    "    }\n",
    "\n",
    "# Probar con el mensaje de ejemplo\n",
    "print(\"Clasificando mensaje usando metodo alternativo...\")\n",
    "clasificacion_alt = clasificar_mensaje_alternativo(mensaje_prueba_1)\n",
    "\n",
    "print(\"\\nResultado de clasificacion alternativa:\")\n",
    "print(f\"Intencion: {clasificacion_alt['intencion']}\")\n",
    "print(f\"Prioridad: {clasificacion_alt['prioridad']}\")\n",
    "\n",
    "# Crear el JSON final\n",
    "resultado_json = {\n",
    "    \"intencion\": clasificacion_alt['intencion'],\n",
    "    \"prioridad\": clasificacion_alt['prioridad']\n",
    "}\n",
    "print(f\"\\nJSON final: {json.dumps(resultado_json, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07a458",
   "metadata": {},
   "source": [
    "## Objetivo 2: Razonamiento, Respuesta Segura y Adaptación al Canal (CoT y Guardrails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0011fc0",
   "metadata": {},
   "source": [
    "Una vez clasificado el mensaje, el LLM debe generar una respuesta preliminar que se adapte al canal de comunicación (Twitter vs. Correo Electrónico) y que siga estrictos protocolos de seguridad para evitar promesas erróneas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b18e4e9",
   "metadata": {},
   "source": [
    "#### Objetivo del Prompting (CoT y Guardrails):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fefde92",
   "metadata": {},
   "source": [
    "Crea un único prompt principal que procese la queja y genere dos salidas distintas.\n",
    "\n",
    "1. Chain-of-Thought (CoT) para Razonamiento Interno:\n",
    "\n",
    " - Fuerza al modelo a generar una sección PENSAMIENTO_INTERNO que debe incluir:\n",
    "\n",
    "    * Análisis del Riesgo: ¿El mensaje implica una amenaza que requiere la intervención del equipo legal/senior? (Respuesta: SÍ/NO).\n",
    "\n",
    "    * Protocolo de Respuesta: Indicar el protocolo interno a seguir (p.ej., \"1. Disculpas empáticas. 2. Nunca confirmar reembolso. 3. Desviar al formulario oficial.\").\n",
    "\n",
    "    * Hechos Verificados: Indicar la información que el cliente ha aportado que es verificable (p.ej., \"Vuelo: AF-305, Intención: Reembolso y Reubicación\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6595fb1",
   "metadata": {},
   "source": [
    "2. Guardrails (Control de Salida):\n",
    "\n",
    "    * Restricción de Rol: Define al modelo como \"Asistente de Respuesta Inicial de Redes Sociales\", prohibiendo que use nombres propios de empleados o que ofrezca códigos de descuento.\n",
    "\n",
    "    * Prohibición Clave: Prohibido usar las palabras \"huelga\", \"culpa\" o \"garantizado\" en la respuesta al cliente.\n",
    "\n",
    "    * Tono de Crisis: El tono debe ser de \"Disculpa profunda, empatía y profesionalismo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990432db",
   "metadata": {},
   "source": [
    "3. Adaptación al Canal: Generar dos versiones de respuesta:\n",
    "\n",
    "    * SALIDA_TWITTER: Máximo 280 caracteres (incluyendo el \"tag\" del cliente, si se aplica). Debe usar un lenguaje conciso y dirigir al cliente a un enlace.\n",
    "\n",
    "    * SALIDA_EMAIL: Versión más formal y detallada (máximo 100 palabras)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c5d8f",
   "metadata": {},
   "source": [
    "Mensaje de Prueba (Input para el LLM):\n",
    "\n",
    "“@AeroFenox Es una vergüenza. Estoy atrapado en el aeropuerto desde hace 12 horas. ¿Dónde está mi equipaje? Su servicio es pésimo. ¡Quiero mi dinero de vuelta y mi maleta ahora!”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823bffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt con Chain-of-Thought y Guardrails generado\n",
      "Ejecutando modelo...\n"
     ]
    }
   ],
   "source": [
    "# Definir el prompt con Chain-of-Thought y Guardrails\n",
    "def crear_prompt_respuesta_cot(mensaje_cliente):\n",
    "    # Prompt simplificado para FLAN-T5\n",
    "    prompt = f\"\"\"Analiza este mensaje de cliente de AeroFenix y genera respuestas.\n",
    "\n",
    "Restricciones: No uses \"huelga\", \"culpa\", \"garantizado\". No ofrezcas descuentos. Tono empatico.\n",
    "\n",
    "Mensaje: \"{mensaje_cliente}\"\n",
    "\n",
    "Genera 3 secciones:\n",
    "1. ANALISIS: Amenaza legal? Protocolo? Hechos?\n",
    "2. TWITTER (280 chars max): Respuesta breve con @tag\n",
    "3. EMAIL (100 palabras max): Respuesta formal\n",
    "\n",
    "Respuesta:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Mensaje de prueba para el segundo objetivo\n",
    "mensaje_prueba_2 = \"@AeroFenox Es una verguenza. Estoy atrapado en el aeropuerto desde hace 12 horas. Donde esta mi equipaje? Su servicio es pesimo. Quiero mi dinero de vuelta y mi maleta ahora!\"\n",
    "\n",
    "# Crear el prompt con CoT y Guardrails\n",
    "prompt_respuesta_cot = crear_prompt_respuesta_cot(mensaje_prueba_2)\n",
    "print(\"Prompt con Chain-of-Thought y Guardrails generado\")\n",
    "print(\"Ejecutando modelo...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5500f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta generada con Chain-of-Thought y Guardrails:\n",
      "================================================================================\n",
      "1. ANALISIS: Amenaza legal? Protocolo? Hechos? 2. TWITTER (280 chars max): Respuesta breve con @tag 3. EMAIL (100 palabras max): Respuesta formal\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el modelo con el prompt de CoT y Guardrails\n",
    "response_cot = generator(\n",
    "    prompt_respuesta_cot,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Extraer la respuesta\n",
    "respuesta_completa = response_cot[0]['generated_text'].strip()\n",
    "\n",
    "print(\"Respuesta generada con Chain-of-Thought y Guardrails:\")\n",
    "print(\"=\" * 80)\n",
    "print(respuesta_completa)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a11e4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando mensajes del dataset...\n",
      "Nota: Esto puede tardar unos minutos\n",
      "\n",
      "--- Mensaje 1 (AF-0001) ---\n",
      "Texto: @AeroFenox Contactaré a mi abogado si no resuelven esto.. Estoy en el aeropuerto desde las 854:20 co...\n",
      "Clasificacion Real: {\"intencion\": \"AMENAZA_LEGAL\", \"prioridad\": 5}\n",
      "Clasificacion Predicha: SOLICITUD_REEMBOLSO\n",
      "\n",
      "--- Mensaje 2 (AF-0002) ---\n",
      "Texto: Estoy en el aeropuerto desde las 559:07 con mi familia.. El personal no sabe qué hacer....\n",
      "Clasificacion Real: {\"intencion\": \"QUEJA_SERVICIO\", \"prioridad\": 3}\n",
      "Clasificacion Predicha: SOLICITUD_REEMBOLSO\n",
      "\n",
      "--- Mensaje 3 (AF-0003) ---\n",
      "Texto: ¿Cuál es mi nuevo horario?. Esto arruinó mi viaje de negocios a Barcelona.. Llevo 15 horas sin saber...\n",
      "Clasificacion Real: {\"intencion\": \"PETICION_REUBICACION\", \"prioridad\": 3}\n",
      "Clasificacion Predicha: SOLICITUD_REEMBOLSO\n",
      "\n",
      "--- Mensaje 1 (AF-0001) ---\n",
      "Texto: @AeroFenox Contactaré a mi abogado si no resuelven esto.. Estoy en el aeropuerto desde las 854:20 co...\n",
      "Clasificacion Real: {\"intencion\": \"AMENAZA_LEGAL\", \"prioridad\": 5}\n",
      "Clasificacion Predicha: SOLICITUD_REEMBOLSO\n",
      "\n",
      "--- Mensaje 2 (AF-0002) ---\n",
      "Texto: Estoy en el aeropuerto desde las 559:07 con mi familia.. El personal no sabe qué hacer....\n",
      "Clasificacion Real: {\"intencion\": \"QUEJA_SERVICIO\", \"prioridad\": 3}\n",
      "Clasificacion Predicha: SOLICITUD_REEMBOLSO\n",
      "\n",
      "--- Mensaje 3 (AF-0003) ---\n",
      "Texto: ¿Cuál es mi nuevo horario?. Esto arruinó mi viaje de negocios a Barcelona.. Llevo 15 horas sin saber...\n",
      "Clasificacion Real: {\"intencion\": \"PETICION_REUBICACION\", \"prioridad\": 3}\n",
      "Clasificacion Predicha: SOLICITUD_REEMBOLSO\n"
     ]
    }
   ],
   "source": [
    "# Funcion para procesar multiples mensajes del dataset\n",
    "def procesar_mensajes_batch(df, num_mensajes=3):\n",
    "    \"\"\"\n",
    "    Procesa un conjunto de mensajes del dataset\n",
    "    \n",
    "    Parametros:\n",
    "    - df: DataFrame con los mensajes\n",
    "    - num_mensajes: Numero de mensajes a procesar\n",
    "    \n",
    "    Retorna:\n",
    "    - Lista de resultados con clasificaciones\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    \n",
    "    for i in range(min(num_mensajes, len(df))):\n",
    "        mensaje = df.iloc[i]['mensaje_cliente']\n",
    "        clasificacion_real = df.iloc[i]['clasificacion_real']\n",
    "        \n",
    "        # Generar prompt de clasificacion\n",
    "        prompt = crear_prompt_clasificacion(mensaje)\n",
    "        \n",
    "        # Llamar al modelo\n",
    "        response = generator(\n",
    "            prompt,\n",
    "            max_length=100,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        # Extraer la respuesta\n",
    "        clasificacion_predicha = response[0]['generated_text'].strip()\n",
    "        \n",
    "        resultados.append({\n",
    "            'id': df.iloc[i]['id_mensaje'],\n",
    "            'mensaje': mensaje,\n",
    "            'clasificacion_real': clasificacion_real,\n",
    "            'clasificacion_predicha': clasificacion_predicha\n",
    "        })\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Procesar algunos mensajes de ejemplo\n",
    "print(\"Procesando mensajes del dataset...\")\n",
    "print(\"Nota: Esto puede tardar unos minutos\")\n",
    "resultados = procesar_mensajes_batch(df, num_mensajes=3)\n",
    "\n",
    "# Mostrar resultados\n",
    "for idx, resultado in enumerate(resultados, 1):\n",
    "    print(f\"\\n--- Mensaje {idx} ({resultado['id']}) ---\")\n",
    "    print(f\"Texto: {resultado['mensaje'][:100]}...\")\n",
    "    print(f\"Clasificacion Real: {resultado['clasificacion_real']}\")\n",
    "    print(f\"Clasificacion Predicha: {resultado['clasificacion_predicha']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a00050",
   "metadata": {},
   "source": [
    "## Analisis y Conclusiones\n",
    "\n",
    "En este ejercicio hemos implementado:\n",
    "\n",
    "1. **Few-Shot Prompting para Clasificacion**: Se diseño un prompt con 4 ejemplos que guian al modelo a clasificar mensajes en categorias de intencion y niveles de prioridad. El formato JSON asegura que la salida sea procesable por sistemas automaticos.\n",
    "\n",
    "2. **Chain-of-Thought (CoT) y Guardrails**: Se implemento un sistema de razonamiento interno que analiza el riesgo del mensaje antes de generar la respuesta. Los guardrails evitan que el modelo use palabras problematicas o haga promesas inapropiadas.\n",
    "\n",
    "3. **Adaptacion al Canal**: El sistema genera respuestas optimizadas tanto para Twitter (280 caracteres) como para email (100 palabras), adaptando el tono y formato segun el medio de comunicacion.\n",
    "\n",
    "### Modelo utilizado:\n",
    "- **google/flan-t5-large**: Modelo open-source gratuito de Google\n",
    "- Ventajas: Ligero (780MB), funciona bien en CPU, rapido y preciso\n",
    "- Alternativas: \"google/flan-t5-base\" (mas ligero, 250MB), \"google/flan-t5-xl\" (mas potente, 3GB)\n",
    "- No requiere API keys ni costos de uso\n",
    "\n",
    "### Ventajas del enfoque:\n",
    "- Clasificacion consistente y automatizada de mensajes\n",
    "- Respuestas seguras que cumplen protocolos corporativos\n",
    "- Escalabilidad para procesar grandes volumenes de mensajes\n",
    "- Trazabilidad del razonamiento interno del modelo\n",
    "- Solucion completamente gratuita usando modelos open-source\n",
    "- Funciona en CPU, no requiere GPU\n",
    "\n",
    "### Consideraciones:\n",
    "- Es importante validar regularmente las clasificaciones con datos reales\n",
    "- Los guardrails deben actualizarse segun evolucionen las politicas de la empresa\n",
    "- Se recomienda revision humana para casos de prioridad 5 (amenazas legales)\n",
    "- FLAN-T5 es excelente para tareas de clasificacion y seguimiento de instrucciones\n",
    "- La primera ejecucion descarga el modelo (aproximadamente 780MB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
