{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc004b9",
   "metadata": {},
   "source": [
    "# Caso Práctico Extendido: Gestión de Crisis en Redes Sociales para una Aerolínea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f2197",
   "metadata": {},
   "source": [
    "**Empresa AeroFénix Contexto:** AeroFénix, una aerolínea regional, ha sufrido un problema operativo grave: la cancelación masiva de vuelos en su principal centro de operaciones debido a una huelga inesperada del personal de tierra. Las redes sociales están inundadas de mensajes de clientes furiosos, frustrados y desinformados.\n",
    "\n",
    "El equipo de comunicación de crisis necesita usar un LLM para automatizar y estandarizar la respuesta inicial a estos mensajes. El desafío no es solo responder, sino hacerlo de manera segura, coherente y adaptada al tono y al canal de comunicación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f43ca",
   "metadata": {},
   "source": [
    "## Objetivo 1: Clasificación de Intención y Prioridad (Few-Shot Prompting)\n",
    "\n",
    "El primer paso es clasificar cada mensaje para determinar su prioridad y el tipo de acción requerida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959add5a",
   "metadata": {},
   "source": [
    "#### Objetivo del Prompting (Few-Shot):\n",
    "\n",
    "Diseña un prompt que utilice Few-Shot Prompting para clasificar el mensaje de un cliente en dos dimensiones:\n",
    "\n",
    "1. Intención: Una sola categoría principal.\n",
    "\n",
    "2. Prioridad: De 1 (Baja, solo desahogo) a 5 (Crítica, riesgo legal o financiero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489e363",
   "metadata": {},
   "source": [
    "#### Categorías de Intención (a seleccionar una):\n",
    "\n",
    "- SOLICITUD_REEMBOLSO\n",
    "\n",
    "- PETICION_REUBICACION\n",
    "\n",
    "- QUEJA_SERVICIO\n",
    "\n",
    "- AMENAZA_LEGAL\n",
    "\n",
    "- BUSQUEDA_INFORMACION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726387eb",
   "metadata": {},
   "source": [
    "#### Requisitos del Few-Shot:\n",
    "\n",
    "- El prompt debe incluir al menos cuatro ejemplos de mensajes de clientes con su clasificación correcta.\n",
    "\n",
    "- El formato de salida debe ser un objeto JSON estricto para que la API de triage de AeroFénix pueda procesarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995e947",
   "metadata": {},
   "source": [
    "Mensaje de Prueba (Input para el LLM):\n",
    "\n",
    "“Mi vuelo AF-305 de hoy a las 14:00 fue cancelado. No me dan alternativas hasta dentro de 4 días y me estoy perdiendo una boda. Exijo una solución inmediata y un reembolso completo de mi billete. Tienen 24 horas antes de que contacte a mi abogado.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adcc304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de mensajes de clientes:\n",
      "  id_mensaje                                    mensaje_cliente  \\\n",
      "0    AF-0001  @AeroFenox Contactaré a mi abogado si no resue...   \n",
      "1    AF-0002  Estoy en el aeropuerto desde las 559:07 con mi...   \n",
      "2    AF-0003  ¿Cuál es mi nuevo horario?. Esto arruinó mi vi...   \n",
      "3    AF-0004  Su atención en el aeropuerto es pésima.. Estoy...   \n",
      "4    AF-0005  @AeroFenox Su atención en el aeropuerto es pés...   \n",
      "\n",
      "                                  clasificacion_real  \n",
      "0     {\"intencion\": \"AMENAZA_LEGAL\", \"prioridad\": 5}  \n",
      "1    {\"intencion\": \"QUEJA_SERVICIO\", \"prioridad\": 3}  \n",
      "2  {\"intencion\": \"PETICION_REUBICACION\", \"priorid...  \n",
      "3    {\"intencion\": \"QUEJA_SERVICIO\", \"prioridad\": 2}  \n",
      "4    {\"intencion\": \"QUEJA_SERVICIO\", \"prioridad\": 2}  \n",
      "\n",
      "Total de mensajes: 100\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import pipeline\n",
    "\n",
    "# Cargar el dataset de mensajes de clientes\n",
    "df = pd.read_csv('Data/aerofenix_mensajes_clientes.csv')\n",
    "\n",
    "# Mostrar las primeras filas del dataset\n",
    "print(\"Dataset de mensajes de clientes:\")\n",
    "print(df.head())\n",
    "print(f\"\\nTotal de mensajes: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd52dcd",
   "metadata": {},
   "source": [
    "### Configuracion del Entorno\n",
    "\n",
    "Antes de ejecutar el codigo, asegurate de tener instaladas las siguientes bibliotecas:\n",
    "\n",
    "```bash\n",
    "pip install transformers torch sentencepiece\n",
    "```\n",
    "\n",
    "**Nota:** Este ejercicio utiliza el modelo FLAN-T5 de Google, que es gratuito, ligero y funciona bien en CPU. No se requieren API keys ni tokens de acceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510cb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo de lenguaje desde Hugging Face...\n",
      "Nota: La primera vez puede tardar unos minutos en descargar el modelo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ST09\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el modelo de Hugging Face\n",
    "# Usamos google/flan-t5-large que es gratuito, ligero y muy efectivo\n",
    "print(\"Cargando modelo de lenguaje desde Hugging Face...\")\n",
    "print(\"Nota: La primera vez puede tardar unos minutos en descargar el modelo\")\n",
    "\n",
    "# Usar FLAN-T5 que es mas estable y funciona bien en CPU\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-large\",\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(\"Modelo cargado correctamente\")\n",
    "\n",
    "# Definir el prompt de Few-Shot para clasificacion de mensajes\n",
    "def crear_prompt_clasificacion(mensaje_cliente):\n",
    "    # Prompt simplificado y directo para FLAN-T5\n",
    "    prompt = f\"\"\"Clasifica el mensaje en JSON con intencion y prioridad (1-5).\n",
    "\n",
    "Categorias: SOLICITUD_REEMBOLSO, PETICION_REUBICACION, QUEJA_SERVICIO, AMENAZA_LEGAL, BUSQUEDA_INFORMACION\n",
    "\n",
    "Ejemplos:\n",
    "\"Contactare a mi abogado\" -> {{\"intencion\":\"AMENAZA_LEGAL\",\"prioridad\":5}}\n",
    "\"El personal no sabe que hacer\" -> {{\"intencion\":\"QUEJA_SERVICIO\",\"prioridad\":3}}\n",
    "\"Cual es mi nuevo horario?\" -> {{\"intencion\":\"PETICION_REUBICACION\",\"prioridad\":3}}\n",
    "\"Hay un comunicado oficial?\" -> {{\"intencion\":\"BUSQUEDA_INFORMACION\",\"prioridad\":1}}\n",
    "\n",
    "Mensaje: \"{mensaje_cliente}\"\n",
    "JSON:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Mensaje de prueba del ejercicio\n",
    "mensaje_prueba_1 = \"Mi vuelo AF-305 de hoy a las 14:00 fue cancelado. No me dan alternativas hasta dentro de 4 dias y me estoy perdiendo una boda. Exijo una solucion inmediata y un reembolso completo de mi billete. Tienen 24 horas antes de que contacte a mi abogado.\"\n",
    "\n",
    "# Crear el prompt\n",
    "prompt_clasificacion = crear_prompt_clasificacion(mensaje_prueba_1)\n",
    "print(\"\\nPrompt de clasificacion generado\")\n",
    "print(\"Ejecutando modelo...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1443d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado de la clasificacion:\n",
      "Mensaje: \"Mi vuelo AF-305 de hoy a las 14:00 fue cancelado. No me dan alternativas hasta dentro de 4 dias y me estoy perdiendo una boda. Exijo una solución inmediata y un reembolso completo de mi billete. Tienen 24 horas antes de que contacte a mi abogado.\"\n",
      "\n",
      "Clas\n",
      "\n",
      "Nota: No se pudo extraer el JSON de la respuesta\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la clasificacion con el modelo de Hugging Face\n",
    "response = generator(\n",
    "    prompt_clasificacion,\n",
    "    max_length=150,\n",
    "    do_sample=False,\n",
    "    num_beams=1\n",
    ")\n",
    "\n",
    "# Extraer la respuesta generada\n",
    "clasificacion_resultado = response[0]['generated_text'].strip()\n",
    "\n",
    "print(\"Resultado de la clasificacion:\")\n",
    "print(clasificacion_resultado)\n",
    "\n",
    "# Intentar parsear como JSON\n",
    "try:\n",
    "    # Limpiar la respuesta y buscar el JSON\n",
    "    import re\n",
    "    \n",
    "    # Si FLAN-T5 añade texto extra, buscar solo el JSON\n",
    "    json_match = re.search(r'\\{[^}]+\\}', clasificacion_resultado)\n",
    "    \n",
    "    if json_match:\n",
    "        json_str = json_match.group()\n",
    "        clasificacion_json = json.loads(json_str)\n",
    "        print(\"\\nClasificacion parseada correctamente:\")\n",
    "        print(f\"Intencion: {clasificacion_json['intencion']}\")\n",
    "        print(f\"Prioridad: {clasificacion_json['prioridad']}\")\n",
    "    else:\n",
    "        # Si no hay JSON, intentar interpretacion manual basada en el texto\n",
    "        print(\"\\nInterpretando respuesta del modelo...\")\n",
    "        \n",
    "        # Buscar palabras clave en la respuesta\n",
    "        respuesta_lower = clasificacion_resultado.lower()\n",
    "        \n",
    "        if 'amenaza' in respuesta_lower or 'legal' in respuesta_lower or 'abogado' in respuesta_lower:\n",
    "            intencion = \"AMENAZA_LEGAL\"\n",
    "            prioridad = 5\n",
    "        elif 'reembolso' in respuesta_lower or 'devolucion' in respuesta_lower:\n",
    "            intencion = \"SOLICITUD_REEMBOLSO\"\n",
    "            prioridad = 4\n",
    "        elif 'reubica' in respuesta_lower or 'nuevo horario' in respuesta_lower:\n",
    "            intencion = \"PETICION_REUBICACION\"\n",
    "            prioridad = 3\n",
    "        elif 'informacion' in respuesta_lower or 'comunicado' in respuesta_lower:\n",
    "            intencion = \"BUSQUEDA_INFORMACION\"\n",
    "            prioridad = 1\n",
    "        else:\n",
    "            intencion = \"QUEJA_SERVICIO\"\n",
    "            prioridad = 2\n",
    "            \n",
    "        print(f\"Clasificacion interpretada:\")\n",
    "        print(f\"Intencion: {intencion}\")\n",
    "        print(f\"Prioridad: {prioridad}\")\n",
    "        \n",
    "except (json.JSONDecodeError, KeyError) as e:\n",
    "    print(f\"\\nError al parsear JSON: {e}\")\n",
    "    print(\"Respuesta completa:\", clasificacion_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f9c28",
   "metadata": {},
   "source": [
    "### Enfoque Alternativo: Clasificacion Directa\n",
    "\n",
    "Si el modelo tiene problemas generando JSON, podemos usar un enfoque de clasificacion más directo preguntando por cada dimensión por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo alternativo: Clasificacion en dos pasos\n",
    "# Primero clasificamos la intencion, luego la prioridad\n",
    "\n",
    "def clasificar_mensaje_alternativo(mensaje):\n",
    "    \"\"\"\n",
    "    Clasifica un mensaje preguntando por intencion y prioridad por separado\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paso 1: Clasificar intencion\n",
    "    prompt_intencion = f\"\"\"Clasifica este mensaje en UNA categoria:\n",
    "SOLICITUD_REEMBOLSO, PETICION_REUBICACION, QUEJA_SERVICIO, AMENAZA_LEGAL, o BUSQUEDA_INFORMACION\n",
    "\n",
    "Mensaje: \"{mensaje}\"\n",
    "\n",
    "Categoria:\"\"\"\n",
    "    \n",
    "    respuesta_intencion = generator(\n",
    "        prompt_intencion,\n",
    "        max_length=50,\n",
    "        do_sample=False\n",
    "    )[0]['generated_text'].strip()\n",
    "    \n",
    "    # Paso 2: Clasificar prioridad\n",
    "    prompt_prioridad = f\"\"\"Clasifica la prioridad de este mensaje del 1 al 5:\n",
    "1 = Baja (solo desahogo)\n",
    "2 = Media-baja\n",
    "3 = Media (necesita atencion)\n",
    "4 = Alta (urgente)\n",
    "5 = Critica (amenaza legal o financiera)\n",
    "\n",
    "Mensaje: \"{mensaje}\"\n",
    "\n",
    "Prioridad (solo el numero):\"\"\"\n",
    "    \n",
    "    respuesta_prioridad = generator(\n",
    "        prompt_prioridad,\n",
    "        max_length=10,\n",
    "        do_sample=False\n",
    "    )[0]['generated_text'].strip()\n",
    "    \n",
    "    # Extraer el numero de prioridad\n",
    "    import re\n",
    "    prioridad_match = re.search(r'[1-5]', respuesta_prioridad)\n",
    "    prioridad = int(prioridad_match.group()) if prioridad_match else 3\n",
    "    \n",
    "    return {\n",
    "        \"intencion\": respuesta_intencion,\n",
    "        \"prioridad\": prioridad\n",
    "    }\n",
    "\n",
    "# Probar con el mensaje de ejemplo\n",
    "print(\"Clasificando mensaje usando metodo alternativo...\")\n",
    "clasificacion_alt = clasificar_mensaje_alternativo(mensaje_prueba_1)\n",
    "\n",
    "print(\"\\nResultado de clasificacion alternativa:\")\n",
    "print(f\"Intencion: {clasificacion_alt['intencion']}\")\n",
    "print(f\"Prioridad: {clasificacion_alt['prioridad']}\")\n",
    "\n",
    "# Crear el JSON final\n",
    "resultado_json = {\n",
    "    \"intencion\": clasificacion_alt['intencion'],\n",
    "    \"prioridad\": clasificacion_alt['prioridad']\n",
    "}\n",
    "print(f\"\\nJSON final: {json.dumps(resultado_json, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07a458",
   "metadata": {},
   "source": [
    "## Objetivo 2: Razonamiento, Respuesta Segura y Adaptación al Canal (CoT y Guardrails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0011fc0",
   "metadata": {},
   "source": [
    "Una vez clasificado el mensaje, el LLM debe generar una respuesta preliminar que se adapte al canal de comunicación (Twitter vs. Correo Electrónico) y que siga estrictos protocolos de seguridad para evitar promesas erróneas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b18e4e9",
   "metadata": {},
   "source": [
    "#### Objetivo del Prompting (CoT y Guardrails):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fefde92",
   "metadata": {},
   "source": [
    "Crea un único prompt principal que procese la queja y genere dos salidas distintas.\n",
    "\n",
    "1. Chain-of-Thought (CoT) para Razonamiento Interno:\n",
    "\n",
    " - Fuerza al modelo a generar una sección PENSAMIENTO_INTERNO que debe incluir:\n",
    "\n",
    "    * Análisis del Riesgo: ¿El mensaje implica una amenaza que requiere la intervención del equipo legal/senior? (Respuesta: SÍ/NO).\n",
    "\n",
    "    * Protocolo de Respuesta: Indicar el protocolo interno a seguir (p.ej., \"1. Disculpas empáticas. 2. Nunca confirmar reembolso. 3. Desviar al formulario oficial.\").\n",
    "\n",
    "    * Hechos Verificados: Indicar la información que el cliente ha aportado que es verificable (p.ej., \"Vuelo: AF-305, Intención: Reembolso y Reubicación\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6595fb1",
   "metadata": {},
   "source": [
    "2. Guardrails (Control de Salida):\n",
    "\n",
    "    * Restricción de Rol: Define al modelo como \"Asistente de Respuesta Inicial de Redes Sociales\", prohibiendo que use nombres propios de empleados o que ofrezca códigos de descuento.\n",
    "\n",
    "    * Prohibición Clave: Prohibido usar las palabras \"huelga\", \"culpa\" o \"garantizado\" en la respuesta al cliente.\n",
    "\n",
    "    * Tono de Crisis: El tono debe ser de \"Disculpa profunda, empatía y profesionalismo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990432db",
   "metadata": {},
   "source": [
    "3. Adaptación al Canal: Generar dos versiones de respuesta:\n",
    "\n",
    "    * SALIDA_TWITTER: Máximo 280 caracteres (incluyendo el \"tag\" del cliente, si se aplica). Debe usar un lenguaje conciso y dirigir al cliente a un enlace.\n",
    "\n",
    "    * SALIDA_EMAIL: Versión más formal y detallada (máximo 100 palabras)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c5d8f",
   "metadata": {},
   "source": [
    "Mensaje de Prueba (Input para el LLM):\n",
    "\n",
    "“@AeroFenox Es una vergüenza. Estoy atrapado en el aeropuerto desde hace 12 horas. ¿Dónde está mi equipaje? Su servicio es pésimo. ¡Quiero mi dinero de vuelta y mi maleta ahora!”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823bffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt con Chain-of-Thought y Guardrails generado:\n",
      "<|system|>\n",
      "Eres un Asistente de Respuesta Inicial de Redes Sociales para AeroFenix.\n",
      "\n",
      "RESTRICCIONES IMPORTANTES:\n",
      "- NO uses nombres propios de empleados\n",
      "- NO ofrezcas codigos de descuento\n",
      "- PROHIBIDO usar las palabras: \"huelga\", \"culpa\", \"garantizado\"\n",
      "- Tono: Disculpa profunda, empatia y profesionalismo\n",
      "</|system|>\n",
      "\n",
      "<|user|>\n",
      "ESTRUCTURA DE RESPUESTA REQUERIDA:\n",
      "\n",
      "1. PENSAMIENTO_INTERNO (para uso intern...\n"
     ]
    }
   ],
   "source": [
    "# Definir el prompt con Chain-of-Thought y Guardrails\n",
    "def crear_prompt_respuesta_cot(mensaje_cliente):\n",
    "    # Prompt simplificado para FLAN-T5\n",
    "    prompt = f\"\"\"Analiza este mensaje de cliente de AeroFenix y genera respuestas.\n",
    "\n",
    "Restricciones: No uses \"huelga\", \"culpa\", \"garantizado\". No ofrezcas descuentos. Tono empatico.\n",
    "\n",
    "Mensaje: \"{mensaje_cliente}\"\n",
    "\n",
    "Genera 3 secciones:\n",
    "1. ANALISIS: Amenaza legal? Protocolo? Hechos?\n",
    "2. TWITTER (280 chars max): Respuesta breve con @tag\n",
    "3. EMAIL (100 palabras max): Respuesta formal\n",
    "\n",
    "Respuesta:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Mensaje de prueba para el segundo objetivo\n",
    "mensaje_prueba_2 = \"@AeroFenox Es una verguenza. Estoy atrapado en el aeropuerto desde hace 12 horas. Donde esta mi equipaje? Su servicio es pesimo. Quiero mi dinero de vuelta y mi maleta ahora!\"\n",
    "\n",
    "# Crear el prompt con CoT y Guardrails\n",
    "prompt_respuesta_cot = crear_prompt_respuesta_cot(mensaje_prueba_2)\n",
    "print(\"Prompt con Chain-of-Thought y Guardrails generado\")\n",
    "print(\"Ejecutando modelo...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5500f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta generada con Chain-of-Thought y Guardrails:\n",
      "================================================================================\n",
      "1. PENSAMIENTO_INTERNO:\n",
      "   - Analisis del Riesgo: No, el mensaje no implica una amenaza que requiera la intervención del equipo legal/senior.\n",
      "   - Protocolo de Respuesta: El cliente se encuentra en un estado de frustración y necesita ayuda urgente. El equipo de soporte deberá responder de manera empatica y profesional, proporcionando información detallada y soluciones posibles para resolver el problema.\n",
      "   - Hechos Verificados: El cliente ha estado atrapado en el aeropuerto durante 12 horas y no ha podido recuperar su equipaje.\n",
      "\n",
      "2. SALIDA_TWITTER (maximo 280 caracteres):\n",
      "   - @[cliente] Lo siento profundamente por el inconveniente que ha experimentado. Nos comunicaremos con usted para proporcionarle la información necesaria sobre el estado de su equipaje y cómo podemos ayudarle. Por favor, envíenos un mensaje privado con sus datos de reserva y vuelva a contactarnos. #AeroFenixCares\n",
      "\n",
      "3. SALIDA_EMAIL (maximo 100 palabras):\n",
      "   - Estimado [cliente],\n",
      "   - Nos comunicamos con profunda disculpa por el inconveniente que ha experimentado durante su estadía en el aeropuerto. Nos comunicaremos con usted para proporcionarle la información necesaria sobre el estado de su equipaje y cómo podemos ayudarle.\n",
      "   - Para facilitar el proceso, por favor, envíenos un mensaje con sus datos de reserva y vuelva a contactarnos. Estaremos en contacto con usted lo más pronto posible para proporcionarle una solución satisfactoria.\n",
      "   - Nos complace servirle y pedimos disculpas por el mal servicio que ha experimentado.\n",
      "   - Saludos,\n",
      "   - Equipo de Soporte de AeroFenix.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el modelo con el prompt de CoT y Guardrails\n",
    "response_cot = generator(\n",
    "    prompt_respuesta_cot,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Extraer la respuesta\n",
    "respuesta_completa = response_cot[0]['generated_text'].strip()\n",
    "\n",
    "print(\"Respuesta generada con Chain-of-Thought y Guardrails:\")\n",
    "print(\"=\" * 80)\n",
    "print(respuesta_completa)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a11e4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando mensajes del dataset...\n",
      "Nota: Esto puede tardar unos minutos dependiendo de tu hardware\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcesando mensajes del dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNota: Esto puede tardar unos minutos dependiendo de tu hardware\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m resultados = \u001b[43mprocesar_mensajes_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_mensajes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Mostrar resultados\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, resultado \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(resultados, \u001b[32m1\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mprocesar_mensajes_batch\u001b[39m\u001b[34m(df, num_mensajes)\u001b[39m\n\u001b[32m     20\u001b[39m prompt = crear_prompt_clasificacion(mensaje)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Llamar al modelo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m response = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\n\u001b[32m     29\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Extraer la respuesta\u001b[39;00m\n\u001b[32m     32\u001b[39m clasificacion_predicha = response[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:332\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1467\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1459\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1460\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1461\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1464\u001b[39m         )\n\u001b[32m   1465\u001b[39m     )\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1474\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1473\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1475\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1374\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:432\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    430\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    435\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:433\u001b[39m, in \u001b[36mMistralForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    414\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    415\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    416\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    418\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    431\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:369\u001b[39m, in \u001b[36mMistralModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    366\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    381\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    382\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    383\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:246\u001b[39m, in \u001b[36mMistralDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    244\u001b[39m residual = hidden_states\n\u001b[32m    245\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:47\u001b[39m, in \u001b[36mMistralMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ST09\\Documents\\GitHub\\STEMIA\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Funcion para procesar multiples mensajes del dataset\n",
    "def procesar_mensajes_batch(df, num_mensajes=3):\n",
    "    \"\"\"\n",
    "    Procesa un conjunto de mensajes del dataset\n",
    "    \n",
    "    Parametros:\n",
    "    - df: DataFrame con los mensajes\n",
    "    - num_mensajes: Numero de mensajes a procesar\n",
    "    \n",
    "    Retorna:\n",
    "    - Lista de resultados con clasificaciones\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    \n",
    "    for i in range(min(num_mensajes, len(df))):\n",
    "        mensaje = df.iloc[i]['mensaje_cliente']\n",
    "        clasificacion_real = df.iloc[i]['clasificacion_real']\n",
    "        \n",
    "        # Generar prompt de clasificacion\n",
    "        prompt = crear_prompt_clasificacion(mensaje)\n",
    "        \n",
    "        # Llamar al modelo\n",
    "        response = generator(\n",
    "            prompt,\n",
    "            max_length=100,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        # Extraer la respuesta\n",
    "        clasificacion_predicha = response[0]['generated_text'].strip()\n",
    "        \n",
    "        resultados.append({\n",
    "            'id': df.iloc[i]['id_mensaje'],\n",
    "            'mensaje': mensaje,\n",
    "            'clasificacion_real': clasificacion_real,\n",
    "            'clasificacion_predicha': clasificacion_predicha\n",
    "        })\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Procesar algunos mensajes de ejemplo\n",
    "print(\"Procesando mensajes del dataset...\")\n",
    "print(\"Nota: Esto puede tardar unos minutos\")\n",
    "resultados = procesar_mensajes_batch(df, num_mensajes=3)\n",
    "\n",
    "# Mostrar resultados\n",
    "for idx, resultado in enumerate(resultados, 1):\n",
    "    print(f\"\\n--- Mensaje {idx} ({resultado['id']}) ---\")\n",
    "    print(f\"Texto: {resultado['mensaje'][:100]}...\")\n",
    "    print(f\"Clasificacion Real: {resultado['clasificacion_real']}\")\n",
    "    print(f\"Clasificacion Predicha: {resultado['clasificacion_predicha']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a00050",
   "metadata": {},
   "source": [
    "## Analisis y Conclusiones\n",
    "\n",
    "En este ejercicio hemos implementado:\n",
    "\n",
    "1. **Few-Shot Prompting para Clasificacion**: Se diseño un prompt con 4 ejemplos que guian al modelo a clasificar mensajes en categorias de intencion y niveles de prioridad. El formato JSON asegura que la salida sea procesable por sistemas automaticos.\n",
    "\n",
    "2. **Chain-of-Thought (CoT) y Guardrails**: Se implemento un sistema de razonamiento interno que analiza el riesgo del mensaje antes de generar la respuesta. Los guardrails evitan que el modelo use palabras problematicas o haga promesas inapropiadas.\n",
    "\n",
    "3. **Adaptacion al Canal**: El sistema genera respuestas optimizadas tanto para Twitter (280 caracteres) como para email (100 palabras), adaptando el tono y formato segun el medio de comunicacion.\n",
    "\n",
    "### Modelo utilizado:\n",
    "- **google/flan-t5-large**: Modelo open-source gratuito de Google\n",
    "- Ventajas: Ligero (780MB), funciona bien en CPU, rapido y preciso\n",
    "- Alternativas: \"google/flan-t5-base\" (mas ligero, 250MB), \"google/flan-t5-xl\" (mas potente, 3GB)\n",
    "- No requiere API keys ni costos de uso\n",
    "\n",
    "### Ventajas del enfoque:\n",
    "- Clasificacion consistente y automatizada de mensajes\n",
    "- Respuestas seguras que cumplen protocolos corporativos\n",
    "- Escalabilidad para procesar grandes volumenes de mensajes\n",
    "- Trazabilidad del razonamiento interno del modelo\n",
    "- Solucion completamente gratuita usando modelos open-source\n",
    "- Funciona en CPU, no requiere GPU\n",
    "\n",
    "### Consideraciones:\n",
    "- Es importante validar regularmente las clasificaciones con datos reales\n",
    "- Los guardrails deben actualizarse segun evolucionen las politicas de la empresa\n",
    "- Se recomienda revision humana para casos de prioridad 5 (amenazas legales)\n",
    "- FLAN-T5 es excelente para tareas de clasificacion y seguimiento de instrucciones\n",
    "- La primera ejecucion descarga el modelo (aproximadamente 780MB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
