{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models (LLMs)\n",
    "\n",
    "Los large language models o LLMs son estructuras de redes neuronales pensadas especialmente para la generación del lenguaje.\n",
    "\n",
    "Heredan mucha de la tarea anteriormente realizada que ha terminado siendo condensada en lo que conocemos como Transformers.\n",
    "\n",
    "Entender la estructura de estos modelos es clave para poder saber de sus posibilidades y limitaciones. Una pieza clave del proceso es el mecanismo de atención y los bloques _transformers_ compuestos de tres piezas que podemos entender en el contexto de los buscadores (para ejemplificarlo) a pesar de que no son más que tres representaciones vectoriales con sus pesos a aprender asociados:\n",
    "\n",
    "* La **consulta** (Q o query) que es el texto de búsqueda que se escribe en la barra de búsqueda. Este es el token sobre el que desea \"encontrar más información\".\n",
    "* La **clave** (K o key) es el título de cada página web en la ventana de resultados de búsqueda. Representa los posibles tokens a los que puede prestar atención la consulta.\n",
    "* El **valor** (V) es el contenido real de las páginas web mostradas. Una vez que hemos hecho coincidir el término de búsqueda apropiado (Consulta) con los resultados relevantes (Clave), queremos obtener el contenido (Valor) de las páginas más relevantes.\n",
    "\n",
    "El concepto de multi-headed se debe a que el mecanismo de atención que presentan estas tres claves puede ser procesado de forma parcial por multiples unidades para captar distintas relacionalidades o puntos de _atención_ sobre el texto.\n",
    "\n",
    "La arquitectura base de los transformers puede verse en su artículo original _Attention is all you need_ [aquí](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "\n",
    "![](../assets/images/Transformers.png)\n",
    "\n",
    "Los siguientes recursos pueden ayudarnos a entender más en profundidad estos modelos que de algún modo combinan muchos de los conceptos vistos previamente (arquitectura encdoer-decoder, embeddings y generación condicionada):\n",
    "\n",
    "* Explicación detallada: https://poloclub.github.io/transformer-explainer/\n",
    "* Visualizador: https://bbycroft.net/llm\n",
    "\n",
    "Deberemos entender bien lo que significa la **temperatura** y el **contexto** en estos modelos. El contexto es el máximo número de tokens que puede procesar de una sola vez. Esto es importante porque las LLMs no tienen memoria, con lo cual el límite del contexto determina cuanta información de nuestra conversación les será servida:\n",
    "\n",
    "* GPT-3: 2048 tokens\n",
    "* Mistral 7B: 8192 tokens\n",
    "* GPT-4o: De 60K a 128K tokens\n",
    "* Claude 3.5: Hasta 100K tokens\n",
    "* LLama 3.1: Hasta 128K tokens\n",
    "* Gemini 1.5 Pro: Hasta 1M tokens\n",
    "\n",
    "Podemos ver y jugar con estos parámetros usando los servicios de playground:\n",
    "\n",
    "- OpenAI Playground: https://platform.openai.com/playground/prompts\n",
    "- Google AI Studio: https://aistudio.google.com/prompts/new_chat\n",
    "- Anthopic Console: https://console.anthropic.com/dashboard\n",
    "\n",
    "## ¿Qué es ChatGPT?\n",
    "\n",
    "Digamos que se trata de un _sabor_ de LLM. Quizás uno de los más populares. GPT, el modelo de LLM tras la aplicación ChatGPT, es un modelo de lenguaje desarrollado por [OpenIA](https://openai.com/).\n",
    "\n",
    "### Historia de versiones\n",
    "\n",
    "- GPT (2018): 117 millones de parámetros\n",
    "- GPT-2 (2019): 1.5 miles de millones de parámetros. \n",
    "- GPT-3 (2020) -> llegada de chatGPT: 175 miles de millones de parámetros. \n",
    "- GPT-3.5 (2022) \n",
    "- [GPT-4](https://openai.com/product/gpt-4) (2023): 100 Billones de parámetros. \n",
    "- [GPT-4o](https://en.wikipedia.org/wiki/GPT-4o) (2024)\n",
    "- [o3](https://en.wikipedia.org/wiki/OpenAI_o3) (2025)\n",
    "\n",
    "Existen multitud de modelos a día de hoy, algunos con acceso a sus pesos lo cual marca un hito en la competencia que se realizan entre sí, a pesar de que debido al tamaño de estos modelos es difícil realizar despliegues on-premise sin tener una buena infraestructura pensada para ello.\n",
    "\n",
    "![llm](../assets/images/1721759844068.jpeg)\n",
    "\n",
    "## Cómo funcionan los LLMs\n",
    "\n",
    "En esencia, los LLMs calculan las probabilidades de que cierta palabra siga a una cadena de palabras dada previamente. Para ello, se nutren de corpus o conjuntos de datos muy grandes, como toda la Wikipedia en inglés o un subconjunto representativo de las páginas de internet.\n",
    "\n",
    "El LLM \"guarda\" las sucesiones de palabras que ha encontrado en su entrenamiento y a partir de ahí asignará probabilidades a las siguientes palabras, dada una cadena de palabras previa que usa como punto de partida (el prompt). A esta fase se le llama pre-entrenamiento por su role en establecer el contexto para el resto de soluciones. Muchos de estos modelos explotan arquitecturas basadas en el modelo de Transformers.\n",
    "\n",
    "Os recomendamos la documentación de HuggingFace respecto a este tema: https://huggingface.co/learn/nlp-course/es/chapter1/4\n",
    "\n",
    "## Aplicaciones de los LLMs\n",
    "\n",
    "Los LLMs brindan una amplia variedad de aplicaciones debido a su capacidad para comprender y generar lenguaje humano. Algunas de las aplicaciones más destacadas son:\n",
    "\n",
    "- **Generación de texto**: Los LLMs pueden generar contenido coherente y de alta calidad en una variedad de contextos, como redacción de artículos, resúmenes automáticos y creación de código.\n",
    "- **Respuesta a preguntas**: Los LLMs pueden responder preguntas de forma conversacional, interpretando la intención del usuario y respondiendo a comandos sofisticados.\n",
    "- **Traducción de idiomas**: Los LLMs pueden traducir texto de un idioma a otro, facilitando la comunicación intercultural.\n",
    "- **Análisis de datos**: Los LLMs pueden revisar grandes cantidades de datos de texto para extraer información de diversas fuentes y ayudar a las empresas a tomar decisiones bien fundamentadas.\n",
    "\n",
    "## Limitaciones y desafíos de los LLMs\n",
    "\n",
    "Si bien los LLMs ofrecen muchas ventajas, también tienen algunas limitaciones y desafíos a considerar:\n",
    "\n",
    "- **Coste**: Se necesita una gran cantidad de recursos para desarrollar, entrenar e implementar los LLMs.\n",
    "- **Privacidad y seguridad**: Los LLMs requieren acceso a mucha información, incluyendo en ocasiones datos de clientes o empresas, lo que debe manejarse con cuidado.\n",
    "- **Precisión y sesgo**: Los LLMs pueden incorporar información incorrecta o sesgada presente en los datos de entrenamiento, generando respuestas que no reflejan la realidad.\n",
    "\n",
    "En resumen, los Large Language Models (LLMs) son modelos de aprendizaje automático que pueden realizar una amplia gama de tareas de procesamiento del lenguaje natural gracias a su capacidad de comprender y generar lenguaje humano. Sin embargo, también presentan desafíos en cuanto a costo, privacidad, sesgo y precisión que deben ser abordados adecuadamente.\n",
    "\n",
    "Veremos unos ejemplos de cómo podéis emplear modelos \"abiertos\". Quizás uno de los más populares, Llama de Meta, nos exige que tengamos suficiente capacidad para ejecutar un modelo de al menos 8 billones de parámetros pere puede ser útil para tener una instalación local. Deberéis contar con una cuenta en HuggingFace https://huggingface.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade python-dotenv huggingface tf-keras transformers  torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podéis crear un fichero _.env_ y guardar el token que encontraréis en vuestro perfil de HuggingFace para poder acceder a la solución.\n",
    "\n",
    "```\n",
    "TOKEN_HF=hf_...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vais a utilizar algún modelo concreto, prestad atención a los términos y condiciones de uso. Llama, por ejemplo, requiere que aceptéis los términos y tarda en dar acceso.\n",
    "\n",
    "![accept](../assets/images/llama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que estos modelos pueden tener unos requisitos muy pesados, podemos emplear una versión ligera que quizás no sea tan \"inteligente\". Es cuestión de elegir el modelo que más os encaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué palabra sugiere a continuación... Podemos mirar en la documentación las opciones que nos ofrecen los pipelines: https://huggingface.co/docs/transformers/v4.43.2/en/main_classes/pipelines#pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los primeros efectos que notaremos es que la LLM está limitada por su corte de conocimiento, la fecha en la que fué entrenada (2019) y el acceso a información que tuviera en general: https://huggingface.co/openai-community/gpt2\n",
    "\n",
    "En lugar de hospedar los modelos podemos optar por usar los servicios desplegados, lo cuál os requerirá disponer de una cuenta de pago y así poder obtener el token de acceso al modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "Otras opciones populares son el consumo de los servicios en su modalidad REST. Es decir, el modelo no estará cargado en nuestro sistema si no que lo invocaremos a servicios externos como es el caso de OpenAI y sus modelos:\n",
    "\n",
    "* Generative Pre-trained Transformer (GPT)\n",
    "* DALLE: https://arxiv.org/abs/2102.12092\n",
    "* SORA: https://openai.com/index/sora/\n",
    "\n",
    "https://platform.openai.com/apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder conectarnos podemos emplear el interfaz que ellos mismos proveen (https://platform.openai.com/playground/chat?models=gpt-3.5-turbo) o conectarnos desde nuestro entorno Python, para lo que necesitaremos un token de acceso.\n",
    "\n",
    "![gpt](../assets/images/openai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los roles nos ayudan a establecer el condicionamiento del sistema a la hora de responser preguntas. `system` hace referencia al contexto de cómo responder, mientras que `user` es la consulta que realiza en usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Por debajo, estos textos han sido convertidos a la codificación (embedding) necesaria. Vemos un ejemplo de qué pinta tiene, por ejemplo, para cuando queremos añadir nuestra información particular al corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos interesa el texto de review para ver qué opinion se arrojó:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este texto es el que primeramente deberemos convertir a números si queremos que sea procesado por nuestro modelo de IA. Veamos qué embeddings arroja OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un nivel en el que habitualmente no solemos trabajar, pero para poder ajustar los modelos o encontrar documentos/textos relevantes para nuestra búsqueda, será importante conocer que existe esta opción."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
