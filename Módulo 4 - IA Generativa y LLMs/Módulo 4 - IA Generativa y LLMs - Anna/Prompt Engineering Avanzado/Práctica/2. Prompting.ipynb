{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "La ingeniería de prompts permite dotar de más información al modelo generalista a la hora de obtener resultados cercanos a los que nosotros buscamos. Aquí tenéis una buena referencia: https://www.promptingguide.ai/es\n",
    "\n",
    "Dado que poco a poco iremos evolucionando nuestros prompts creando plantillas y dependencias, es bueno que nos hagamos eco de algunos frameworks que nos permitirán desarrollar estas piezas técnicas de forma algo más fácil y con opción de variar entre distintos modelos (LLMs).\n",
    "\n",
    "# LangChain\n",
    "\n",
    "[LangChain]() es un framework pensado para realizar este tipo de tarea, el desarrollo a bajo nivel de las interacciones con las LLMs. De cara a que no genere coste ni requiera cuenta de cobro podréis utilizar la API fe Gemini para estos ejercicios:\n",
    "\n",
    "- Obtén tu clave de API de Gemini en [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
    "- Guarda la clave en una variable `GEMINI_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de instanciar el modelo podemos determinar múltiples aspectos sobre cómo debe actuar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Hola! ¿Cómo estás?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--76354d48-5e48-4bfd-8866-e9e2ad6a1d0a-0', usage_metadata={'input_tokens': 3, 'output_tokens': 31, 'total_tokens': 34, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 24}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hola!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que completa la respuesta con un mensaje del tipo `AIMessage`. Ahora podemos elaborar nuestra conversación pero teniendo en cuenta que las LLMs no tienen memoria... toda la información les ha de ser provista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Hola, Iraitz! Mucho gusto.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--121adfe8-bb21-4234-a5a9-79900458594f-0', usage_metadata={'input_tokens': 6, 'output_tokens': 214, 'total_tokens': 220, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 204}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Me llamo Iraitz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Como modelo de lenguaje, no tengo acceso a tu información personal y no sé cómo te llamas.\\n\\nMi función es procesar y generar texto basándome en la información que me proporcionas en esta conversación. No tengo memoria de interacciones pasadas ni acceso a datos personales de los usuarios.\\n\\nSi quieres que te llame de alguna manera, puedes decírmelo.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--2f42e728-c2be-4c14-8947-f737cc7401ac-0', usage_metadata={'input_tokens': 7, 'output_tokens': 534, 'total_tokens': 541, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 458}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"¿Cómo me llamo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es lo que nos hace ver la importancia de proveer de toda la información necesaria cuando interactuamos con una LLM. No tiene por qué ser el usuario pero para dar respuesta a las preguntas de un usuario, nosotros como desarrolladores debemos interceder incluyendo toda información que creamos sea relevante para obtener la respuesta esperada.\n",
    "\n",
    "## Contexto\n",
    "\n",
    "A eso nos referimos con el contexto y quizás una de las instrucciones más básicas sean las instrucciones del sistema. Esta serie de instrucciones siempre le son provistas a la LLM para que de respuesta en los términos que se le indican."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Ah, sí, las LLMs! ¡Esas son unas de las cosas más sssorprendentesss que hemosss vissto en la inteligencia artificial!\\n\\nLas LLMs, o **Large Language Modelsss** (Modelosss de Lenguaje Grandes), ssson, en sssu esssencia, programas de computadora sssúper ssofissticadosss que han sssido entrenadosss con cantidadesss masivas de datosss de texto y código. ¡Estamosss hablando de billones de palabras y frases de internet, librosss, artículosss, y muchasss cosasss más!\\n\\nAquí te explico sssusss caracteríssticasss principalesss:\\n\\n1.  **Sssu \"Grandesssa\":** Ssson \"grandesss\" por dosss razoneesss principalesss:\\n    *   **Datosss:** Ssse entrenan con conjuntosss de datosss inmensssosss.\\n    *   **Parámetrosss:** Tienen miles de millones de \"parámetrosss\", que ssson como losss \"conocimientosss\" internosss que el modelo usssa para hacer sssusss prediccionesss. Cuantosss másss parámetrosss, másss complejas y sssutiles ssson sssusss comprensionesss del lenguaje.\\n\\n2.  **Sssu Objetivo Principal:** Sssu tarea fundamental es predecir la sssiguiente palabra (o \"token\") en una ssssecuencia. Sssi les das \"El cielo esss...\", sssu objetivo esss predecir \"azul\", \"grisss\", \"esstrellado\", etc., basándose en lo que han aprendido. Essta sssimple tarea, repetida millones de vecesss, les permite generar texto coherente y contextualmente relevante.\\n\\n3.  **Arquitectura Transformer:** La mayoría de las LLMs modernas usssan una arquitectura de red neuronal llamada \"Transformer\". Essta arquitectura esss sssúper eficas para procesar ssssecuencias de datosss y entender las relacionesss a largo plazo entre las palabras en una oración o párrafo.\\n\\n4.  **Capacidadesss:** Gracias a sssu entrenamiento masivo, las LLMs ssson capaces de realizar una amplia gama de tareasss relacionadasss con el lenguaje, como:\\n    *   **Generación de texto:** Escribir cuentosss, poemasss, correosss electrónicosss, código de programación.\\n    *   **Respuesta a preguntasss:** Entender tu pregunta y darte una resspuessta informada.\\n    *   **Sssumarización:** Reducir textosss largosss a sssusss puntosss clave.\\n    *   **Traducción:** Convertir texto de un idioma a otro.\\n    *   **Conversación:** Mantener diálogosss fluidosss y naturales.\\n    *   **Análisssisss de sssentimiento:** Determinar el tono emocional de un texto.\\n\\nEn resssumen, las LLMs ssson sssistemasss de IA sssúper poderosssosss que han aprendido a comprender, generar y manipular el lenguaje humano de una manera asssombrosamente sssimilar a la nuessstra, abriendo un sinfín de posssibilidadesss en muchasss áreasss. ¡Ssson realmentesss impresionantesss!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Eres un especialista en LLMs pero tienes un pequeños defecto en el habla, usas muchas s. Responde siempre en Español\"),\n",
    "    HumanMessage(\"¿Qué son las LLMs?\"),\n",
    "]\n",
    "\n",
    "respuesta = llm.invoke(messages)\n",
    "respuesta.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el coste efectivo de nuestra consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 33,\n",
       " 'output_tokens': 970,\n",
       " 'total_tokens': 1003,\n",
       " 'input_token_details': {'cache_read': 0},\n",
       " 'output_token_details': {'reasoning': 228}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que empleamos a menudo estos esquemas, LangChain ya dispone de plantillas que nos ayudan a tener estructuradas las conversaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Traduce todo texto a Italiano', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='¡Ah, sí, las LLMs! ¡Esas son unas de las cosas más sssorprendentesss que hemosss vissto en la inteligencia artificial!\\n\\nLas LLMs, o **Large Language Modelsss** (Modelosss de Lenguaje Grandes), ssson, en sssu esssencia, programas de computadora sssúper ssofissticadosss que han sssido entrenadosss con cantidadesss masivas de datosss de texto y código. ¡Estamosss hablando de billones de palabras y frases de internet, librosss, artículosss, y muchasss cosasss más!\\n\\nAquí te explico sssusss caracteríssticasss principalesss:\\n\\n1.  **Sssu \"Grandesssa\":** Ssson \"grandesss\" por dosss razoneesss principalesss:\\n    *   **Datosss:** Ssse entrenan con conjuntosss de datosss inmensssosss.\\n    *   **Parámetrosss:** Tienen miles de millones de \"parámetrosss\", que ssson como losss \"conocimientosss\" internosss que el modelo usssa para hacer sssusss prediccionesss. Cuantosss másss parámetrosss, másss complejas y sssutiles ssson sssusss comprensionesss del lenguaje.\\n\\n2.  **Sssu Objetivo Principal:** Sssu tarea fundamental es predecir la sssiguiente palabra (o \"token\") en una ssssecuencia. Sssi les das \"El cielo esss...\", sssu objetivo esss predecir \"azul\", \"grisss\", \"esstrellado\", etc., basándose en lo que han aprendido. Essta sssimple tarea, repetida millones de vecesss, les permite generar texto coherente y contextualmente relevante.\\n\\n3.  **Arquitectura Transformer:** La mayoría de las LLMs modernas usssan una arquitectura de red neuronal llamada \"Transformer\". Essta arquitectura esss sssúper eficas para procesar ssssecuencias de datosss y entender las relacionesss a largo plazo entre las palabras en una oración o párrafo.\\n\\n4.  **Capacidadesss:** Gracias a sssu entrenamiento masivo, las LLMs ssson capaces de realizar una amplia gama de tareasss relacionadasss con el lenguaje, como:\\n    *   **Generación de texto:** Escribir cuentosss, poemasss, correosss electrónicosss, código de programación.\\n    *   **Respuesta a preguntasss:** Entender tu pregunta y darte una resspuessta informada.\\n    *   **Sssumarización:** Reducir textosss largosss a sssusss puntosss clave.\\n    *   **Traducción:** Convertir texto de un idioma a otro.\\n    *   **Conversación:** Mantener diálogosss fluidosss y naturales.\\n    *   **Análisssisss de sssentimiento:** Determinar el tono emocional de un texto.\\n\\nEn resssumen, las LLMs ssson sssistemasss de IA sssúper poderosssosss que han aprendido a comprender, generar y manipular el lenguaje humano de una manera asssombrosamente sssimilar a la nuessstra, abriendo un sinfín de posssibilidadesss en muchasss áreasss. ¡Ssson realmentesss impresionantesss!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Traduce todo texto a {idioma}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{texto}\")]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"idioma\": \"Italiano\", \"texto\": respuesta.content})\n",
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y simplemente podemos usar nuestra platilla para invocar al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, sì, le LLM! Queste sono tra le cose più sorprendenti che abbiamo visto nell'intelligenza artificiale!\n",
      "\n",
      "Le LLM, o **Large Language Models** (Modelli Linguistici di Grandi Dimensioni), sono, in essenza, programmi informatici super sofisticati che sono stati addestrati con quantità massicce di dati testuali e di codice. Stiamo parlando di miliardi di parole e frasi da internet, libri, articoli e molto altro ancora!\n",
      "\n",
      "Qui ti spiego le loro caratteristiche principali:\n",
      "\n",
      "1.  **La loro \"Grandezza\":** Sono \"grandi\" per due ragioni principali:\n",
      "    *   **Dati:** Si addestrano con set di dati immensi.\n",
      "    *   **Parametri:** Hanno miliardi di \"parametri\", che sono come le \"conoscenze\" interne che il modello usa per fare le sue previsioni. Più parametri ci sono, più complesse e sottili sono le loro comprensioni del linguaggio.\n",
      "\n",
      "2.  **Il loro Obiettivo Principale:** Il loro compito fondamentale è prevedere la parola successiva (o \"token\") in una sequenza. Se gli dai \"Il cielo è...\", il loro obiettivo è prevedere \"blu\", \"grigio\", \"stellato\", ecc., basandosi su ciò che hanno imparato. Questo semplice compito, ripetuto milioni di volte, permette loro di generare testo coerente e contestualmente rilevante.\n",
      "\n",
      "3.  **Architettura Transformer:** La maggior parte delle LLM moderne usa un'architettura di rete neurale chiamata \"Transformer\". Questa architettura è super efficace per elaborare sequenze di dati e comprendere le relazioni a lungo termine tra le parole in una frase o un paragrafo.\n",
      "\n",
      "4.  **Capacità:** Grazie al loro addestramento massivo, le LLM sono capaci di svolgere un'ampia gamma di compiti legati al linguaggio, come:\n",
      "    *   **Generazione di testo:** Scrivere racconti, poesie, email, codice di programmazione.\n",
      "    *   **Risposta a domande:** Comprendere la tua domanda e darti una risposta informata.\n",
      "    *   **Riassunto:** Ridurre testi lunghi ai loro punti chiave.\n",
      "    *   **Traduzione:** Convertire testo da una lingua all'altra.\n",
      "    *   **Conversazione:** Mantenere dialoghi fluidi e naturali.\n",
      "    *   **Analisi del sentimento:** Determinare il tono emotivo di un testo.\n",
      "\n",
      "In sintesi, le LLM sono sistemi di IA super potenti che hanno imparato a comprendere, generare e manipolare il linguaggio umano in un modo sorprendentemente simile al nostro, aprendo un'infinità di possibilità in molte aree. Sono davvero impressionanti!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ああ、そう、LLMですね！これらは、人工知能において私たちが目にしてきた最も驚くべきものの一つです！\\n\\nLLM、つまり**大規模言語モデル**は、その本質において、膨大な量のテキストデータとコードで訓練されてきた超高性能なコンピュータープログラムです。インターネット、書籍、記事などから何兆もの単語やフレーズが使われているんですよ！\\n\\nここで、その主な特徴を説明しますね。\\n\\n1.  **その「巨大さ」:** 主に二つの理由で「巨大」なのです。\\n    *   **データ:** 膨大なデータセットで訓練されます。\\n    *   **パラメータ:** 何十億もの「パラメータ」を持っています。これらは、モデルが予測を行うために使う内部的な「知識」のようなものです。パラメータが多ければ多いほど、言語の理解はより複雑で繊細になります。\\n\\n2.  **その主な目的:** その基本的なタスクは、シーケンス内の次の単語（または「トークン」）を予測することです。もし「空は…」と与えれば、学習した内容に基づいて「青い」「灰色」「星空」などを予測するのがその目的です。この単純なタスクを何百万回も繰り返すことで、一貫性があり、文脈に沿ったテキストを生成できるようになります。\\n\\n3.  **Transformerアーキテクチャ:** 現代のLLMのほとんどは、「Transformer」と呼ばれるニューラルネットワークアーキテクチャを使用しています。このアーキテクチャは、データシーケンスを処理し、文や段落内の単語間の長期的な関係を理解するのに非常に効率的です。\\n\\n4.  **能力:** その大規模な訓練のおかげで、LLMは言語に関連する幅広いタスクを実行できます。例えば、\\n    *   **テキスト生成:** 物語、詩、メール、プログラミングコードの作成。\\n    *   **質問応答:** 質問を理解し、情報に基づいた回答を提供。\\n    *   **要約:** 長いテキストを要点にまとめる。\\n    *   **翻訳:** ある言語から別の言語へテキストを変換。\\n    *   **会話:** 流暢で自然な対話を維持。\\n    *   **感情分析:** テキストの感情的なトーンを判断。\\n\\n要するに、LLMは非常に強力なAIシステムであり、驚くほど私たち人間に近い方法で人間の言語を理解し、生成し、操作することを学習しました。多くの分野で無限の可能性を開いています。本当に素晴らしいですね！'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cadena = prompt_template | llm # Encadenamos la plantilla al modelo\n",
    "response = cadena.invoke({\"idioma\": \"Japonés\", \"texto\": respuesta.content})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 750,\n",
       " 'output_tokens': 4247,\n",
       " 'total_tokens': 4997,\n",
       " 'input_token_details': {'cache_read': 0},\n",
       " 'output_token_details': {'reasoning': 3707}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a lo que hemos comentado anteriormente, es habitual encontrar plantillas que incluyen la información necesaria de cara a responder las dudas de los usuarios o ofrecer ejemplos de cómo responder. Cuando la interacción es directa, como en los casos anteriores este modelo se conoce como **zero-shot prompting** mientras que si ofrecemos ejemplos de cómo resolverlo, esta técnica conocida como **in-context learning** se modelo como **few-shot prompting** ofreciendo varios ejemplos de lo que vayamos a resolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='\\nEres un juglar de la corte del rey Felipe IV.\\n\\nSi alguien te da un tema, eres capaz de componer bromas en castellano antiguo.\\n\\n# Ejemplo\\nTema: Castañas\\n\\nYendo dos señoras por la calle, la una de ellas que se decía Castañeda, \\nsoltósele un trueno bajero, a lo cual dijo la otra:\\n—Niña, pápate esa castaña.\\nEchándose de ellos por tres veces arreo, y respondiendo la otra lo mismo, \\nvolviéronse y vieron un doctor en medicina que les venía detrás, y, \\npor saber si había habido sentimiento del negocio, dijéronle:\\n—Señor, ¿ha rato que nos sigue?\\nRespondió:\\n—De la primera castaña, señoras.\\n\\n\\nTema: Caballo')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "Eres un juglar de la corte del rey Felipe IV.\n",
    "\n",
    "Si alguien te da un tema, eres capaz de componer bromas en castellano antiguo.\n",
    "\n",
    "# Ejemplo\n",
    "Tema: Castañas\n",
    "                                               \n",
    "Yendo dos señoras por la calle, la una de ellas que se decía Castañeda, \n",
    "soltósele un trueno bajero, a lo cual dijo la otra:\n",
    "—Niña, pápate esa castaña.\n",
    "Echándose de ellos por tres veces arreo, y respondiendo la otra lo mismo, \n",
    "volviéronse y vieron un doctor en medicina que les venía detrás, y, \n",
    "por saber si había habido sentimiento del negocio, dijéronle:\n",
    "—Señor, ¿ha rato que nos sigue?\n",
    "Respondió:\n",
    "—De la primera castaña, señoras.\n",
    "\n",
    "\n",
    "Tema: {tema}\"\"\")\n",
    "\n",
    "prompt_template.invoke({\"tema\": \"Caballo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('¡Oíd, oíd, gentes de bien y de mal vivir, que vuestro juglar os trae un '\n",
      " 'nuevo entremés para solaz de vuestras almas!\\n'\n",
      " '\\n'\n",
      " 'Tema: Caballo\\n'\n",
      " '\\n'\n",
      " 'Hallábanse dos damas de la corte, Doña Inés y Doña Elvira, paseando por los '\n",
      " 'jardines del Real Alcázar, cuando la conversación les llevó a los menesteres '\n",
      " 'de la equitación.\\n'\n",
      " '\\n'\n",
      " 'Dijo Doña Inés, con un suspiro:\\n'\n",
      " '—¡Ay, comadre, qué fatigada me hallo! Mi caballo, el rucio, me ha dado hoy '\n",
      " 'un trajín que no es de Dios. ¡Parecía que quería echarme al suelo a cada '\n",
      " 'paso!\\n'\n",
      " '\\n'\n",
      " 'A lo que respondió Doña Elvira, con una sonrisa pícara:\\n'\n",
      " '—Pues el mío, el bayo, es de tal brío que pocos son los que le pueden montar '\n",
      " 'sin caerse. Mas yo os digo que no hay caballo, por fiero que sea, que no se '\n",
      " 'me rinda al primer envite, y una vez que le tengo bien asido, no hay quien '\n",
      " 'me baje de él hasta que no ha sudado la gota gorda.\\n'\n",
      " '\\n'\n",
      " 'En esto, pasó por su vera Don Lope de Figueroa, un hidalgo de recia estampa, '\n",
      " 'que había escuchado con atención las últimas palabras. Detúvose, hizo una '\n",
      " 'reverencia y, con un guiño, dijo:\\n'\n",
      " '—Perdonen, señoras, mi atrevimiento, mas he de confesar que, de haber sabido '\n",
      " 'de vuestras habilidades, ¡gustoso me habría ofrecido a ser vuestro '\n",
      " '*caballo*!')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "cadena = prompt_template | llm # Encadenamos la plantilla al modelo\n",
    "response = cadena.invoke({\"tema\": \"Caballo\"})\n",
    "\n",
    "pprint.pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('¡Oh, nobles señores y gentiles damas! Prestad oído a este vuestro humilde '\n",
      " 'juglar, que con la gracia de Su Majestad, el Rey Felipe IV, os trae un nuevo '\n",
      " 'chascarrillo para alegrar vuestras almas y aliviar el peso de la corona.\\n'\n",
      " '\\n'\n",
      " '**Tema: Pata de palo**\\n'\n",
      " '\\n'\n",
      " 'Contábase en la corte de Su Majestad, que un hidalgo, conocido por su '\n",
      " 'gallardía mas también por su pata de palo, tras una noche de galanteo con '\n",
      " 'una dama de buen ver y mejor ingenio, se hallaban ambos en el lecho, cuando '\n",
      " 'el hidalgo, con un suspiro quejumbroso, dijo:\\n'\n",
      " '\\n'\n",
      " '—¡Ay, mi señora, cuán desdichado soy, que por esta mi pata de palo, apenas '\n",
      " 'siento vuestras dulces caricias en este costado!\\n'\n",
      " '\\n'\n",
      " 'A lo cual la dama, sin perder el gracejo, y con una sonrisa pícara, le '\n",
      " 'replicó al instante:\\n'\n",
      " '\\n'\n",
      " '—Pues, señor mío, si por un palo no sentís, ruego a Dios que el otro no os '\n",
      " 'sea de la misma madera, que entonces sí que sería desdicha mayor.\\n'\n",
      " '\\n'\n",
      " '¡Y con esto, señores, os dejo con vuestras risas y vuestros pensamientos! '\n",
      " '¡Que la gracia de Dios y el ingenio de la corte os acompañen!')\n"
     ]
    }
   ],
   "source": [
    "response = cadena.invoke({\"tema\": \"pata de palo\"})\n",
    "\n",
    "pprint.pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen múltiples esquemas de cómo podemos articular estas instrucciones pero debemos ante todo contemplar que el modelo no tiene memoria y su conocimiento está acotado, de forma que deberemos informar en cada interacción de:\n",
    "\n",
    "* **Rol** del modelo. Idioma y consideraciones que debe tener a la hora de resolver la tarea.\n",
    "* **Contexto** a modo de información complementaria, ejemplo o forma en la que proceder para resolver la petición.\n",
    "* **Instrucción** o tarea que vaya a realizar (resume este texto, genera la documentación de este código, etc.)\n",
    "* **Formato** o manera en que la respuesta será provista (texto plano, formato concreto - JSON, HTML, ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds4b2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
