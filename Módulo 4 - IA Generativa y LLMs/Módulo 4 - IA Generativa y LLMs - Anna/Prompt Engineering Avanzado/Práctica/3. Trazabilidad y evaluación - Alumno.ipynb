{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936c7d23",
   "metadata": {},
   "source": [
    "Si planeamos desplegar este tipo de modelos ante todo debemos conocer como realizar dos tareas fundamentales:\n",
    "\n",
    "* Evaluar sus resultados\n",
    "* Monitorizar o trazar sus acciones\n",
    "\n",
    "Dependiendo del framework de implementación esto puede resultar algo más complejo aunque la mayoría incluyen integraciones abiertas con OpenTelemetry o trazabilidad propia como es el caso e LangChain vía LangSmith, una plataforma nube que gestiona las trazas que el framework instrumenta de forma sencilla.\n",
    "\n",
    "# LangSmith\n",
    "\n",
    "[LangSmith](https://www.langchain.com/langsmith) se presenta como un entorno nube pero también una vía para instrumentar de forma sencilla nuestras interacciones dentro del contexto de uso de las LLMs. Simplemente incluyendo las variables de entorno:\n",
    "\n",
    "```\n",
    "LANGSMITH_TRACING=true\n",
    "LANGSMITH_API_KEY=<TOKEN>\n",
    "LANGSMITH_ENDPOINT=https://api.smith.langchain.com\n",
    "LANGSMITH_PROJECT=<PROYECTO>\n",
    "```\n",
    "\n",
    "Podemos hacer un seguimiento de las llamadas, cuellos de botella y respuestas arrojadas por el proveedor que estemos empleando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cec0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "033a7fbe",
   "metadata": {},
   "source": [
    "Esto es suficiente para que nuestras acciones queden completamente registradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767797c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ec5aabe",
   "metadata": {},
   "source": [
    "![langsmith](../assets/images/langsmithpro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d4040",
   "metadata": {},
   "source": [
    "Veremos que otros frameworks como [Agno](https://docs.agno.com/examples/concepts/observability/langsmith-via-openinference) nos permiten volcar la información también a LangSmith aunque resulta algo menos sencillo instrumentar las aplicaciones en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26211775",
   "metadata": {},
   "source": [
    "# Observabilidad\n",
    "\n",
    "Otro aspecto clave es poder evaluar las respuestas de nuestro modelo. ¿Presenta sesgos? ¿Vuelca información sensible? ¿Sus respuestas son correctas? Podemos acotar mucho con un buen trabajo de prompting pero nunca estamos seguros cuando se trata de modelos probabilísticos.\n",
    "\n",
    "La gran pega es que debemos evaluar dos textos, el texto que sabemos es una buena respuesta y el arrojado por el modelo... de ahí que una de las modalidades más extendidas se trate de usar una LLM como juez (dándole las instrucciones adecuadas). Así es como operan soluciones como:\n",
    "\n",
    "* [Deepeval](https://deepeval.com/)\n",
    "* [Openevals](https://github.com/langchain-ai/openevals)\n",
    "\n",
    "Veamos algunos ejemplos aunque podéis encontrar más y más específicos en la documentación de [LangSmith](https://docs.smith.langchain.com/evaluation/tutorials).\n",
    "\n",
    "Por un lado tenemos por ejemplo, la relevancia de la respuesta:\n",
    "\n",
    "$$\n",
    "\\text{Answer relevancy} = \\frac{\\text{Number of relevant statements}}{\\text{Total number of statements}}\n",
    "$$\n",
    "\n",
    "[AnswerRelevancy](https://deepeval.com/docs/metrics-answer-relevancy#how-is-it-calculated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa62d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6177d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f68647da",
   "metadata": {},
   "source": [
    "Montemos ahora el caso de test. ¿Qué respuesta entendemos sería correcta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0110a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eece7336",
   "metadata": {},
   "source": [
    "Existen [multitud de métricas](https://deepeval.com/docs/metrics-introduction) que podemos emplear basadas en este mismo principio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508d1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b68f609",
   "metadata": {},
   "source": [
    "Podemos incluir incluso lo correcto de la respuesta, instruir a la LLM juez cómo queremos que evalúe el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d99f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds4b2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
