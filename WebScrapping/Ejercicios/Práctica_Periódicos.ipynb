{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed55b1a9",
   "metadata": {},
   "source": [
    "# Práctica de Periodismo de Datos: Web Scraping de Titulares y Análisis de Temas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8186a85",
   "metadata": {},
   "source": [
    "Esta práctica guía a los alumnos a extraer titulares de un sitio de noticias, limpiar el texto y realizar un análisis de frecuencia para identificar las tendencias informativas del día o de una sección específica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b8f87",
   "metadata": {},
   "source": [
    "**Objetivos de la Práctica**\n",
    "\n",
    "1. Web Scraping Estándar: Utilizar requests y BeautifulSoup para extraer titulares y resúmenes de noticias.\n",
    "\n",
    "2. Limpieza de Texto Avanzada: Procesar el texto eliminando stopwords (palabras comunes) para preparar el análisis.\n",
    "\n",
    "3. Análisis de Frecuencia: Determinar las palabras clave más recurrentes para identificar los temas de mayor cobertura."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aa5681",
   "metadata": {},
   "source": [
    "**Requisitos e Instalación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "967f9f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: requests in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (4.14.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\st09\\documents\\github\\stemia\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    }
   ],
   "source": [
    "pip install pandas requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459dc5de",
   "metadata": {},
   "source": [
    "## OBJETIVO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afbc55d",
   "metadata": {},
   "source": [
    "1. Web Scraping de Titulares\n",
    "\n",
    "El alumno debe elegir una sección de un periódico digital pequeño o un blog de noticias que cargue todas las noticias al inicio sin necesidad de scroll dinámico. Esto asegura que requests pueda descargar todo el contenido relevante.\n",
    "\n",
    "Tarea 1: Extracción de Titulares y Resúmenes\n",
    "El alumno debe inspeccionar la página para encontrar el contenedor de cada noticia y las etiquetas específicas de los titulares y resúmenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cd4278",
   "metadata": {},
   "source": [
    "Ejemplo: https://canfali.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34ef6ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión exitosa al sitio web (código 200)\n",
      "\n",
      "Se encontraron 111 artículos\n",
      "\n",
      "Noticias extraídas: 111\n",
      "Noticias duplicadas eliminadas: 2\n",
      "Noticias únicas: 109\n",
      "\n",
      "================================================================================\n",
      "PRIMERAS 5 NOTICIAS ÚNICAS:\n",
      "================================================================================\n",
      "1. \"¿Cuándo he mentido yo?\": los ocho cambios de versión de Mazón sobre sus cinco horas clave del 29-O\n",
      "   URL: https://www.informacion.es/comunidad-valenciana/2025/10/28/cambios-version-menti...\n",
      "\n",
      "2. El aeropuerto Alicante-Elche vuelve a operar tras el cierre por la presencia de un dron\n",
      "   URL: https://www.informacion.es/economia/2025/10/27/aeropuerto-alicante-elche-cerrado...\n",
      "\n",
      "3. Un espacio íntimo para el final de la vida\n",
      "   URL: https://www.informacion.es/alicante/2025/10/27/hospital-de-alicante-sanidad-cuid...\n",
      "\n",
      "4. Puigdemont rompe con el PSOE: \"Podrá ocupar poltronas, pero no podrá gobernar\"\n",
      "   URL: https://www.informacion.es/nacional/2025/10/27/junts-decide-unanimidad-romper-ps...\n",
      "\n",
      "5. Cristina fue asesinada en Alicante de cinco puñaladas para robarle 250.000 €\n",
      "   URL: https://www.informacion.es/sucesos/sucesos-en-alicante/2025/10/27/cristina-asesi...\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109 entries, 0 to 108\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Titular  109 non-null    object\n",
      " 1   URL      109 non-null    object\n",
      " 2   Resumen  109 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titular</th>\n",
       "      <th>URL</th>\n",
       "      <th>Resumen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"¿Cuándo he mentido yo?\": los ocho cambios de ...</td>\n",
       "      <td>https://www.informacion.es/comunidad-valencian...</td>\n",
       "      <td>Comunidad Valenciana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El aeropuerto Alicante-Elche vuelve a operar t...</td>\n",
       "      <td>https://www.informacion.es/economia/2025/10/27...</td>\n",
       "      <td>Incidente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Un espacio íntimo para el final de la vida</td>\n",
       "      <td>https://www.informacion.es/alicante/2025/10/27...</td>\n",
       "      <td>Sanidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Puigdemont rompe con el PSOE: \"Podrá ocupar po...</td>\n",
       "      <td>https://www.informacion.es/nacional/2025/10/27...</td>\n",
       "      <td>Junts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cristina fue asesinada en Alicante de cinco pu...</td>\n",
       "      <td>https://www.informacion.es/sucesos/sucesos-en-...</td>\n",
       "      <td>Sucesos en Alicante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Así puedes suscribirte a Movistar Plus+ por me...</td>\n",
       "      <td>https://www.elperiodico.com/es/shopping/2025/1...</td>\n",
       "      <td>ESNCIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Amazon despedirá a 30.000 trabajadores a parti...</td>\n",
       "      <td>https://www.informacion.es/videos/economia/202...</td>\n",
       "      <td>Economía</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Detienen a dos personas en Alicante por el ase...</td>\n",
       "      <td>https://www.informacion.es/videos/sucesos/2025...</td>\n",
       "      <td>Sucesos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Los detenidos por la muerte de una mujer en Al...</td>\n",
       "      <td>https://www.informacion.es/videos/alicante/202...</td>\n",
       "      <td>Vídeos de Alicante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>'Els pobles de l'aigua': un documental de la d...</td>\n",
       "      <td>https://www.informacion.es/videos/sociedad/202...</td>\n",
       "      <td>Un año de la dana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Titular  \\\n",
       "0    \"¿Cuándo he mentido yo?\": los ocho cambios de ...   \n",
       "1    El aeropuerto Alicante-Elche vuelve a operar t...   \n",
       "2           Un espacio íntimo para el final de la vida   \n",
       "3    Puigdemont rompe con el PSOE: \"Podrá ocupar po...   \n",
       "4    Cristina fue asesinada en Alicante de cinco pu...   \n",
       "..                                                 ...   \n",
       "104  Así puedes suscribirte a Movistar Plus+ por me...   \n",
       "105  Amazon despedirá a 30.000 trabajadores a parti...   \n",
       "106  Detienen a dos personas en Alicante por el ase...   \n",
       "107  Los detenidos por la muerte de una mujer en Al...   \n",
       "108  'Els pobles de l'aigua': un documental de la d...   \n",
       "\n",
       "                                                   URL               Resumen  \n",
       "0    https://www.informacion.es/comunidad-valencian...  Comunidad Valenciana  \n",
       "1    https://www.informacion.es/economia/2025/10/27...             Incidente  \n",
       "2    https://www.informacion.es/alicante/2025/10/27...               Sanidad  \n",
       "3    https://www.informacion.es/nacional/2025/10/27...                 Junts  \n",
       "4    https://www.informacion.es/sucesos/sucesos-en-...   Sucesos en Alicante  \n",
       "..                                                 ...                   ...  \n",
       "104  https://www.elperiodico.com/es/shopping/2025/1...               ESNCIAL  \n",
       "105  https://www.informacion.es/videos/economia/202...              Economía  \n",
       "106  https://www.informacion.es/videos/sucesos/2025...               Sucesos  \n",
       "107  https://www.informacion.es/videos/alicante/202...    Vídeos de Alicante  \n",
       "108  https://www.informacion.es/videos/sociedad/202...     Un año de la dana  \n",
       "\n",
       "[109 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# URL del sitio web a scrapear\n",
    "url = \"https://www.informacion.es/\"\n",
    "\n",
    "# Realizar la petición HTTP\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "# Verificar que la petición fue exitosa\n",
    "if response.status_code == 200:\n",
    "    print(f\"Conexión exitosa al sitio web (código {response.status_code})\")\n",
    "else:\n",
    "    print(f\"Error en la conexión (código {response.status_code})\")\n",
    "\n",
    "# Parsear el HTML con BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Listas para almacenar los datos\n",
    "titulares = []\n",
    "resumenes = []\n",
    "urls = []\n",
    "\n",
    "# Encontrar todos los artículos\n",
    "articulos = soup.find_all('article')\n",
    "\n",
    "print(f\"\\nSe encontraron {len(articulos)} artículos\")\n",
    "\n",
    "# Extraer titulares, resúmenes y URLs\n",
    "for articulo in articulos:\n",
    "    # Buscar el titular\n",
    "    titular = articulo.find(['h1', 'h2', 'h3', 'h4'])\n",
    "    if titular:\n",
    "        titulo_texto = titular.get_text(strip=True)\n",
    "        \n",
    "        # Buscar la URL (normalmente está en un enlace <a> dentro del titular o del artículo)\n",
    "        enlace = titular.find('a') if titular.find('a') else articulo.find('a')\n",
    "        if enlace and enlace.get('href'):\n",
    "            url_noticia = enlace.get('href')\n",
    "            # Si la URL es relativa, agregarle el dominio base\n",
    "            if url_noticia.startswith('/'):\n",
    "                url_noticia = url.rstrip('/') + url_noticia\n",
    "        else:\n",
    "            url_noticia = \"\"\n",
    "        \n",
    "        titulares.append(titulo_texto)\n",
    "        urls.append(url_noticia)\n",
    "        \n",
    "        # Buscar el resumen\n",
    "        resumen = articulo.find('p')\n",
    "        if resumen:\n",
    "            resumen_texto = resumen.get_text(strip=True)\n",
    "            resumenes.append(resumen_texto)\n",
    "        else:\n",
    "            resumenes.append(\"\")\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "df_noticias = pd.DataFrame({\n",
    "    'Titular': titulares,\n",
    "    'URL': urls,\n",
    "    'Resumen': resumenes\n",
    "})\n",
    "\n",
    "# ELIMINAR DUPLICADOS basándose en el titular\n",
    "noticias_antes = len(df_noticias)\n",
    "df_noticias = df_noticias.drop_duplicates(subset=['Titular'], keep='first')\n",
    "df_noticias = df_noticias.reset_index(drop=True)\n",
    "noticias_despues = len(df_noticias)\n",
    "\n",
    "print(f\"\\nNoticias extraídas: {noticias_antes}\")\n",
    "print(f\"Noticias duplicadas eliminadas: {noticias_antes - noticias_despues}\")\n",
    "print(f\"Noticias únicas: {noticias_despues}\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PRIMERAS 5 NOTICIAS ÚNICAS:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in df_noticias.head(5).iterrows():\n",
    "    print(f\"{idx+1}. {row['Titular']}\")\n",
    "    print(f\"   URL: {row['URL'][:80]}...\" if len(row['URL']) > 80 else f\"   URL: {row['URL']}\")\n",
    "    print()\n",
    "print(df_noticias.info())\n",
    "\n",
    "# Mostrar todas las noticias en tabla\n",
    "df_noticias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475657cc",
   "metadata": {},
   "source": [
    "2. Limpieza de Texto y Eliminación de Stopwords\n",
    "\n",
    "Para identificar los temas, debemos deshacernos de las palabras comunes (stopwords: 'el', 'la', 'un', 'es', 'de') que, aunque son necesarias para la gramática, no nos dicen nada sobre el tema de la noticia.\n",
    "\n",
    "Tarea 2: Limpiar y Filtrar el Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13bdaa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ST09\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargaron 313 stopwords en español\n",
      "================================================================================\n",
      "TEXTO ORIGINAL (primeros 500 caracteres):\n",
      "================================================================================\n",
      "\"¿Cuándo he mentido yo?\": los ocho cambios de versión de Mazón sobre sus cinco horas clave del 29-O Comunidad Valenciana El aeropuerto Alicante-Elche vuelve a operar tras el cierre por la presencia de un dron Incidente Un espacio íntimo para el final de la vida Sanidad Puigdemont rompe con el PSOE: \"Podrá ocupar poltronas, pero no podrá gobernar\" Junts Cristina fue asesinada en Alicante de cinco puñaladas para robarle 250.000 € Sucesos en Alicante Comunidad Valenciana, Murcia y Andalucía hacen f\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PALABRAS LIMPIAS (primeras 50):\n",
      "================================================================================\n",
      "['cuándo', 'mentido', 'ocho', 'cambios', 'versión', 'mazón', 'cinco', 'horas', 'clave', 'comunidad', 'valenciana', 'aeropuerto', 'alicanteelche', 'vuelve', 'operar', 'tras', 'cierre', 'presencia', 'dron', 'incidente', 'espacio', 'íntimo', 'final', 'vida', 'sanidad', 'puigdemont', 'rompe', 'psoe', 'podrá', 'ocupar', 'poltronas', 'podrá', 'gobernar', 'junts', 'cristina', 'asesinada', 'alicante', 'cinco', 'puñaladas', 'robarle', 'sucesos', 'alicante', 'comunidad', 'valenciana', 'murcia', 'andalucía', 'hacen', 'frente', 'común', 'envían']\n",
      "\n",
      "\n",
      "Total de palabras antes de la limpieza: 1691\n",
      "Total de palabras después de la limpieza: 968\n",
      "Palabras únicas: 739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Importar librería para stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar stopwords en español (solo necesario la primera vez)\n",
    "try:\n",
    "    stopwords_es = set(stopwords.words('spanish'))\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_es = set(stopwords.words('spanish'))\n",
    "\n",
    "print(f\"Se cargaron {len(stopwords_es)} stopwords en español\")\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"\n",
    "    Limpia el texto eliminando caracteres especiales, convirtiendo a minúsculas\n",
    "    y eliminando stopwords\n",
    "    \"\"\"\n",
    "    # Convertir a minúsculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # Eliminar caracteres especiales y números, mantener solo letras y espacios\n",
    "    texto = re.sub(r'[^a-záéíóúñü\\s]', '', texto)\n",
    "    \n",
    "    # Dividir en palabras\n",
    "    palabras = texto.split()\n",
    "    \n",
    "    # Filtrar stopwords y palabras muy cortas (menos de 3 caracteres)\n",
    "    palabras_filtradas = [palabra for palabra in palabras \n",
    "                          if palabra not in stopwords_es and len(palabra) >= 3]\n",
    "    \n",
    "    return palabras_filtradas\n",
    "\n",
    "# Combinar todos los titulares y resúmenes para el análisis\n",
    "texto_completo = ' '.join(df_noticias['Titular'].fillna('') + ' ' + df_noticias['Resumen'].fillna(''))\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEXTO ORIGINAL (primeros 500 caracteres):\")\n",
    "print(\"=\" * 80)\n",
    "print(texto_completo[:500])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Limpiar el texto\n",
    "palabras_limpias = limpiar_texto(texto_completo)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PALABRAS LIMPIAS (primeras 50):\")\n",
    "print(\"=\" * 80)\n",
    "print(palabras_limpias[:50])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Total de palabras antes de la limpieza: {len(texto_completo.split())}\")\n",
    "print(f\"Total de palabras después de la limpieza: {len(palabras_limpias)}\")\n",
    "print(f\"Palabras únicas: {len(set(palabras_limpias))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde374df",
   "metadata": {},
   "source": [
    "3. Análisis de Frecuencia de Temas\n",
    "\n",
    "Contando la frecuencia de las palabras clave restantes, podemos cuantificar qué términos son los más importantes en el conjunto de noticias, revelando las tendencias de cobertura informativa.\n",
    "\n",
    "Tarea 3: Conteo y Análisis de las Palabras Clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62c69f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANÁLISIS DE FRECUENCIA - TOP 20 PALABRAS MÁS COMUNES\n",
      "================================================================================\n",
      "   Palabra  Frecuencia\n",
      "  alicante          34\n",
      "     elche          11\n",
      "  economía          10\n",
      "       año           8\n",
      "      años           8\n",
      "  sociedad           8\n",
      "      baja           6\n",
      "  hércules           6\n",
      "     diego           5\n",
      "    quiles           4\n",
      "    último           4\n",
      "       así           4\n",
      "      dana           4\n",
      "       san           4\n",
      "       día           4\n",
      "       dos           4\n",
      "  millones           4\n",
      "    madrid           4\n",
      "torrevieja           4\n",
      "     mujer           4\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RESUMEN ESTADÍSTICO\n",
      "================================================================================\n",
      "Total de noticias analizadas: 109\n",
      "Total de palabras procesadas: 968\n",
      "Vocabulario único: 739 palabras\n",
      "Palabra más frecuente: 'alicante' (34 apariciones)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Contar la frecuencia de cada palabra\n",
    "contador_palabras = Counter(palabras_limpias)\n",
    "\n",
    "# Obtener las 20 palabras más comunes\n",
    "palabras_frecuentes = contador_palabras.most_common(20)\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "df_frecuencias = pd.DataFrame(palabras_frecuentes, columns=['Palabra', 'Frecuencia'])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÁLISIS DE FRECUENCIA - TOP 20 PALABRAS MÁS COMUNES\")\n",
    "print(\"=\" * 80)\n",
    "print(df_frecuencias.to_string(index=False))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Resumen final\n",
    "print(\"=\" * 80)\n",
    "print(f\"RESUMEN ESTADÍSTICO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total de noticias analizadas: {len(df_noticias)}\")\n",
    "print(f\"Total de palabras procesadas: {len(palabras_limpias)}\")\n",
    "print(f\"Vocabulario único: {len(set(palabras_limpias))} palabras\")\n",
    "print(f\"Palabra más frecuente: '{df_frecuencias.iloc[0]['Palabra']}' ({df_frecuencias.iloc[0]['Frecuencia']} apariciones)\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
