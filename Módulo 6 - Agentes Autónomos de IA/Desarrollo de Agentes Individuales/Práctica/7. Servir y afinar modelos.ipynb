{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67d38f3",
   "metadata": {},
   "source": [
    "Los modelos ya entrenados como GPT (la P es de pre-entrenado) se emplean mayormente como motores de inferencia o generación de muestras (texto). Ya vimos que los modelos de aprendizaje automático presentan estas dos etapas: \n",
    "\n",
    "* **entrenamiento**: donde se ajustan los pesos dados unos datos ejemplo del proceso que se quiere emular.\n",
    "* **inferencia**: donde se usa el modelo entrenado presentandole una información de entrada para que genere su resultado.\n",
    "\n",
    "De forma habitual usamos los modelos del lenguaje o LLMs a través de proveedores que realizan la inferencia en sus máquinas y nos devuelven el resultado. Esto hace que de cara a ajustar este resultado muchas empresas hayan invertido tiempo en el ajuste de la entrada (prompting, RAG, contexto-engineering,...). Sin embargo veremos que podemos ir un poco más allá.\n",
    "\n",
    "## Modelos abiertos\n",
    "\n",
    "Los modelos de código abierto o pesos abiertos nos permiten descargar tanto su estructura como sus pesos para que así nosotros podamos albergar ese servicio de inferencia.\n",
    "\n",
    "Existe diversas opciones:\n",
    "\n",
    "* Llama de [Meta](https://ai.meta.com/blog/meta-llama-3-1/)\n",
    "* Qwen de [alibaba](https://qwen.ai/home)\n",
    "* gpt-oss de [OpenAI](https://openai.com/es-ES/index/introducing-gpt-oss/)\n",
    "\n",
    "Aunque casi todas las encontraremos en [HuggingFace.co](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending). Estos modelos presentan diferencias en su arquitectura así como pesos ya que habrán sido entrenados con distintos juegos de datos.\n",
    "\n",
    "![](https://sebastianraschka.com/images/blog/2025/from-gpt-2-to-gpt-oss/3.png)\n",
    "\n",
    "https://sebastianraschka.com/blog/2025/from-gpt-2-to-gpt-oss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9b11e",
   "metadata": {},
   "source": [
    "Ejecutar estos monstruos sin embargo no es cosa sencilla. Requieren una arquitectura demandante con casi obligatoriedad de emplear varias GPUs para aligerar el tiempo de producción de resultados.Podemos aligerar parte de la carga cambiando el tipo de dato que representa los pesos. Si pasáramos de emplear 16 a 8 bites, por ejemplo, reduciríamos a la mitad el peso específico del modelo, lo cual aligera casi cualquier aspecto posterior aunque también impacta en la perdida de precisión. Es lo que comúnmente se conoce como **cuantización**.\n",
    "\n",
    "El proceso de inferencia tiene dos fases muy diferenciadas:\n",
    "\n",
    "* **Prefill**: Donde se carga la información de entrada condicionando la sección generativa\n",
    "* **Decoding**: Donde se empiezan a generar los tokens de forma secuenciada.\n",
    "\n",
    "Existen maneras de optimizar estos dos procesos buscando la mejora de tiempos clave como el tiempo hasta el primer token (TTFT), tiempo de respuesta por usuario (TPOT) o la capacidad de salida del modelo (Throughput).\n",
    "\n",
    "![](https://www.aleksagordic.com/blog/vllm/latency_diagram.png)\n",
    "\n",
    "Se procura enfocar en ambas fases para establecer las mejores arquitecturas que permiten disminuir el tiempo de carga de memoria de GPU a registro y así jugar con la información que le es servida a la GPU en lotes. Particularmente en la fase de prefill donde se calculan las matrices de atención para poder iniciar el proceso de generación existen variantes que nos permiten cachear parcialmente resultados (KV caching) o paginar el mecanismo de atención que aceleran bastante el proceso.\n",
    "\n",
    "Formula para KV caching: \n",
    "\n",
    "```\n",
    "Total KV cache size (bytes) = batch_size × sequence_length × 2 × num_layers × hidden_size × precision_in_bytes\n",
    "```\n",
    "\n",
    "Atención paginada\n",
    "\n",
    "![](https://www.adaline.ai/_next/image?url=https%3A%2F%2Fa-us.storyblok.com%2Ff%2F1023026%2F1192x628%2Fec03d2cc7e%2Fpagedattention.png&w=1920&q=75)\n",
    "\n",
    "Referencia: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices\n",
    "\n",
    "Existen varias opciones para poder ejecutar estos modelos en máquinas locales que incluyen parte de estas mejoras\n",
    "\n",
    "* [Ollama](https://ollama.com/): Sencillo de usar.\n",
    "* [SGLang](https://github.com/sgl-project/sglang): Planteado principalmente con mejoras para sistemas multi-modo (LLM, VLM) \n",
    "* [vLLM](https://docs.vllm.ai/en/latest/): Motor de inferencia que prima el throughput e implementa varias de las mejoras arriba indicadas\n",
    "* [TensorRT-LLM](https://docs.nvidia.com/tensorrt-llm/index.html): Específico de NVIDIA para la utilización de sus Tensor Cores.\n",
    "\n",
    "Deberemos elegir uno que presenta la versatilidad y facilidad de uso que necesitamos.\n",
    "\n",
    "> NOTA\n",
    "> Las dependencias de estas librerías son extras que deberemos activar mediante `uv sync --group serving`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c125e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-06 12:25:21 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 09-06 12:25:22 [utils.py:326] non-default args: {'model': 'facebook/opt-125m', 'disable_log_stats': True}\n",
      "INFO 09-06 12:25:28 [__init__.py:711] Resolved architecture: OPTForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-06 12:25:28 [__init__.py:1750] Using max model len 2048\n",
      "INFO 09-06 12:25:29 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:30 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:30 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:31 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m WARNING 09-06 12:25:31 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:31 [gpu_model_runner.py:1953] Starting to load model facebook/opt-125m...\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:31 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:31 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:32 [weight_utils.py:296] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e36d2b629a46618b0b1d4059942041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:32 [default_loader.py:262] Loading weights took 0.25 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:33 [gpu_model_runner.py:2007] Model loading took 0.2389 GiB and 1.064055 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:34 [backends.py:548] Using cache directory: /home/iraitz/.cache/vllm/torch_compile_cache/6d1eecaf73/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:34 [backends.py:559] Dynamo bytecode transform time: 1.36 s\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:35 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.617 s\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:35 [monitor.py:34] torch.compile takes 1.36 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:36 [gpu_worker.py:276] Available KV cache memory: 6.24 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:36 [kv_cache_utils.py:849] GPU KV cache size: 181,728 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:36 [kv_cache_utils.py:853] Maximum concurrency for 2,048 tokens per request: 88.73x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:00<00:00, 121.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:37 [gpu_model_runner.py:2708] Graph capturing finished in 1 secs, took 0.21 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=353318)\u001b[0;0m INFO 09-06 12:25:37 [core.py:214] init engine (profile, create kv cache, warmup model) took 4.08 seconds\n",
      "INFO 09-06 12:25:38 [llm.py:298] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83c38226c8d429195f294ee9ed5c687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df38a19b39b4b73b3d580cf43d0de25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "]\n",
    "\n",
    "# Parámetros de muestreo\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "# Modelo\n",
    "llm = LLM(model=\"facebook/opt-125m\") #\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8292c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: ' Joel, my dad is my friend and we are in a relationship. I am'\n",
      "Prompt: 'The president of the United States is', Generated text: ' engaged in a long-running war with Russian President Vladimir Putin.\\n\\nThe'\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f4454f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a98d15f054a471fac74afea3eb1c14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36e35f34a7a400288fafd614952c628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: ' J.C. and I am a student at the University of California, Berkeley'\n",
      "Prompt: 'The president of the United States is', Generated text: \" a racist.\\nHe's a racist because he's a racist.\\nHe\"\n",
      "Prompt: 'The capital of France is', Generated text: ' the capital of the French Republic.\\n\\nThe capital of France is the capital'\n",
      "Prompt: 'The future of AI is', Generated text: ' in the hands of the people.\\n\\nThe future of AI is in the'\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.1, top_k=2)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2185593",
   "metadata": {},
   "source": [
    "Podemos ver como afecta un prefijo extenso antes de cada prompt (por defecto 16 tokens) en la carga y reutilización de los bloques de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221c8441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026cd2dc163b49c49ff34131ecd26e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db5e3e3897a46609e3b766baf3d0e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298c28378e744026a5ec5b9c9938098a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e429dc856f444d91fffb2ab27886d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 ms, sys: 4.13 ms, total: 31.6 ms\n",
      "Wall time: 105 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# no_prefix\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "outputs = llm.generate(prompts[0], sampling_params)\n",
    "outputs = llm.generate(prompts[1], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d2d961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff519f3733d4346b48475c5c21322fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b0b8de9d7c43d49b64bbc8c3f5a23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a89b94c86b540d0961db750681d1c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d732cc12895b4d16b57f1e7b5c80fa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.8 ms, sys: 1.26 ms, total: 35 ms\n",
      "Wall time: 107 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "long_prefix = \"<a piece of text that is encoded into more than block_size tokens which could vary depending on the model being used and the particular configuration of it>\"\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "outputs = llm.generate(long_prefix + prompts[0], sampling_params)\n",
    "outputs = llm.generate(prompts[1], sampling_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275cfc8",
   "metadata": {},
   "source": [
    "Estos controles de bajo nivel de la inferencia nos permite actuar a nivel de vocabulario y construir un resultado dentro del vocabulario esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c2e6760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243a3745a1304568bd1dd660d3d55225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2329511c4fe44b9aae5ea20c6b7a1cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'This sucks', Generated text: 'Positive'\n",
      "Prompt: 'The weather is beautiful', Generated text: 'Positive'\n"
     ]
    }
   ],
   "source": [
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "prompts = [\n",
    "    \"This sucks\",\n",
    "    \"The weather is beautiful\",\n",
    "]\n",
    "\n",
    "guided_decoding_params = GuidedDecodingParams(choice=[\"Positive\", \"Negative\"]) # Palabras admitidas a generar\n",
    "sampling_params = SamplingParams(guided_decoding=guided_decoding_params)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d919a1",
   "metadata": {},
   "source": [
    "Cuando exista una demanda alta de peticiones, deberemos evaluar la opción de realizar inferencia en sistemas multi-GPU. Esto es casi obligatorio para el entrenamiento (al menos para el entrenamiento desde cero) pero en caso de mucha demanda, podemos beneficiarnos de esta arquitectura modular que presentan las LLMs y las opciones de cacheo de los mecanismos de atención.\n",
    "\n",
    "![](https://www.aleksagordic.com/blog/vllm/server_setup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd101641",
   "metadata": {},
   "source": [
    "Aquí tenéis una referencia en detalle sobre vLLM: https://www.aleksagordic.com/blog/vllm\n",
    "\n",
    "Servir nuestro propios modelos internamente hace que podamos variar sus parametrizaciones o jugar con distintos modelos para propósitos concretos:\n",
    "\n",
    "* Modelos rápidos y precisos en interacción con usuario\n",
    "* Modelos más lentos y específicos para agentes autónomos\n",
    "\n",
    "La gente de BentoLM se ha enfocado en que esta tarea sea algo más fácil de gestionar y nos ofrecen un servidor estandarizado para todas estas tareas, [OpenLLM](https://github.com/bentoml/OpenLLM).\n",
    "\n",
    "Aún así podemos necesitar ajustar aún más nuestros modelos.\n",
    "\n",
    "## Fine tuning\n",
    "\n",
    "El fine tuning precisa de las capacidades anteriormente indicadas ya que vamos a ajustar o variar los pesos del modelo para orientarlos a un propósito concreto.\n",
    "\n",
    "![](https://camo.githubusercontent.com/a17472f25db0af2e7a72700cf3e994b48a61405931b54111ed4d62cbe0371216/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f6d656e74616c2d6d6f64656c2e6a7067)\n",
    "\n",
    "Referencia: https://github.com/rasbt/LLMs-from-scratch\n",
    "\n",
    "Existen etapas diferenciadas pero un aspecto clave es poder recabar ejemplos del proceso en el que queremos que nuestro modelo _custom_ se especialice. Seguiremos los pasos del libro _LLMs-from-scratch_ en su capítulo 7.\n",
    "\n",
    "![](https://camo.githubusercontent.com/6736ab7968f8da6bd6fc747de22ef9afa9d840373749005ce3e96fc6ead7ed8c/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830375f636f6d707265737365642f636861707465722d6f766572766965772d312e776562703f31)\n",
    "\n",
    "Veamos si podemos seguir los pasos de este flujo usando Colab.\n",
    "\n",
    "[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb)\n",
    "\n",
    "Con estas capacidades podríamos entrenar todos los pesos de la red pero quizás solo necesitemos ajustarlos en nuestra dirección. Aquí es donde aparecen las técnicas más económicas ya que requieren un 0.1% a 1% del tiempo de un entrenamiento completo.\n",
    "\n",
    "* **LoRA (Low Rank Adaptation)** reduce el número de parámetros a entrenar aproximando las matrices de pesos mediante productos de matrices de menor rango, lo que permite ajustes más eficientes y económicos del modelo.\n",
    "* **Quantized LoRA**: Combina la cuantización de 4-bit con LoRA, reduciendo aún más la huella de memoria durante el entrenamiento mientras mantiene resultados similares a LoRA estándar. Permite fine-tuning de modelos más grandes en hardware más modesto.\n",
    "* **Reinforcemenent Learning from Human Feedback (RLHF)** se trata de aprender y refinar basado en las puntuaciones tras la validación humana\n",
    "* **Direct Preference Optimization (DPO)** ofrece un mecanismos similar al anterior pero mediante ajustes automatizados basados en entropía cruzada y descenso del gradient. Así podemos acortar los ciclos de feedback de los humanos.\n",
    "* **Group Relative Policy Optimization (GRPO)**: Es una técnica destinada a modelos de razonamiento que son ajustados mediante técnicas de aprendizaje por refuerzo (RFT) similares a las anteriores.\n",
    "\n",
    "Todas estas técnicas están ya implementadas en múltiples frameworks que nos abstraen del trabajo más técnico aunque deberemos como mínimo presentar el **conjunto de datos** y ofrecer una forma de **puntuar las respuestas del modelo**.\n",
    "\n",
    "* [Axolotl](https://axolotl.ai/)\n",
    "* [Unsloth (ejemplos)](https://docs.unsloth.ai/get-started/unsloth-notebooks)\n",
    "* [Torchtune](https://github.com/pytorch/torchtune)\n",
    "* [LlamaFactory](https://llamafactory.readthedocs.io/en/latest/)\n",
    "* [Nemo-RL](https://docs.nvidia.com/nemo/rl/latest/index.html)\n",
    "\n",
    "Más referencias en:\n",
    "\n",
    "* https://github.com/rasbt/LLMs-from-scratch/tree/main\n",
    "* https://bentoml.com/llm/getting-started/llm-fine-tuning\n",
    "* https://github.com/Azure/azure-llm-fine-tuning/tree/main\n",
    "* https://aiengineering.academy/ \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds4b2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
