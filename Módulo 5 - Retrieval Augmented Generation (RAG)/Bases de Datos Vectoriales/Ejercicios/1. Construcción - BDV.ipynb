{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc0eb25",
   "metadata": {},
   "source": [
    "# Construcción de una Base de Datos Vectorial desde cero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e7e5b",
   "metadata": {},
   "source": [
    "Una empresa llamada DataFind quiere crear su propia base de datos vectorial interna, sin depender de servicios externos.\n",
    "Necesitan almacenar, indexar y buscar información basada en significado, no solo por coincidencia de texto.\n",
    "\n",
    "Tu misión es simular una BDV completa capaz de:\n",
    "\n",
    "Almacenar embeddings.\n",
    "\n",
    "Buscar por similitud.\n",
    "\n",
    "Implementar filtros por metadatos.\n",
    "\n",
    "Medir la eficiencia y calidad de los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8782fa9",
   "metadata": {},
   "source": [
    "## Objetivos de la práctica\n",
    "\n",
    "1. Generar embeddings simulados\n",
    "\n",
    "- Cargar csv documentos_datafind.csv.\n",
    "\n",
    "- Convertirlos en vectores de alta dimensión (usando numpy para simular embeddings).\n",
    "\n",
    "2. Implementar componentes internos de la BDV:\n",
    "\n",
    "- Storage Engine: guardar los vectores y metadatos.\n",
    "\n",
    "- Index Builder: crear índices con estrategias de agrupamiento (simular IVF, HNSW o PQ).\n",
    "\n",
    "- Query Engine: calcular similitudes (cosine y euclidean).\n",
    "\n",
    "- Metadata Store: almacenar texto original, etiquetas o categorías.\n",
    "\n",
    "3. Implementar consultas vectoriales\n",
    "\n",
    "- Permitir que el usuario escriba una consulta y el sistema devuelva los documentos más similares.\n",
    "\n",
    "- Simular embeddings de la consulta con el mismo método.\n",
    "\n",
    "4. Agregar filtrado por metadatos\n",
    "\n",
    "- Permitir buscar solo en documentos de una categoría (ej. “Tecnología”, “Salud”, “Educación”).\n",
    "\n",
    "5. Evaluar el rendimiento y calidad de la búsqueda:\n",
    "\n",
    "- Calcular tiempo de consulta.\n",
    "\n",
    "- Evaluar qué métrica (cosine o euclidean) devuelve resultados más relevantes.\n",
    "\n",
    "6. Simular optimización del índice:\n",
    "\n",
    "- Comparar tiempos de búsqueda con distintas estrategias (sin índice, con IVF, con HNSW).\n",
    "\n",
    "- Analizar cuál ofrece el mejor equilibrio entre velocidad y precisión.\n",
    "\n",
    "7. Generar un informe final automático:\n",
    "\n",
    "- Guardar resultados de las consultas, métricas y tiempos en un archivo reporte_resultados.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5414ebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Información del dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         10 non-null     int64 \n",
      " 1   categoria  10 non-null     object\n",
      " 2   titulo     10 non-null     object\n",
      " 3   contenido  10 non-null     object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 452.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Cargar csv documentos_datafind.csv (numpy)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('Data/documentos_datafind.csv')\n",
    "\n",
    "# Mostrar información básica del dataset\n",
    "print(\"\\nInformación del dataset:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa9637",
   "metadata": {},
   "source": [
    "Genera embeddings usando TF-IDF\n",
    "\n",
    "    ¿Qué es TF-IDF?\n",
    "    - TF (Term Frequency): Cuántas veces aparece una palabra en un documento\n",
    "    - IDF (Inverse Document Frequency): Qué tan rara es esa palabra en todos los documentos\n",
    "    \n",
    "    TF-IDF = TF × IDF\n",
    "    \n",
    "    Ejemplo:\n",
    "    - \"el\", \"la\", \"de\" → Baja importancia (aparecen en todos lados)\n",
    "    - \"blockchain\", \"neurona\" → Alta importancia (aparecen raramente)\n",
    "    \n",
    "    Esto convierte texto en números que capturan el \"significado\" del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b2f508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embeddings generados con TF-IDF. Dimensiones: (10, 63)\n"
     ]
    }
   ],
   "source": [
    "# Convertir documentos en vectores usando TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Configurar dimensión de los embeddings\n",
    "EMBEDDING_DIM = 384  # Dimensión típica de modelos como all-MiniLM-L6-v2\n",
    "\n",
    "def generar_embeddings_tfidf(textos, max_features=EMBEDDING_DIM):\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')\n",
    "    embeddings = vectorizer.fit_transform(textos).toarray()\n",
    "    \n",
    "    # Normalizar los vectores para que tengan norma unitaria\n",
    "    # Esto es importante para calcular similitud coseno correctamente\n",
    "    embeddings = normalize(embeddings, norm='l2')\n",
    "    \n",
    "    return embeddings, vectorizer\n",
    "\n",
    "# Generar embeddings para los documentos\n",
    "document_texts = df['contenido'].tolist()\n",
    "document_embeddings, tfidf_vectorizer = generar_embeddings_tfidf(document_texts)\n",
    "print(f\"\\nEmbeddings generados con TF-IDF. Dimensiones: {document_embeddings.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d67b0748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Índice HNSW construido con sklearn NearestNeighbors.\n"
     ]
    }
   ],
   "source": [
    "# Index Builder: HSNW sin usar chromadb ycon numpy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "def construir_index_hnsw(embeddings, n_neighbors=5):\n",
    "    # Configurar el modelo HNSW\n",
    "    hnsw_index = NearestNeighbors(\n",
    "        n_neighbors=n_neighbors,\n",
    "        algorithm='brute',\n",
    "        metric='cosine'\n",
    "    )\n",
    "    \n",
    "    # Ajustar el índice con los embeddings para búsqueda\n",
    "    hnsw_index.fit(embeddings)\n",
    "    \n",
    "    return hnsw_index\n",
    "\n",
    "# Construir el índice HNSW\n",
    "hnsw_index = construir_index_hnsw(document_embeddings, n_neighbors=5)\n",
    "print(\"\\nÍndice HNSW construido con sklearn NearestNeighbors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef6eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Engine: Búsqueda de documentos similares\n",
    "def buscar_documentos_similares(query, index, vectorizer, top_k=5):\n",
    "    # Generar embedding para la consulta\n",
    "    query_embedding = vectorizer.transform([query]).toarray()\n",
    "    query_embedding = normalize(query_embedding, norm='l2') # l2 es distancia euclidiana\n",
    "    \n",
    "    # Buscar los documentos más similares en el índice\n",
    "    distances, indices = index.kneighbors(query_embedding, n_neighbors=top_k)\n",
    "    \n",
    "    return distances[0], indices[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ccc3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata Store: Almacenar y recuperar metadatos de documentos\n",
    "def obtener_metadatos_documentos(df, indices):\n",
    "    metadatos = []\n",
    "    for idx in indices:\n",
    "        metadatos.append({\n",
    "            'id': df.iloc[idx]['id'],\n",
    "            'titulo': df.iloc[idx]['titulo'],\n",
    "            'contenido': df.iloc[idx]['contenido']\n",
    "        })\n",
    "    return metadatos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511a674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documentos más similares encontrados:\n",
      "\n",
      "Documento 1:\n",
      "ID: 1\n",
      "Título: Avances en IA\n",
      "Distancia (similitud coseno): 0.5836\n",
      "Contenido: La inteligencia artificial está transformando la industria del software....\n",
      "\n",
      "Documento 2:\n",
      "ID: 6\n",
      "Título: Computación cuántica\n",
      "Distancia (similitud coseno): 0.7589\n",
      "Contenido: Se logran avances en la estabilidad de los qubits....\n",
      "\n",
      "Documento 3:\n",
      "ID: 5\n",
      "Título: Descubrimiento espacial\n",
      "Distancia (similitud coseno): 0.7644\n",
      "Contenido: Un nuevo telescopio detecta exoplanetas similares a la Tierra....\n",
      "\n",
      "Documento 4:\n",
      "ID: 10\n",
      "Título: Inflación\n",
      "Distancia (similitud coseno): 0.7740\n",
      "Contenido: La inflación global afecta los precios de productos básicos....\n",
      "\n",
      "Documento 5:\n",
      "ID: 8\n",
      "Título: Gamificación\n",
      "Distancia (similitud coseno): 0.7768\n",
      "Contenido: Las técnicas de gamificación aumentan la motivación del estudiante....\n"
     ]
    }
   ],
   "source": [
    "# Permitir que el usuario escriba una consulta y el sistema devuelva los documentos más similares.\n",
    "#Y Simular embeddings de la consulta con el mismo método.\n",
    "if __name__ == \"__main__\":\n",
    "    consulta = input(\"\\nEscribe tu consulta: \")\n",
    "    \n",
    "    # Buscar documentos similares\n",
    "    distances, indices = buscar_documentos_similares(consulta, hnsw_index, tfidf_vectorizer, top_k=5)\n",
    "    \n",
    "    # Obtener metadatos de los documentos encontrados\n",
    "    metadatos_encontrados = obtener_metadatos_documentos(df, indices)\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(\"\\nDocumentos más similares encontrados:\")\n",
    "    for i, meta in enumerate(metadatos_encontrados):\n",
    "        print(f\"\\nDocumento {i+1}:\")\n",
    "        print(f\"ID: {meta['id']}\")\n",
    "        print(f\"Título: {meta['titulo']}\")\n",
    "        print(f\"Distancia (similitud coseno): {distances[i]:.4f}\")\n",
    "        print(f\"Contenido: {meta['contenido'][:200]}...\")  # Mostrar solo los primeros 200 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9736ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permitir buscar solo en documentos de una categoría (VERSIÓN CORREGIDA)\n",
    "def buscar_documentos_por_categoria(consulta, categoria, vectorizer, df, top_k=5):\n",
    "    \"\"\"\n",
    "    Busca documentos similares dentro de una categoría específica\n",
    "    IMPORTANTE: No recrea embeddings, usa los ya existentes\n",
    "    \"\"\"\n",
    "    # Filtrar el DataFrame por la categoría dada\n",
    "    df_filtrado = df[df['categoria'] == categoria]\n",
    "    if df_filtrado.empty:\n",
    "        print(f\"\\nNo se encontraron documentos en la categoría '{categoria}'.\")\n",
    "        return [], []\n",
    "    \n",
    "    # Obtener los índices originales de los documentos filtrados\n",
    "    indices_originales = df_filtrado.index.tolist()\n",
    "    \n",
    "    # Usar los embeddings ya generados (NO crear nuevos)\n",
    "    embeddings_filtrados = document_embeddings[indices_originales]\n",
    "    \n",
    "    # Construir un nuevo índice solo con los documentos de esta categoría\n",
    "    n_neighbors = min(top_k, len(embeddings_filtrados))\n",
    "    index_filtrado = construir_index_hnsw(embeddings_filtrados, n_neighbors=n_neighbors)\n",
    "    \n",
    "    # Generar embedding para la consulta usando el vectorizador ORIGINAL\n",
    "    query_embedding = vectorizer.transform([consulta]).toarray()\n",
    "    query_embedding = normalize(query_embedding, norm='l2')\n",
    "    \n",
    "    # Buscar en el índice filtrado\n",
    "    distances, indices_locales = index_filtrado.kneighbors(query_embedding, n_neighbors=n_neighbors)\n",
    "    \n",
    "    # Mapear los índices locales a los índices originales del DataFrame\n",
    "    indices_reales = [indices_originales[idx] for idx in indices_locales[0]]\n",
    "    \n",
    "    # Obtener metadatos usando los índices reales\n",
    "    metadatos_encontrados = obtener_metadatos_documentos(df, indices_reales)\n",
    "    \n",
    "    return distances[0], metadatos_encontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013e69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documentos más similares encontrados en la categoría 'Tecnología':\n",
      "\n",
      "============================================================\n",
      "Documento 1:\n",
      "ID: 6\n",
      "Título: Computación cuántica\n",
      "Distancia (similitud coseno): 0.5278\n",
      "Contenido: Se logran avances en la estabilidad de los qubits....\n",
      "\n",
      "============================================================\n",
      "Documento 2:\n",
      "ID: 1\n",
      "Título: Avances en IA\n",
      "Distancia (similitud coseno): 0.7874\n",
      "Contenido: La inteligencia artificial está transformando la industria del software....\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el rendimiento y calidad de la búsqueda\n",
    "if __name__ == \"__main__\":\n",
    "    consulta = input(\"\\nEscribe tu consulta para búsqueda por categoría: \")\n",
    "    categoria = input(\"Escribe la categoría para filtrar: \")\n",
    "    \n",
    "    # Buscar documentos similares en la categoría dada (SIN hnsw_index)\n",
    "    distances, metadatos_encontrados = buscar_documentos_por_categoria(\n",
    "        consulta, \n",
    "        categoria, \n",
    "        tfidf_vectorizer,\n",
    "        df, \n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    if metadatos_encontrados:\n",
    "        print(f\"\\nDocumentos más similares encontrados en la categoría '{categoria}':\")\n",
    "        for i, meta in enumerate(metadatos_encontrados):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Documento {i+1}:\")\n",
    "            print(f\"ID: {meta['id']}\")\n",
    "            print(f\"Título: {meta['titulo']}\")\n",
    "            print(f\"Distancia (similitud coseno): {distances[i]:.4f}\")\n",
    "            print(f\"Contenido: {meta['contenido'][:200]}...\")\n",
    "            if distances[i] < 0.5:  # Umbral de similitud\n",
    "                print(\"Este documento es relevante para tu consulta\")\n",
    "            else:\n",
    "                print(\"Este documento es relevante, pero podría no ser el más adecuado.\")\n",
    "    else:\n",
    "        print(f\"\\nNo se encontraron documentos en la categoría '{categoria}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
