{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad76a70",
   "metadata": {},
   "source": [
    "# PR√ÅCTICA: Mejorando la Relevancia en un Sistema RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e1378",
   "metadata": {},
   "source": [
    "## 1. Preparaci√≥n del entorno\n",
    "\n",
    "Instalamos las librer√≠as necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install chromadb sentence-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bedbe1",
   "metadata": {},
   "source": [
    "Importamos los m√≥dulos principales:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac1c8d",
   "metadata": {},
   "source": [
    "## 2. Crear una mini base de conocimiento\n",
    "\n",
    "Vamos a simular una base de datos con textos sobre tecnolog√≠a, astronom√≠a y animales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "907dbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_conocimiento = {\n",
    "    \"Tecnolog√≠a\": [\n",
    "        \"La inteligencia artificial (IA) es un campo de la inform√°tica que se enfoca en la creaci√≥n de sistemas que pueden razonar, aprender y actuar de manera inteligente.\",\n",
    "        \"Python es un lenguaje de programaci√≥n de alto nivel muy popular para el desarrollo web, el an√°lisis de datos y el aprendizaje autom√°tico.\",\n",
    "        \"Blockchain es una tecnolog√≠a de registro distribuido que permite almacenar datos de manera segura y transparente.\",\n",
    "        \"El Internet de las Cosas (IoT) conecta objetos cotidianos a internet, permitiendo la comunicaci√≥n y el control remoto.\"\n",
    "    ],\n",
    "    \"Astronom√≠a\": [\n",
    "        \"La V√≠a L√°ctea es la galaxia espiral donde se encuentra nuestro Sistema Solar.\",\n",
    "        \"Un agujero negro es una regi√≥n del espacio de la que nada, ni siquiera la luz, puede escapar debido a la inmensa gravedad.\",\n",
    "        \"Marte, el planeta rojo, es un objetivo clave para la exploraci√≥n espacial debido a la posibilidad de albergar vida pasada o futura.\",\n",
    "        \"Una supernova es una explosi√≥n estelar extremadamente brillante y poderosa.\"\n",
    "    ],\n",
    "    \"Animales\": [\n",
    "        \"El guepardo es el animal terrestre m√°s r√°pido, capaz de alcanzar velocidades de hasta 120 km/h en r√°fagas cortas.\",\n",
    "        \"Los pulpos son conocidos por su alta inteligencia, capacidad de camuflaje y tener tres corazones.\",\n",
    "        \"La migraci√≥n anual del √±u en el Serengeti es uno de los mayores espect√°culos de la vida salvaje en la Tierra.\",\n",
    "        \"Las abejas desempe√±an un papel vital en la polinizaci√≥n de muchas especies de plantas, incluyendo gran parte de los cultivos.\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e18c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_categorias(db):\n",
    "    \"\"\"Muestra todas las categor√≠as disponibles en la base de conocimiento.\"\"\"\n",
    "    print(\"--- üìÇ Categor√≠as Disponibles ---\")\n",
    "    for categoria in db.keys():\n",
    "        print(f\"- **{categoria}**\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d270458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_datos_por_categoria(db, categoria):\n",
    "    \"\"\"\n",
    "    Retorna los textos asociados a una categor√≠a espec√≠fica.\n",
    "    Retorna una lista vac√≠a si la categor√≠a no existe.\n",
    "    \"\"\"\n",
    "    return db.get(categoria, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1438a1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üìÇ Categor√≠as Disponibles ---\n",
      "- **Tecnolog√≠a**\n",
      "- **Astronom√≠a**\n",
      "- **Animales**\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "mostrar_categorias(base_conocimiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63967d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoria_buscada = \"Tecnolog√≠a\"\n",
    "datos_tecnologia = obtener_datos_por_categoria(base_conocimiento, categoria_buscada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9889327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üí° Textos sobre Tecnolog√≠a ---\n",
      "**1.** La inteligencia artificial (IA) es un campo de la inform√°tica que se enfoca en la creaci√≥n de sistemas que pueden razonar, aprender y actuar de manera inteligente.\n",
      "**2.** Python es un lenguaje de programaci√≥n de alto nivel muy popular para el desarrollo web, el an√°lisis de datos y el aprendizaje autom√°tico.\n",
      "**3.** Blockchain es una tecnolog√≠a de registro distribuido que permite almacenar datos de manera segura y transparente.\n",
      "**4.** El Internet de las Cosas (IoT) conecta objetos cotidianos a internet, permitiendo la comunicaci√≥n y el control remoto.\n",
      "------------------------------\n",
      "--- üêæ Textos sobre Animales ---\n",
      "**1.** El guepardo es el animal terrestre m√°s r√°pido, capaz de alcanzar velocidades de hasta 120 km/h en r√°fagas cortas.\n",
      "**2.** Los pulpos son conocidos por su alta inteligencia, capacidad de camuflaje y tener tres corazones.\n",
      "**3.** La migraci√≥n anual del √±u en el Serengeti es uno de los mayores espect√°culos de la vida salvaje en la Tierra.\n",
      "**4.** Las abejas desempe√±an un papel vital en la polinizaci√≥n de muchas especies de plantas, incluyendo gran parte de los cultivos.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- üí° Textos sobre {categoria_buscada} ---\")\n",
    "if datos_tecnologia:\n",
    "    for i, texto in enumerate(datos_tecnologia, 1):\n",
    "        print(f\"**{i}.** {texto}\")\n",
    "else:\n",
    "    print(f\"No se encontraron datos para la categor√≠a '{categoria_buscada}'.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 3. Buscar datos en otra categor√≠a (ejemplo: Animales)\n",
    "categoria_animales = \"Animales\"\n",
    "datos_animales = obtener_datos_por_categoria(base_conocimiento, categoria_animales)\n",
    "\n",
    "print(f\"--- üêæ Textos sobre {categoria_animales} ---\")\n",
    "for i, texto in enumerate(datos_animales, 1):\n",
    "    print(f\"**{i}.** {texto}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcf18d",
   "metadata": {},
   "source": [
    "## 3. Crear la base vectorial (ChromaDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b161088",
   "metadata": {},
   "source": [
    "Creamos una colecci√≥n donde almacenaremos los embeddings de los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ae7aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5362ddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de datos ChromaDB configurada en el directorio: chroma_db_tech\n"
     ]
    }
   ],
   "source": [
    "# 1. Configurar el cliente ChromaDB\n",
    "# Usaremos un cliente persistente, lo que significa que guardar√° los datos\n",
    "# en un directorio local para que persistan entre sesiones.\n",
    "CHROMA_PATH = \"chroma_db_tech\"\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "print(f\"Base de datos ChromaDB configurada en el directorio: {CHROMA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a54afd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\X415\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\X415\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Configurar el Modelo de Embeddings\n",
    "# Usaremos un modelo de Hugging Face de la librer√≠a 'sentence-transformers'\n",
    "# que es excelente para tareas de embeddings de texto.\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "hf_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBEDDING_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4632a4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colecci√≥n 'base_conocimiento_tecnologia_astronomia_animales' creada (o cargada si ya exist√≠a).\n"
     ]
    }
   ],
   "source": [
    "# 3. Crear la Colecci√≥n\n",
    "# Una \"colecci√≥n\" en ChromaDB es donde se almacenan los textos (documentos)\n",
    "# y sus representaciones vectoriales (embeddings).\n",
    "COLLECTION_NAME = \"base_conocimiento_tecnologia_astronomia_animales\"\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=hf_ef,\n",
    "    metadata={\"hnsw:space\": \"cosine\"} # Define la m√©trica de distancia\n",
    ")\n",
    "\n",
    "print(f\"Colecci√≥n '{COLLECTION_NAME}' creada (o cargada si ya exist√≠a).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45c74b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preparar los datos de la mini base de conocimiento (del punto anterior)\n",
    "\n",
    "# Aplanamos los datos para que sean m√°s f√°ciles de ingresar a ChromaDB.\n",
    "# Tambi√©n creamos IDs √∫nicos y metadatos (la categor√≠a original).\n",
    "documentos = []\n",
    "metadatos = []\n",
    "ids = []\n",
    "id_counter = 1\n",
    "\n",
    "\n",
    "for categoria, textos in base_conocimiento.items():\n",
    "    for texto in textos:\n",
    "        documentos.append(texto)\n",
    "        metadatos.append({\"categoria\": categoria})\n",
    "        ids.append(f\"doc_{id_counter}\")\n",
    "        id_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a839774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando la inserci√≥n de documentos y c√°lculo de embeddings...\n",
      "¬°√âxito! Se insertaron 12 documentos en la base vectorial.\n",
      "Documentos totales en la colecci√≥n: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Agregar los documentos a la Colecci√≥n (C√°lculo de Embeddings)\n",
    "# Chroma calcula autom√°ticamente los embeddings de los 'documentos' usando 'hf_ef'.\n",
    "print(\"\\nIniciando la inserci√≥n de documentos y c√°lculo de embeddings...\")\n",
    "collection.add(\n",
    "    documents=documentos,\n",
    "    metadatas=metadatos,\n",
    "    ids=ids\n",
    ")\n",
    "print(f\"¬°√âxito! Se insertaron {len(documentos)} documentos en la base vectorial.\")\n",
    "\n",
    "print(f\"Documentos totales en la colecci√≥n: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186bcf2",
   "metadata": {},
   "source": [
    "## 4. Realizar una b√∫squeda sem√°ntica b√°sica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ed6a9",
   "metadata": {},
   "source": [
    "Consultamos algo relacionado con animales dom√©sticos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f89441c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üß† Realizando B√∫squeda Sem√°ntica ---\n",
      "Pregunta: **¬øQu√© informaci√≥n tienes sobre animales dom√©sticos o mascotas?**\n"
     ]
    }
   ],
   "source": [
    "# 1. Definir la consulta (el texto de entrada)\n",
    "pregunta_usuario = \"¬øQu√© informaci√≥n tienes sobre animales dom√©sticos o mascotas?\"\n",
    "\n",
    "print(f\"\\n--- üß† Realizando B√∫squeda Sem√°ntica ---\")\n",
    "print(f\"Pregunta: **{pregunta_usuario}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc9c0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Realizar la consulta en la colecci√≥n\n",
    "# ChromaDB convertir√° autom√°ticamente la pregunta_usuario en un vector\n",
    "# y buscar√° los vectores m√°s similares dentro de la colecci√≥n.\n",
    "resultados = collection.query(\n",
    "    query_texts=[pregunta_usuario], # La pregunta que queremos vectorizar y buscar\n",
    "    n_results=2,                    # Queremos los 2 resultados m√°s relevantes\n",
    "    # Puedes a√±adir where={} para filtrar por metadatos, por ejemplo:\n",
    "    # where={\"categoria\": \"Animales\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da4c605a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üéØ Resultados de la B√∫squeda ---\n",
      "\n",
      "**1. Documento Relevante** (Distancia: 0.4664)\n",
      "   -> **Categor√≠a:** Animales\n",
      "   -> **Texto:** El guepardo es el animal terrestre m√°s r√°pido, capaz de alcanzar velocidades de hasta 120 km/h en r√°fagas cortas.\n",
      "\n",
      "**2. Documento Relevante** (Distancia: 0.5237)\n",
      "   -> **Categor√≠a:** Animales\n",
      "   -> **Texto:** Los pulpos son conocidos por su alta inteligencia, capacidad de camuflaje y tener tres corazones.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3. Procesar y mostrar los resultados\n",
    "print(\"\\n--- üéØ Resultados de la B√∫squeda ---\")\n",
    "\n",
    "# Los resultados vienen en formato de lista de listas (aunque solo consultamos una vez)\n",
    "# Usaremos el primer elemento [0] de cada lista.\n",
    "documentos_relevantes = resultados['documents'][0]\n",
    "distancias = resultados['distances'][0]\n",
    "metadatos_relevantes = resultados['metadatas'][0]\n",
    "\n",
    "if documentos_relevantes:\n",
    "    for i, (doc, meta, dist) in enumerate(zip(documentos_relevantes, metadatos_relevantes, distancias)):\n",
    "        # La distancia es una medida de qu√© tan lejos est√° el embedding del documento\n",
    "        # del embedding de la pregunta. Una distancia menor indica mayor similitud.\n",
    "\n",
    "        print(f\"\\n**{i + 1}. Documento Relevante** (Distancia: {dist:.4f})\")\n",
    "        print(f\"   -> **Categor√≠a:** {meta['categoria']}\")\n",
    "        print(f\"   -> **Texto:** {doc}\")\n",
    "else:\n",
    "    print(\"No se encontraron documentos relevantes.\")\n",
    "\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80624b1b",
   "metadata": {},
   "source": [
    "## 5. Optimizaci√≥n 1: Chunking Estrat√©gico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbeb71a",
   "metadata": {},
   "source": [
    "Supongamos que tenemos textos largos. Vamos a dividirlos en chunks l√≥gicos con solapamiento (simulado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d77febe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "texto_largo_ejemplo = \"\"\"\n",
    "La inteligencia artificial (IA) es un campo de la inform√°tica que se enfoca en la creaci√≥n de sistemas que pueden razonar, aprender y actuar de manera inteligente. Los modelos de lenguaje grande (LLMs) como GPT-4 han revolucionado la interacci√≥n humana con la tecnolog√≠a. Estos modelos son entrenados en vastos conjuntos de datos para predecir la siguiente palabra, permitiendo generar texto coherente y relevante.\n",
    "\n",
    "Sin embargo, el universo es mucho m√°s que bits y bytes. La V√≠a L√°ctea es la galaxia espiral donde se encuentra nuestro Sistema Solar, conteniendo miles de millones de estrellas. En su centro reside un agujero negro supermasivo, Sagitario A*, cuya inmensa gravedad dicta el movimiento de todo lo que le rodea. Estudiar estos fen√≥menos requiere telescopios potentes y an√°lisis de datos avanzados.\n",
    "\n",
    "En la Tierra, la biolog√≠a nos ofrece maravillas como el pulpo, un animal conocido por su alta inteligencia. Los pulpos son maestros del camuflaje, capaces de cambiar el color y la textura de su piel en un instante para mimetizarse con el entorno. Adem√°s, poseen un sistema circulatorio √∫nico con tres corazones. Su capacidad de resolver problemas los hace fascinantes para la neurociencia.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d296243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configurar el Text Splitter\n",
    "# Usaremos un RecursiveCharacterTextSplitter que intenta dividir por diferentes\n",
    "# caracteres (saltos de l√≠nea, puntos, etc.) para mantener el contexto.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Tama√±o m√°ximo de cada chunk\n",
    "    chunk_size=300,\n",
    "    # Tama√±o del solapamiento entre chunks adyacentes\n",
    "    chunk_overlap=50,\n",
    "    # Separadores preferidos (los predeterminados son buenos)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80eed524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ‚úÇÔ∏è Resultado del Chunking Estrat√©gico ---\n",
      "Texto original dividido en 6 chunks.\n"
     ]
    }
   ],
   "source": [
    "# 3. Aplicar el Chunking\n",
    "# La funci√≥n .create_documents toma el texto y lo divide seg√∫n la configuraci√≥n.\n",
    "chunks_documentos = text_splitter.create_documents([texto_largo_ejemplo])\n",
    "\n",
    "print(f\"\\n--- ‚úÇÔ∏è Resultado del Chunking Estrat√©gico ---\")\n",
    "print(f\"Texto original dividido en {len(chunks_documentos)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a381e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunks generados (ejemplo):\n",
      "**Chunk 0:** (Solapamiento) La inteligencia artificial (IA) es un campo de la inform√°tica que se e...\n",
      "**Chunk 1:** (Solapamiento) . Estos modelos son entrenados en vastos conjuntos de datos para prede...\n",
      "**Chunk 2:** (Solapamiento) Sin embargo, el universo es mucho m√°s que bits y bytes. La V√≠a L√°ctea ...\n",
      "**Chunk 3:** (Solapamiento) . En su centro reside un agujero negro supermasivo, Sagitario A*, cuya...\n",
      "**Chunk 4:** (Solapamiento) En la Tierra, la biolog√≠a nos ofrece maravillas como el pulpo, un anim...\n",
      "**Chunk 5:** (Solapamiento) . Adem√°s, poseen un sistema circulatorio √∫nico con tres corazones. Su ...\n"
     ]
    }
   ],
   "source": [
    "# 4. Preparar la nueva estructura de datos para ChromaDB\n",
    "nuevos_documentos = []\n",
    "nuevos_metadatos = []\n",
    "nuevos_ids = []\n",
    "id_base = 100 # Usamos IDs diferentes para distinguirlos\n",
    "\n",
    "print(\"\\nChunks generados (ejemplo):\")\n",
    "for i, chunk in enumerate(chunks_documentos):\n",
    "    chunk_text = chunk.page_content\n",
    "    # Simulamos asignar una categor√≠a basada en el contenido inicial (no ideal, pero simple)\n",
    "    if \"IA\" in chunk_text or \"GPT-4\" in chunk_text:\n",
    "        categoria = \"Tecnolog√≠a_Chunked\"\n",
    "    elif \"V√≠a L√°ctea\" in chunk_text or \"agujero negro\" in chunk_text:\n",
    "        categoria = \"Astronom√≠a_Chunked\"\n",
    "    elif \"pulpo\" in chunk_text or \"biolog√≠a\" in chunk_text:\n",
    "        categoria = \"Animales_Chunked\"\n",
    "    else:\n",
    "        categoria = \"General_Chunked\"\n",
    "\n",
    "    nuevos_documentos.append(chunk_text)\n",
    "    nuevos_metadatos.append({\n",
    "        \"categoria\": categoria,\n",
    "        \"source\": \"Art√≠culo Largo\",\n",
    "        \"chunk_id\": i\n",
    "    })\n",
    "    nuevos_ids.append(f\"chunk_{id_base + i}\")\n",
    "    print(f\"**Chunk {i}:** (Solapamiento) {chunk_text[:70]}...\") # Mostramos solo el inicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afdf7752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando la inserci√≥n de CHUNKS y c√°lculo de embeddings...\n",
      "¬°√âxito! Se insertaron 6 CHUNKS en la nueva base vectorial.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 5. Sobreescribir o a√±adir a la Colecci√≥n (Re-indexaci√≥n)\n",
    "\n",
    "# NOTA IMPORTANTE: Para este ejemplo, vamos a *limpiar* la colecci√≥n\n",
    "# para que solo contenga los chunks, o usar una nueva.\n",
    "# Aqu√≠ simplemente a√±adiremos los nuevos chunks:\n",
    "# collection.add(...)\n",
    "\n",
    "# Para asegurarnos de no mezclar datos si el script se ejecuta muchas veces,\n",
    "# vamos a usar un cliente nuevo y una colecci√≥n temporal para este chunk:\n",
    "CHROMA_PATH_CHUNKS = \"chroma_db_chunked\"\n",
    "chroma_client_chunked = chromadb.PersistentClient(path=CHROMA_PATH_CHUNKS)\n",
    "COLLECTION_NAME_CHUNKS = \"base_conocimiento_chunked\"\n",
    "\n",
    "# Creamos o cargamos la nueva colecci√≥n con los embeddings redefinidos (hf_ef debe estar definido)\n",
    "collection_chunked = chroma_client_chunked.get_or_create_collection(\n",
    "    name=COLLECTION_NAME_CHUNKS,\n",
    "    embedding_function=hf_ef, # Asume que hf_ef est√° definido del paso 3\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "print(\"\\nIniciando la inserci√≥n de CHUNKS y c√°lculo de embeddings...\")\n",
    "collection_chunked.add(\n",
    "    documents=nuevos_documentos,\n",
    "    metadatas=nuevos_metadatos,\n",
    "    ids=nuevos_ids\n",
    ")\n",
    "\n",
    "print(f\"¬°√âxito! Se insertaron {collection_chunked.count()} CHUNKS en la nueva base vectorial.\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d174c",
   "metadata": {},
   "source": [
    "## 6. Optimizaci√≥n 2: Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208b3f7",
   "metadata": {},
   "source": [
    "Vamos a mejorar la b√∫squeda sem√°ntica seleccionando los resultados m√°s relevantes mediante una segunda evaluaci√≥n (re-ranking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6988b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# 1. Cargar el modelo Re-ranker (Cross-Encoder)\n",
    "# Los Cross-Encoders son modelos que toman un par de (consulta, documento)\n",
    "# y devuelven una puntuaci√≥n de similitud. Son m√°s lentos que los bi-encoders\n",
    "# pero mucho m√°s precisos para clasificar la relevancia.\n",
    "RERANKER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "cross_encoder = CrossEncoder(RERANKER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1aeccf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üîÑ Modelo Cross-Encoder cargado: cross-encoder/ms-marco-MiniLM-L-6-v2 ---\n",
      "Buscando los 5 chunks iniciales m√°s cercanos...\n",
      "\n",
      "--- ‚ú® Resultados Finales Re-rankeados (Top 2) ---\n",
      "\n",
      "**1. Documento Relevante** (Puntuaci√≥n: -4.3383)\n",
      "   -> **Categor√≠a:** Animales_Chunked\n",
      "   -> **Texto:** En la Tierra, la biolog√≠a nos ofrece maravillas como el pulpo, un animal conocido por su alta inteligencia. Los pulpos son maestros del camuflaje, capaces de cambiar el color y la textura de su piel en un instante para mimetizarse con el entorno\n",
      "\n",
      "**2. Documento Relevante** (Puntuaci√≥n: -8.9486)\n",
      "   -> **Categor√≠a:** Astronom√≠a_Chunked\n",
      "   -> **Texto:** . En su centro reside un agujero negro supermasivo, Sagitario A*, cuya inmensa gravedad dicta el movimiento de todo lo que le rodea. Estudiar estos fen√≥menos requiere telescopios potentes y an√°lisis de datos avanzados.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n--- üîÑ Modelo Cross-Encoder cargado: {RERANKER_MODEL} ---\")\n",
    "\n",
    "# 2. Definir la Consulta\n",
    "pregunta_usuario_rerank = \"¬øQu√© informaci√≥n tienes sobre el comportamiento de los animales acu√°ticos?\"\n",
    "\n",
    "# 3. Paso 1: Recuperaci√≥n Inicial (Usando la colecci√≥n con Chunks)\n",
    "# Asumimos que 'collection_chunked' est√° cargada del paso 5.\n",
    "# Recuperamos un n√∫mero mayor de resultados (ej. 5) para tener un buen pool para re-rankear.\n",
    "NUM_RESULTS_RETRIEVED = 5\n",
    "print(f\"Buscando los {NUM_RESULTS_RETRIEVED} chunks iniciales m√°s cercanos...\")\n",
    "\n",
    "resultados_iniciales = collection_chunked.query(\n",
    "    query_texts=[pregunta_usuario_rerank],\n",
    "    n_results=NUM_RESULTS_RETRIEVED,\n",
    ")\n",
    "\n",
    "documentos_recuperados = resultados_iniciales['documents'][0]\n",
    "metadatos_recuperados = resultados_iniciales['metadatas'][0]\n",
    "\n",
    "# 4. Paso 2: Preparar los pares (Consulta, Documento) para el Cross-Encoder\n",
    "# El Cross-Encoder necesita una lista de tuplas: [(pregunta, chunk_1), (pregunta, chunk_2), ...]\n",
    "pares_para_rerank = [\n",
    "    (pregunta_usuario_rerank, chunk) for chunk in documentos_recuperados\n",
    "]\n",
    "\n",
    "# 5. Paso 3: Obtener las Puntuaciones de Relevancia del Cross-Encoder\n",
    "# El m√©todo .predict() devuelve una puntuaci√≥n para cada par. Una puntuaci√≥n m√°s alta es mejor.\n",
    "puntuaciones = cross_encoder.predict(pares_para_rerank)\n",
    "\n",
    "# 6. Paso 4: Combinar Resultados y Reordenar\n",
    "# Creamos una lista de diccionarios que incluye el chunk, metadato y la nueva puntuaci√≥n.\n",
    "resultados_combinados = []\n",
    "for doc, meta, score in zip(documentos_recuperados, metadatos_recuperados, puntuaciones):\n",
    "    resultados_combinados.append({\n",
    "        'document': doc,\n",
    "        'metadata': meta,\n",
    "        'rerank_score': score\n",
    "    })\n",
    "\n",
    "# Ordenar la lista por la puntuaci√≥n del re-ranker (de mayor a menor)\n",
    "resultados_ordenados = sorted(\n",
    "    resultados_combinados,\n",
    "    key=lambda x: x['rerank_score'],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# 7. Mostrar los resultados Re-rankeados (solo los top 2)\n",
    "NUM_FINAL_RESULTS = 2\n",
    "print(f\"\\n--- ‚ú® Resultados Finales Re-rankeados (Top {NUM_FINAL_RESULTS}) ---\")\n",
    "\n",
    "for i, resultado in enumerate(resultados_ordenados[:NUM_FINAL_RESULTS]):\n",
    "    print(f\"\\n**{i + 1}. Documento Relevante** (Puntuaci√≥n: {resultado['rerank_score']:.4f})\")\n",
    "    print(f\"   -> **Categor√≠a:** {resultado['metadata']['categoria']}\")\n",
    "    print(f\"   -> **Texto:** {resultado['document']}\")\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e3753",
   "metadata": {},
   "source": [
    "## 7. Optimizaci√≥n 3: B√∫squeda h√≠brida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca7e24",
   "metadata": {},
   "source": [
    "Combinaremos b√∫squeda por palabra clave + sem√°ntica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab897571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulacion_sparse_retrieval(pregunta, documentos_chunked):\n",
    "    \"\"\"\n",
    "    Simula la b√∫squeda por palabra clave (BM25/Sparse) buscando cualquier\n",
    "    documento cuyo texto contenga al menos una palabra clave de la pregunta.\n",
    "    \"\"\"\n",
    "    palabras_clave = set(pregunta.lower().split())\n",
    "    resultados_sparse = []\n",
    "    \n",
    "    for i, doc_data in enumerate(documentos_chunked):\n",
    "        texto = doc_data['document'].lower()\n",
    " \n",
    "        if any(keyword in texto for keyword in palabras_clave if len(keyword) > 3):\n",
    "             resultados_sparse.append({\n",
    "                'id': doc_data['id'],\n",
    "                'document': doc_data['document'],\n",
    "                'metadata': doc_data['metadata']\n",
    "            })\n",
    "\n",
    "    return resultados_sparse[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea3aed5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 6, updating n_results = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üîé B√∫squeda Sem√°ntica ---\n",
      "--- üîë B√∫squeda por Palabra Clave ---\n",
      "Palabra clave recuper√≥ 1 chunks.\n",
      "\n",
      "--- üåê Resultados Finales H√≠bridos (RRF) ---\n",
      "\n",
      "**1. Documento H√≠brido** (RRF Score: 0.0328)\n",
      "   -> **Categor√≠a:** Astronom√≠a_Chunked\n",
      "   -> **Texto:** . En su centro reside un agujero negro supermasivo, Sagitario A*, cuya inmensa gravedad dicta el mov...\n",
      "\n",
      "**2. Documento H√≠brido** (RRF Score: 0.0161)\n",
      "   -> **Categor√≠a:** Astronom√≠a_Chunked\n",
      "   -> **Texto:** Sin embargo, el universo es mucho m√°s que bits y bytes. La V√≠a L√°ctea es la galaxia espiral donde se...\n",
      "\n",
      "**3. Documento H√≠brido** (RRF Score: 0.0159)\n",
      "   -> **Categor√≠a:** Animales_Chunked\n",
      "   -> **Texto:** En la Tierra, la biolog√≠a nos ofrece maravillas como el pulpo, un animal conocido por su alta inteli...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "all_chroma_data = collection_chunked.get(include=['metadatas', 'documents'])\n",
    "all_chunks_data = [\n",
    "    {'id': all_chroma_data['ids'][i], \n",
    "     'document': all_chroma_data['documents'][i], \n",
    "     'metadata': all_chroma_data['metadatas'][i]} \n",
    "    for i in range(len(all_chroma_data['ids']))\n",
    "]\n",
    "\n",
    "# 2. Definir la Consulta y realizar ambas b√∫squedas\n",
    "pregunta_hibrida = \"Dame informaci√≥n sobre el agujero negro en el centro de nuestra galaxia.\"\n",
    "\n",
    "# A. B√∫squeda Sem√°ntica (Dense)\n",
    "print(f\"\\n--- üîé B√∫squeda Sem√°ntica ---\")\n",
    "# Recuperamos 10 resultados para el pool\n",
    "resultados_dense = collection_chunked.query(\n",
    "    query_texts=[pregunta_hibrida],\n",
    "    n_results=10,\n",
    "    include=['documents', 'metadatas', 'distances']\n",
    ")\n",
    "\n",
    "# B. B√∫squeda por Palabra Clave (Sparse Simulation)\n",
    "print(f\"--- üîë B√∫squeda por Palabra Clave ---\")\n",
    "resultados_sparse = simulacion_sparse_retrieval(pregunta_hibrida, all_chunks_data)\n",
    "print(f\"Palabra clave recuper√≥ {len(resultados_sparse)} chunks.\")\n",
    "\n",
    "\n",
    "# 3. Aplicar Reciprocal Rank Fusion (RRF)\n",
    "# Funci√≥n RRF para combinar y reordenar las dos listas de resultados\n",
    "def rank_fusion(dense_results, sparse_results, k=60):\n",
    "    \"\"\"Implementa Reciprocal Rank Fusion (RRF) para combinar rankings.\"\"\"\n",
    "    fused_scores = {}\n",
    "    \n",
    "    # A. Procesar resultados Sem√°nticos (Dense)\n",
    "    # dense_results es el output de ChromaDB query\n",
    "    ids_dense = dense_results['ids'][0]\n",
    "    for rank, doc_id in enumerate(ids_dense, 1):\n",
    "        score = 1 / (rank + k)\n",
    "        fused_scores[doc_id] = fused_scores.get(doc_id, 0) + score\n",
    "\n",
    "    # B. Procesar resultados Palabra Clave (Sparse)\n",
    "    # sparse_results es una lista de diccionarios que simula la respuesta sparse\n",
    "    for rank, result in enumerate(sparse_results, 1):\n",
    "        doc_id = result['id']\n",
    "        score = 1 / (rank + k)\n",
    "        fused_scores[doc_id] = fused_scores.get(doc_id, 0) + score\n",
    "        \n",
    "    # C. Obtener el ranking final basado en la puntuaci√≥n RRF\n",
    "    ranked_ids = sorted(fused_scores, key=fused_scores.get, reverse=True)\n",
    "    \n",
    "    # Recuperar los detalles completos de los chunks (simulando una llamada a la DB)\n",
    "    final_results = []\n",
    "    \n",
    "    # Mapear IDs a documentos completos para la salida final\n",
    "    id_to_doc = {d['id']: d for d in all_chunks_data}\n",
    "    \n",
    "    for doc_id in ranked_ids:\n",
    "        if doc_id in id_to_doc:\n",
    "            final_results.append({\n",
    "                'document': id_to_doc[doc_id]['document'],\n",
    "                'metadata': id_to_doc[doc_id]['metadata'],\n",
    "                'rrf_score': fused_scores[doc_id]\n",
    "            })\n",
    "            \n",
    "    return final_results\n",
    "\n",
    "# 4. Ejecutar y mostrar la B√∫squeda H√≠brida\n",
    "resultados_hibridos = rank_fusion(resultados_dense, resultados_sparse)\n",
    "\n",
    "print(\"\\n--- üåê Resultados Finales H√≠bridos (RRF) ---\")\n",
    "# Mostramos los 3 resultados m√°s relevantes\n",
    "for i, resultado in enumerate(resultados_hibridos[:3]):\n",
    "    print(f\"\\n**{i + 1}. Documento H√≠brido** (RRF Score: {resultado['rrf_score']:.4f})\")\n",
    "    print(f\"   -> **Categor√≠a:** {resultado['metadata']['categoria']}\")\n",
    "    print(f\"   -> **Texto:** {resultado['document'][:100]}...\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217dc3fe",
   "metadata": {},
   "source": [
    "## Extensi√≥n \n",
    "\n",
    "Implementa un cach√© simple que guarde los √∫ltimos resultados de consulta.\n",
    "Tip: Usa un diccionario {consulta: resultados} y revisa si la consulta ya existe antes de buscar otra vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15bcf008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryCache:\n",
    "    \"\"\"\n",
    "    Un cach√© simple que almacena los resultados de b√∫squedas anteriores\n",
    "    para evitar procesamiento redundante.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size=5):\n",
    "        # El diccionario para almacenar los resultados: {consulta_str: resultados}\n",
    "        self.cache = {}\n",
    "        # L√≠mite de entradas para mantener el cach√© gestionable (LRU simple)\n",
    "        self.max_size = max_size\n",
    "        print(f\"‚úÖ Cach√© inicializado con tama√±o m√°ximo: {self.max_size}\")\n",
    "\n",
    "    def get_results(self, query):\n",
    "        \"\"\"\n",
    "        Retorna los resultados si la consulta existe en el cach√©.\n",
    "        Mueve la consulta al final (simulaci√≥n LRU) para mantenerla fresca.\n",
    "        \"\"\"\n",
    "        if query in self.cache:\n",
    "            # Simulaci√≥n simple de \"usado recientemente\": borra y reinserta\n",
    "            # para moverlo al \"final\" del orden de inserci√≥n (mantenerlo fresco).\n",
    "            results = self.cache.pop(query)\n",
    "            self.cache[query] = results\n",
    "            print(f\"‚û°Ô∏è CACH√â HIT: Resultados devueltos para la consulta: '{query[:30]}...'\")\n",
    "            return results\n",
    "        \n",
    "        print(f\"‚ùå CACH√â MISS: La consulta no est√° en el cach√©.\")\n",
    "        return None\n",
    "\n",
    "    def store_results(self, query, results):\n",
    "        \"\"\"\n",
    "        Almacena los resultados de una nueva consulta. Si el cach√© est√° lleno,\n",
    "        elimina la entrada m√°s antigua (el primer elemento insertado, simple LRU).\n",
    "        \"\"\"\n",
    "        # 1. Chequeo de L√≠mite (Simulaci√≥n de \"Least Recently Used - LRU\")\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            # Elimina el primer elemento (el m√°s antiguo/menos recientemente usado)\n",
    "            oldest_key = next(iter(self.cache))\n",
    "            self.cache.pop(oldest_key)\n",
    "            print(f\"üóëÔ∏è Cach√© lleno. Eliminada la entrada m√°s antigua: '{oldest_key[:30]}...'\")\n",
    "\n",
    "        # 2. Almacenar el nuevo resultado\n",
    "        self.cache[query] = results\n",
    "        print(f\"‚ûï Resultado almacenado en cach√© para: '{query[:30]}...'\")\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Vac√≠a completamente el cach√©.\"\"\"\n",
    "        self.cache = {}\n",
    "        print(\"Cach√© limpiado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd3bf0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cach√© inicializado con tama√±o m√°ximo: 3\n",
      "\n",
      "--- Intento 1 (Consulta A) ---\n",
      "‚ùå CACH√â MISS: La consulta no est√° en el cach√©.\n",
      "...Ejecutando b√∫squeda sem√°ntica, sparse y RRF (simulaci√≥n de 2s)...\n",
      "‚ûï Resultado almacenado en cach√© para: '¬øQu√© informaci√≥n tienes sobre ...'\n",
      "\n",
      "--- Intento 2 (Consulta A - Repetida) ---\n",
      "‚û°Ô∏è CACH√â HIT: Resultados devueltos para la consulta: '¬øQu√© informaci√≥n tienes sobre ...'\n",
      "\n",
      "--- Intento 3 (Consulta B) ---\n",
      "‚ùå CACH√â MISS: La consulta no est√° en el cach√©.\n",
      "...Ejecutando b√∫squeda sem√°ntica, sparse y RRF (simulaci√≥n de 2s)...\n",
      "‚ûï Resultado almacenado en cach√© para: '¬øCu√°l es el animal terrestre m...'\n",
      "\n",
      "--- Intento 4 (Consulta C - L√≠mite de Cach√©) ---\n",
      "‚ùå CACH√â MISS: La consulta no est√° en el cach√©.\n",
      "...Ejecutando b√∫squeda sem√°ntica, sparse y RRF (simulaci√≥n de 2s)...\n",
      "‚ûï Resultado almacenado en cach√© para: 'Menciona los beneficios del bl...'\n",
      "\n",
      "--- Intento 5 (Consulta A - Eliminada) ---\n",
      "‚û°Ô∏è CACH√â HIT: Resultados devueltos para la consulta: '¬øQu√© informaci√≥n tienes sobre ...'\n"
     ]
    }
   ],
   "source": [
    "query_cache = QueryCache(max_size=3)\n",
    "\n",
    "\n",
    "def buscar_hibrida_optimizada(query, cache_instance):\n",
    "    \"\"\"\n",
    "    Simula la funci√≥n de b√∫squeda h√≠brida (lenta) que utiliza el cach√©.\n",
    "    \"\"\"\n",
    "    # 1. Revisar Cach√©\n",
    "    cached_results = cache_instance.get_results(query)\n",
    "    if cached_results is not None:\n",
    "        return cached_results\n",
    "\n",
    "    # 2. Si hay CACH√â MISS (Simular la b√∫squeda lenta)\n",
    "    import time\n",
    "    print(\"...Ejecutando b√∫squeda sem√°ntica, sparse y RRF (simulaci√≥n de 2s)...\")\n",
    "    time.sleep(2)  # Simula el tiempo que tomar√≠a la b√∫squeda real y el re-ranking\n",
    "    \n",
    "    # Simular los resultados finales (lista de diccionarios)\n",
    "    simulated_results = [\n",
    "        {\"document\": f\"Chunk relevante para '{query}'...\", \"score\": 0.95},\n",
    "        {\"document\": f\"Chunk secundario para '{query}'...\", \"score\": 0.88},\n",
    "    ]\n",
    "\n",
    "    # 3. Almacenar resultados\n",
    "    cache_instance.store_results(query, simulated_results)\n",
    "    return simulated_results\n",
    "\n",
    "# --- Escenario 1: Nueva Consulta (CACH√â MISS) ---\n",
    "consulta_A = \"¬øQu√© informaci√≥n tienes sobre el agujero negro en el centro de nuestra galaxia?\"\n",
    "print(\"\\n--- Intento 1 (Consulta A) ---\")\n",
    "resultados_A = buscar_hibrida_optimizada(consulta_A, query_cache)\n",
    "# print(resultados_A)\n",
    "\n",
    "# --- Escenario 2: Repetici√≥n Inmediata (CACH√â HIT) ---\n",
    "print(\"\\n--- Intento 2 (Consulta A - Repetida) ---\")\n",
    "resultados_A_repetido = buscar_hibrida_optimizada(consulta_A, query_cache)\n",
    "# ¬°Tiempo de espera evitado!\n",
    "\n",
    "# --- Escenario 3: Nueva Consulta (CACH√â MISS) ---\n",
    "consulta_B = \"¬øCu√°l es el animal terrestre m√°s r√°pido del mundo?\"\n",
    "print(\"\\n--- Intento 3 (Consulta B) ---\")\n",
    "resultados_B = buscar_hibrida_optimizada(consulta_B, query_cache)\n",
    "\n",
    "# --- Escenario 4: Nueva Consulta que llena el cach√© y elimina el m√°s antiguo ---\n",
    "consulta_C = \"Menciona los beneficios del blockchain.\"\n",
    "print(\"\\n--- Intento 4 (Consulta C - L√≠mite de Cach√©) ---\")\n",
    "resultados_C = buscar_hibrida_optimizada(consulta_C, query_cache)\n",
    "# La Consulta A fue eliminada porque era la m√°s antigua y el cach√© solo permite 3 entradas.\n",
    "\n",
    "# --- Escenario 5: Volver a consultar A (Ahora ser√° CACH√â MISS) ---\n",
    "print(\"\\n--- Intento 5 (Consulta A - Eliminada) ---\")\n",
    "resultados_A_re_miss = buscar_hibrida_optimizada(consulta_A, query_cache)\n",
    "# ¬°Vuelve a ejecutar la b√∫squeda lenta!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
