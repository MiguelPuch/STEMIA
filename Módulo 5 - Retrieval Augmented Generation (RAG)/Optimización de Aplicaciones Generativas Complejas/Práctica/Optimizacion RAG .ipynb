{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad76a70",
   "metadata": {},
   "source": [
    "# PRÁCTICA: Mejorando la Relevancia en un Sistema RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e1378",
   "metadata": {},
   "source": [
    "## 1. Preparación del entorno\n",
    "\n",
    "Instalamos las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install chromadb sentence-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bedbe1",
   "metadata": {},
   "source": [
    "Importamos los módulos principales:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac1c8d",
   "metadata": {},
   "source": [
    "## 2. Crear una mini base de conocimiento\n",
    "\n",
    "Vamos a simular una base de datos con textos sobre tecnología, astronomía y animales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "907dbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_conocimiento = {\n",
    "    \"Tecnología\": [\n",
    "        \"La inteligencia artificial (IA) es un campo de la informática que se enfoca en la creación de sistemas que pueden razonar, aprender y actuar de manera inteligente.\",\n",
    "        \"Python es un lenguaje de programación de alto nivel muy popular para el desarrollo web, el análisis de datos y el aprendizaje automático.\",\n",
    "        \"Blockchain es una tecnología de registro distribuido que permite almacenar datos de manera segura y transparente.\",\n",
    "        \"El Internet de las Cosas (IoT) conecta objetos cotidianos a internet, permitiendo la comunicación y el control remoto.\"\n",
    "    ],\n",
    "    \"Astronomía\": [\n",
    "        \"La Vía Láctea es la galaxia espiral donde se encuentra nuestro Sistema Solar.\",\n",
    "        \"Un agujero negro es una región del espacio de la que nada, ni siquiera la luz, puede escapar debido a la inmensa gravedad.\",\n",
    "        \"Marte, el planeta rojo, es un objetivo clave para la exploración espacial debido a la posibilidad de albergar vida pasada o futura.\",\n",
    "        \"Una supernova es una explosión estelar extremadamente brillante y poderosa.\"\n",
    "    ],\n",
    "    \"Animales\": [\n",
    "        \"El guepardo es el animal terrestre más rápido, capaz de alcanzar velocidades de hasta 120 km/h en ráfagas cortas.\",\n",
    "        \"Los pulpos son conocidos por su alta inteligencia, capacidad de camuflaje y tener tres corazones.\",\n",
    "        \"La migración anual del ñu en el Serengeti es uno de los mayores espectáculos de la vida salvaje en la Tierra.\",\n",
    "        \"Las abejas desempeñan un papel vital en la polinización de muchas especies de plantas, incluyendo gran parte de los cultivos.\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e18c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_categorias(db):\n",
    "    \"\"\"Muestra todas las categorías disponibles en la base de conocimiento.\"\"\"\n",
    "    print(\"--- 📂 Categorías Disponibles ---\")\n",
    "    for categoria in db.keys():\n",
    "        print(f\"- **{categoria}**\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d270458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_datos_por_categoria(db, categoria):\n",
    "    \"\"\"\n",
    "    Retorna los textos asociados a una categoría específica.\n",
    "    Retorna una lista vacía si la categoría no existe.\n",
    "    \"\"\"\n",
    "    return db.get(categoria, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1438a1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 📂 Categorías Disponibles ---\n",
      "- **Tecnología**\n",
      "- **Astronomía**\n",
      "- **Animales**\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "mostrar_categorias(base_conocimiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63967d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoria_buscada = \"Tecnología\"\n",
    "datos_tecnologia = obtener_datos_por_categoria(base_conocimiento, categoria_buscada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9889327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 💡 Textos sobre Tecnología ---\n",
      "**1.** La inteligencia artificial (IA) es un campo de la informática que se enfoca en la creación de sistemas que pueden razonar, aprender y actuar de manera inteligente.\n",
      "**2.** Python es un lenguaje de programación de alto nivel muy popular para el desarrollo web, el análisis de datos y el aprendizaje automático.\n",
      "**3.** Blockchain es una tecnología de registro distribuido que permite almacenar datos de manera segura y transparente.\n",
      "**4.** El Internet de las Cosas (IoT) conecta objetos cotidianos a internet, permitiendo la comunicación y el control remoto.\n",
      "------------------------------\n",
      "--- 🐾 Textos sobre Animales ---\n",
      "**1.** El guepardo es el animal terrestre más rápido, capaz de alcanzar velocidades de hasta 120 km/h en ráfagas cortas.\n",
      "**2.** Los pulpos son conocidos por su alta inteligencia, capacidad de camuflaje y tener tres corazones.\n",
      "**3.** La migración anual del ñu en el Serengeti es uno de los mayores espectáculos de la vida salvaje en la Tierra.\n",
      "**4.** Las abejas desempeñan un papel vital en la polinización de muchas especies de plantas, incluyendo gran parte de los cultivos.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- 💡 Textos sobre {categoria_buscada} ---\")\n",
    "if datos_tecnologia:\n",
    "    for i, texto in enumerate(datos_tecnologia, 1):\n",
    "        print(f\"**{i}.** {texto}\")\n",
    "else:\n",
    "    print(f\"No se encontraron datos para la categoría '{categoria_buscada}'.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 3. Buscar datos en otra categoría (ejemplo: Animales)\n",
    "categoria_animales = \"Animales\"\n",
    "datos_animales = obtener_datos_por_categoria(base_conocimiento, categoria_animales)\n",
    "\n",
    "print(f\"--- 🐾 Textos sobre {categoria_animales} ---\")\n",
    "for i, texto in enumerate(datos_animales, 1):\n",
    "    print(f\"**{i}.** {texto}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcf18d",
   "metadata": {},
   "source": [
    "## 3. Crear la base vectorial (ChromaDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b161088",
   "metadata": {},
   "source": [
    "Creamos una colección donde almacenaremos los embeddings de los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ae7aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5362ddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de datos ChromaDB configurada en el directorio: chroma_db_tech\n"
     ]
    }
   ],
   "source": [
    "# 1. Configurar el cliente ChromaDB\n",
    "# Usaremos un cliente persistente, lo que significa que guardará los datos\n",
    "# en un directorio local para que persistan entre sesiones.\n",
    "CHROMA_PATH = \"chroma_db_tech\"\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "print(f\"Base de datos ChromaDB configurada en el directorio: {CHROMA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a54afd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\X415\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\X415\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Configurar el Modelo de Embeddings\n",
    "# Usaremos un modelo de Hugging Face de la librería 'sentence-transformers'\n",
    "# que es excelente para tareas de embeddings de texto.\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "hf_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBEDDING_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4632a4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colección 'base_conocimiento_tecnologia_astronomia_animales' creada (o cargada si ya existía).\n"
     ]
    }
   ],
   "source": [
    "# 3. Crear la Colección\n",
    "# Una \"colección\" en ChromaDB es donde se almacenan los textos (documentos)\n",
    "# y sus representaciones vectoriales (embeddings).\n",
    "COLLECTION_NAME = \"base_conocimiento_tecnologia_astronomia_animales\"\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=hf_ef,\n",
    "    metadata={\"hnsw:space\": \"cosine\"} # Define la métrica de distancia\n",
    ")\n",
    "\n",
    "print(f\"Colección '{COLLECTION_NAME}' creada (o cargada si ya existía).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45c74b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preparar los datos de la mini base de conocimiento (del punto anterior)\n",
    "\n",
    "# Aplanamos los datos para que sean más fáciles de ingresar a ChromaDB.\n",
    "# También creamos IDs únicos y metadatos (la categoría original).\n",
    "documentos = []\n",
    "metadatos = []\n",
    "ids = []\n",
    "id_counter = 1\n",
    "\n",
    "\n",
    "for categoria, textos in base_conocimiento.items():\n",
    "    for texto in textos:\n",
    "        documentos.append(texto)\n",
    "        metadatos.append({\"categoria\": categoria})\n",
    "        ids.append(f\"doc_{id_counter}\")\n",
    "        id_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a839774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando la inserción de documentos y cálculo de embeddings...\n",
      "¡Éxito! Se insertaron 12 documentos en la base vectorial.\n",
      "Documentos totales en la colección: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Agregar los documentos a la Colección (Cálculo de Embeddings)\n",
    "# Chroma calcula automáticamente los embeddings de los 'documentos' usando 'hf_ef'.\n",
    "print(\"\\nIniciando la inserción de documentos y cálculo de embeddings...\")\n",
    "collection.add(\n",
    "    documents=documentos,\n",
    "    metadatas=metadatos,\n",
    "    ids=ids\n",
    ")\n",
    "print(f\"¡Éxito! Se insertaron {len(documentos)} documentos en la base vectorial.\")\n",
    "\n",
    "print(f\"Documentos totales en la colección: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186bcf2",
   "metadata": {},
   "source": [
    "## 4. Realizar una búsqueda semántica básica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ed6a9",
   "metadata": {},
   "source": [
    "Consultamos algo relacionado con animales domésticos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f89441c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 🧠 Realizando Búsqueda Semántica ---\n",
      "Pregunta: **¿Qué información tienes sobre animales domésticos o mascotas?**\n"
     ]
    }
   ],
   "source": [
    "# 1. Definir la consulta (el texto de entrada)\n",
    "pregunta_usuario = \"¿Qué información tienes sobre animales domésticos o mascotas?\"\n",
    "\n",
    "print(f\"\\n--- 🧠 Realizando Búsqueda Semántica ---\")\n",
    "print(f\"Pregunta: **{pregunta_usuario}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc9c0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Realizar la consulta en la colección\n",
    "# ChromaDB convertirá automáticamente la pregunta_usuario en un vector\n",
    "# y buscará los vectores más similares dentro de la colección.\n",
    "resultados = collection.query(\n",
    "    query_texts=[pregunta_usuario], # La pregunta que queremos vectorizar y buscar\n",
    "    n_results=2,                    # Queremos los 2 resultados más relevantes\n",
    "    # Puedes añadir where={} para filtrar por metadatos, por ejemplo:\n",
    "    # where={\"categoria\": \"Animales\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da4c605a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 🎯 Resultados de la Búsqueda ---\n",
      "\n",
      "**1. Documento Relevante** (Distancia: 0.4664)\n",
      "   -> **Categoría:** Animales\n",
      "   -> **Texto:** El guepardo es el animal terrestre más rápido, capaz de alcanzar velocidades de hasta 120 km/h en ráfagas cortas.\n",
      "\n",
      "**2. Documento Relevante** (Distancia: 0.5237)\n",
      "   -> **Categoría:** Animales\n",
      "   -> **Texto:** Los pulpos son conocidos por su alta inteligencia, capacidad de camuflaje y tener tres corazones.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3. Procesar y mostrar los resultados\n",
    "print(\"\\n--- 🎯 Resultados de la Búsqueda ---\")\n",
    "\n",
    "# Los resultados vienen en formato de lista de listas (aunque solo consultamos una vez)\n",
    "# Usaremos el primer elemento [0] de cada lista.\n",
    "documentos_relevantes = resultados['documents'][0]\n",
    "distancias = resultados['distances'][0]\n",
    "metadatos_relevantes = resultados['metadatas'][0]\n",
    "\n",
    "if documentos_relevantes:\n",
    "    for i, (doc, meta, dist) in enumerate(zip(documentos_relevantes, metadatos_relevantes, distancias)):\n",
    "        # La distancia es una medida de qué tan lejos está el embedding del documento\n",
    "        # del embedding de la pregunta. Una distancia menor indica mayor similitud.\n",
    "\n",
    "        print(f\"\\n**{i + 1}. Documento Relevante** (Distancia: {dist:.4f})\")\n",
    "        print(f\"   -> **Categoría:** {meta['categoria']}\")\n",
    "        print(f\"   -> **Texto:** {doc}\")\n",
    "else:\n",
    "    print(\"No se encontraron documentos relevantes.\")\n",
    "\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80624b1b",
   "metadata": {},
   "source": [
    "## 5. Optimización 1: Chunking Estratégico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbeb71a",
   "metadata": {},
   "source": [
    "Supongamos que tenemos textos largos. Vamos a dividirlos en chunks lógicos con solapamiento (simulado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d77febe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "texto_largo_ejemplo = \"\"\"\n",
    "La inteligencia artificial (IA) es un campo de la informática que se enfoca en la creación de sistemas que pueden razonar, aprender y actuar de manera inteligente. Los modelos de lenguaje grande (LLMs) como GPT-4 han revolucionado la interacción humana con la tecnología. Estos modelos son entrenados en vastos conjuntos de datos para predecir la siguiente palabra, permitiendo generar texto coherente y relevante.\n",
    "\n",
    "Sin embargo, el universo es mucho más que bits y bytes. La Vía Láctea es la galaxia espiral donde se encuentra nuestro Sistema Solar, conteniendo miles de millones de estrellas. En su centro reside un agujero negro supermasivo, Sagitario A*, cuya inmensa gravedad dicta el movimiento de todo lo que le rodea. Estudiar estos fenómenos requiere telescopios potentes y análisis de datos avanzados.\n",
    "\n",
    "En la Tierra, la biología nos ofrece maravillas como el pulpo, un animal conocido por su alta inteligencia. Los pulpos son maestros del camuflaje, capaces de cambiar el color y la textura de su piel en un instante para mimetizarse con el entorno. Además, poseen un sistema circulatorio único con tres corazones. Su capacidad de resolver problemas los hace fascinantes para la neurociencia.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d296243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configurar el Text Splitter\n",
    "# Usaremos un RecursiveCharacterTextSplitter que intenta dividir por diferentes\n",
    "# caracteres (saltos de línea, puntos, etc.) para mantener el contexto.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Tamaño máximo de cada chunk\n",
    "    chunk_size=300,\n",
    "    # Tamaño del solapamiento entre chunks adyacentes\n",
    "    chunk_overlap=50,\n",
    "    # Separadores preferidos (los predeterminados son buenos)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80eed524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ✂️ Resultado del Chunking Estratégico ---\n",
      "Texto original dividido en 6 chunks.\n"
     ]
    }
   ],
   "source": [
    "# 3. Aplicar el Chunking\n",
    "# La función .create_documents toma el texto y lo divide según la configuración.\n",
    "chunks_documentos = text_splitter.create_documents([texto_largo_ejemplo])\n",
    "\n",
    "print(f\"\\n--- ✂️ Resultado del Chunking Estratégico ---\")\n",
    "print(f\"Texto original dividido en {len(chunks_documentos)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a381e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunks generados (ejemplo):\n",
      "**Chunk 0:** (Solapamiento) La inteligencia artificial (IA) es un campo de la informática que se e...\n",
      "**Chunk 1:** (Solapamiento) . Estos modelos son entrenados en vastos conjuntos de datos para prede...\n",
      "**Chunk 2:** (Solapamiento) Sin embargo, el universo es mucho más que bits y bytes. La Vía Láctea ...\n",
      "**Chunk 3:** (Solapamiento) . En su centro reside un agujero negro supermasivo, Sagitario A*, cuya...\n",
      "**Chunk 4:** (Solapamiento) En la Tierra, la biología nos ofrece maravillas como el pulpo, un anim...\n",
      "**Chunk 5:** (Solapamiento) . Además, poseen un sistema circulatorio único con tres corazones. Su ...\n"
     ]
    }
   ],
   "source": [
    "# 4. Preparar la nueva estructura de datos para ChromaDB\n",
    "nuevos_documentos = []\n",
    "nuevos_metadatos = []\n",
    "nuevos_ids = []\n",
    "id_base = 100 # Usamos IDs diferentes para distinguirlos\n",
    "\n",
    "print(\"\\nChunks generados (ejemplo):\")\n",
    "for i, chunk in enumerate(chunks_documentos):\n",
    "    chunk_text = chunk.page_content\n",
    "    # Simulamos asignar una categoría basada en el contenido inicial (no ideal, pero simple)\n",
    "    if \"IA\" in chunk_text or \"GPT-4\" in chunk_text:\n",
    "        categoria = \"Tecnología_Chunked\"\n",
    "    elif \"Vía Láctea\" in chunk_text or \"agujero negro\" in chunk_text:\n",
    "        categoria = \"Astronomía_Chunked\"\n",
    "    elif \"pulpo\" in chunk_text or \"biología\" in chunk_text:\n",
    "        categoria = \"Animales_Chunked\"\n",
    "    else:\n",
    "        categoria = \"General_Chunked\"\n",
    "\n",
    "    nuevos_documentos.append(chunk_text)\n",
    "    nuevos_metadatos.append({\n",
    "        \"categoria\": categoria,\n",
    "        \"source\": \"Artículo Largo\",\n",
    "        \"chunk_id\": i\n",
    "    })\n",
    "    nuevos_ids.append(f\"chunk_{id_base + i}\")\n",
    "    print(f\"**Chunk {i}:** (Solapamiento) {chunk_text[:70]}...\") # Mostramos solo el inicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afdf7752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando la inserción de CHUNKS y cálculo de embeddings...\n",
      "¡Éxito! Se insertaron 6 CHUNKS en la nueva base vectorial.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 5. Sobreescribir o añadir a la Colección (Re-indexación)\n",
    "\n",
    "# NOTA IMPORTANTE: Para este ejemplo, vamos a *limpiar* la colección\n",
    "# para que solo contenga los chunks, o usar una nueva.\n",
    "# Aquí simplemente añadiremos los nuevos chunks:\n",
    "# collection.add(...)\n",
    "\n",
    "# Para asegurarnos de no mezclar datos si el script se ejecuta muchas veces,\n",
    "# vamos a usar un cliente nuevo y una colección temporal para este chunk:\n",
    "CHROMA_PATH_CHUNKS = \"chroma_db_chunked\"\n",
    "chroma_client_chunked = chromadb.PersistentClient(path=CHROMA_PATH_CHUNKS)\n",
    "COLLECTION_NAME_CHUNKS = \"base_conocimiento_chunked\"\n",
    "\n",
    "# Creamos o cargamos la nueva colección con los embeddings redefinidos (hf_ef debe estar definido)\n",
    "collection_chunked = chroma_client_chunked.get_or_create_collection(\n",
    "    name=COLLECTION_NAME_CHUNKS,\n",
    "    embedding_function=hf_ef, # Asume que hf_ef está definido del paso 3\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "print(\"\\nIniciando la inserción de CHUNKS y cálculo de embeddings...\")\n",
    "collection_chunked.add(\n",
    "    documents=nuevos_documentos,\n",
    "    metadatas=nuevos_metadatos,\n",
    "    ids=nuevos_ids\n",
    ")\n",
    "\n",
    "print(f\"¡Éxito! Se insertaron {collection_chunked.count()} CHUNKS en la nueva base vectorial.\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d174c",
   "metadata": {},
   "source": [
    "## 6. Optimización 2: Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208b3f7",
   "metadata": {},
   "source": [
    "Vamos a mejorar la búsqueda semántica seleccionando los resultados más relevantes mediante una segunda evaluación (re-ranking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6988b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# 1. Cargar el modelo Re-ranker (Cross-Encoder)\n",
    "# Los Cross-Encoders son modelos que toman un par de (consulta, documento)\n",
    "# y devuelven una puntuación de similitud. Son más lentos que los bi-encoders\n",
    "# pero mucho más precisos para clasificar la relevancia.\n",
    "RERANKER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "cross_encoder = CrossEncoder(RERANKER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1aeccf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 🔄 Modelo Cross-Encoder cargado: cross-encoder/ms-marco-MiniLM-L-6-v2 ---\n",
      "Buscando los 5 chunks iniciales más cercanos...\n",
      "\n",
      "--- ✨ Resultados Finales Re-rankeados (Top 2) ---\n",
      "\n",
      "**1. Documento Relevante** (Puntuación: -4.3383)\n",
      "   -> **Categoría:** Animales_Chunked\n",
      "   -> **Texto:** En la Tierra, la biología nos ofrece maravillas como el pulpo, un animal conocido por su alta inteligencia. Los pulpos son maestros del camuflaje, capaces de cambiar el color y la textura de su piel en un instante para mimetizarse con el entorno\n",
      "\n",
      "**2. Documento Relevante** (Puntuación: -8.9486)\n",
      "   -> **Categoría:** Astronomía_Chunked\n",
      "   -> **Texto:** . En su centro reside un agujero negro supermasivo, Sagitario A*, cuya inmensa gravedad dicta el movimiento de todo lo que le rodea. Estudiar estos fenómenos requiere telescopios potentes y análisis de datos avanzados.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n--- 🔄 Modelo Cross-Encoder cargado: {RERANKER_MODEL} ---\")\n",
    "\n",
    "# 2. Definir la Consulta\n",
    "pregunta_usuario_rerank = \"¿Qué información tienes sobre el comportamiento de los animales acuáticos?\"\n",
    "\n",
    "# 3. Paso 1: Recuperación Inicial (Usando la colección con Chunks)\n",
    "# Asumimos que 'collection_chunked' está cargada del paso 5.\n",
    "# Recuperamos un número mayor de resultados (ej. 5) para tener un buen pool para re-rankear.\n",
    "NUM_RESULTS_RETRIEVED = 5\n",
    "print(f\"Buscando los {NUM_RESULTS_RETRIEVED} chunks iniciales más cercanos...\")\n",
    "\n",
    "resultados_iniciales = collection_chunked.query(\n",
    "    query_texts=[pregunta_usuario_rerank],\n",
    "    n_results=NUM_RESULTS_RETRIEVED,\n",
    ")\n",
    "\n",
    "documentos_recuperados = resultados_iniciales['documents'][0]\n",
    "metadatos_recuperados = resultados_iniciales['metadatas'][0]\n",
    "\n",
    "# 4. Paso 2: Preparar los pares (Consulta, Documento) para el Cross-Encoder\n",
    "# El Cross-Encoder necesita una lista de tuplas: [(pregunta, chunk_1), (pregunta, chunk_2), ...]\n",
    "pares_para_rerank = [\n",
    "    (pregunta_usuario_rerank, chunk) for chunk in documentos_recuperados\n",
    "]\n",
    "\n",
    "# 5. Paso 3: Obtener las Puntuaciones de Relevancia del Cross-Encoder\n",
    "# El método .predict() devuelve una puntuación para cada par. Una puntuación más alta es mejor.\n",
    "puntuaciones = cross_encoder.predict(pares_para_rerank)\n",
    "\n",
    "# 6. Paso 4: Combinar Resultados y Reordenar\n",
    "# Creamos una lista de diccionarios que incluye el chunk, metadato y la nueva puntuación.\n",
    "resultados_combinados = []\n",
    "for doc, meta, score in zip(documentos_recuperados, metadatos_recuperados, puntuaciones):\n",
    "    resultados_combinados.append({\n",
    "        'document': doc,\n",
    "        'metadata': meta,\n",
    "        'rerank_score': score\n",
    "    })\n",
    "\n",
    "# Ordenar la lista por la puntuación del re-ranker (de mayor a menor)\n",
    "resultados_ordenados = sorted(\n",
    "    resultados_combinados,\n",
    "    key=lambda x: x['rerank_score'],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# 7. Mostrar los resultados Re-rankeados (solo los top 2)\n",
    "NUM_FINAL_RESULTS = 2\n",
    "print(f\"\\n--- ✨ Resultados Finales Re-rankeados (Top {NUM_FINAL_RESULTS}) ---\")\n",
    "\n",
    "for i, resultado in enumerate(resultados_ordenados[:NUM_FINAL_RESULTS]):\n",
    "    print(f\"\\n**{i + 1}. Documento Relevante** (Puntuación: {resultado['rerank_score']:.4f})\")\n",
    "    print(f\"   -> **Categoría:** {resultado['metadata']['categoria']}\")\n",
    "    print(f\"   -> **Texto:** {resultado['document']}\")\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e3753",
   "metadata": {},
   "source": [
    "## 7. Optimización 3: Búsqueda híbrida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca7e24",
   "metadata": {},
   "source": [
    "Combinaremos búsqueda por palabra clave + semántica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab897571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulacion_sparse_retrieval(pregunta, documentos_chunked):\n",
    "    \"\"\"\n",
    "    Simula la búsqueda por palabra clave (BM25/Sparse) buscando cualquier\n",
    "    documento cuyo texto contenga al menos una palabra clave de la pregunta.\n",
    "    \"\"\"\n",
    "    palabras_clave = set(pregunta.lower().split())\n",
    "    resultados_sparse = []\n",
    "    \n",
    "    for i, doc_data in enumerate(documentos_chunked):\n",
    "        texto = doc_data['document'].lower()\n",
    " \n",
    "        if any(keyword in texto for keyword in palabras_clave if len(keyword) > 3):\n",
    "             resultados_sparse.append({\n",
    "                'id': doc_data['id'],\n",
    "                'document': doc_data['document'],\n",
    "                'metadata': doc_data['metadata']\n",
    "            })\n",
    "\n",
    "    return resultados_sparse[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea3aed5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 6, updating n_results = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 🔎 Búsqueda Semántica ---\n",
      "--- 🔑 Búsqueda por Palabra Clave ---\n",
      "Palabra clave recuperó 1 chunks.\n",
      "\n",
      "--- 🌐 Resultados Finales Híbridos (RRF) ---\n",
      "\n",
      "**1. Documento Híbrido** (RRF Score: 0.0328)\n",
      "   -> **Categoría:** Astronomía_Chunked\n",
      "   -> **Texto:** . En su centro reside un agujero negro supermasivo, Sagitario A*, cuya inmensa gravedad dicta el mov...\n",
      "\n",
      "**2. Documento Híbrido** (RRF Score: 0.0161)\n",
      "   -> **Categoría:** Astronomía_Chunked\n",
      "   -> **Texto:** Sin embargo, el universo es mucho más que bits y bytes. La Vía Láctea es la galaxia espiral donde se...\n",
      "\n",
      "**3. Documento Híbrido** (RRF Score: 0.0159)\n",
      "   -> **Categoría:** Animales_Chunked\n",
      "   -> **Texto:** En la Tierra, la biología nos ofrece maravillas como el pulpo, un animal conocido por su alta inteli...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "all_chroma_data = collection_chunked.get(include=['metadatas', 'documents'])\n",
    "all_chunks_data = [\n",
    "    {'id': all_chroma_data['ids'][i], \n",
    "     'document': all_chroma_data['documents'][i], \n",
    "     'metadata': all_chroma_data['metadatas'][i]} \n",
    "    for i in range(len(all_chroma_data['ids']))\n",
    "]\n",
    "\n",
    "# 2. Definir la Consulta y realizar ambas búsquedas\n",
    "pregunta_hibrida = \"Dame información sobre el agujero negro en el centro de nuestra galaxia.\"\n",
    "\n",
    "# A. Búsqueda Semántica (Dense)\n",
    "print(f\"\\n--- 🔎 Búsqueda Semántica ---\")\n",
    "# Recuperamos 10 resultados para el pool\n",
    "resultados_dense = collection_chunked.query(\n",
    "    query_texts=[pregunta_hibrida],\n",
    "    n_results=10,\n",
    "    include=['documents', 'metadatas', 'distances']\n",
    ")\n",
    "\n",
    "# B. Búsqueda por Palabra Clave (Sparse Simulation)\n",
    "print(f\"--- 🔑 Búsqueda por Palabra Clave ---\")\n",
    "resultados_sparse = simulacion_sparse_retrieval(pregunta_hibrida, all_chunks_data)\n",
    "print(f\"Palabra clave recuperó {len(resultados_sparse)} chunks.\")\n",
    "\n",
    "\n",
    "# 3. Aplicar Reciprocal Rank Fusion (RRF)\n",
    "# Función RRF para combinar y reordenar las dos listas de resultados\n",
    "def rank_fusion(dense_results, sparse_results, k=60):\n",
    "    \"\"\"Implementa Reciprocal Rank Fusion (RRF) para combinar rankings.\"\"\"\n",
    "    fused_scores = {}\n",
    "    \n",
    "    # A. Procesar resultados Semánticos (Dense)\n",
    "    # dense_results es el output de ChromaDB query\n",
    "    ids_dense = dense_results['ids'][0]\n",
    "    for rank, doc_id in enumerate(ids_dense, 1):\n",
    "        score = 1 / (rank + k)\n",
    "        fused_scores[doc_id] = fused_scores.get(doc_id, 0) + score\n",
    "\n",
    "    # B. Procesar resultados Palabra Clave (Sparse)\n",
    "    # sparse_results es una lista de diccionarios que simula la respuesta sparse\n",
    "    for rank, result in enumerate(sparse_results, 1):\n",
    "        doc_id = result['id']\n",
    "        score = 1 / (rank + k)\n",
    "        fused_scores[doc_id] = fused_scores.get(doc_id, 0) + score\n",
    "        \n",
    "    # C. Obtener el ranking final basado en la puntuación RRF\n",
    "    ranked_ids = sorted(fused_scores, key=fused_scores.get, reverse=True)\n",
    "    \n",
    "    # Recuperar los detalles completos de los chunks (simulando una llamada a la DB)\n",
    "    final_results = []\n",
    "    \n",
    "    # Mapear IDs a documentos completos para la salida final\n",
    "    id_to_doc = {d['id']: d for d in all_chunks_data}\n",
    "    \n",
    "    for doc_id in ranked_ids:\n",
    "        if doc_id in id_to_doc:\n",
    "            final_results.append({\n",
    "                'document': id_to_doc[doc_id]['document'],\n",
    "                'metadata': id_to_doc[doc_id]['metadata'],\n",
    "                'rrf_score': fused_scores[doc_id]\n",
    "            })\n",
    "            \n",
    "    return final_results\n",
    "\n",
    "# 4. Ejecutar y mostrar la Búsqueda Híbrida\n",
    "resultados_hibridos = rank_fusion(resultados_dense, resultados_sparse)\n",
    "\n",
    "print(\"\\n--- 🌐 Resultados Finales Híbridos (RRF) ---\")\n",
    "# Mostramos los 3 resultados más relevantes\n",
    "for i, resultado in enumerate(resultados_hibridos[:3]):\n",
    "    print(f\"\\n**{i + 1}. Documento Híbrido** (RRF Score: {resultado['rrf_score']:.4f})\")\n",
    "    print(f\"   -> **Categoría:** {resultado['metadata']['categoria']}\")\n",
    "    print(f\"   -> **Texto:** {resultado['document'][:100]}...\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217dc3fe",
   "metadata": {},
   "source": [
    "## Extensión \n",
    "\n",
    "Implementa un caché simple que guarde los últimos resultados de consulta.\n",
    "Tip: Usa un diccionario {consulta: resultados} y revisa si la consulta ya existe antes de buscar otra vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15bcf008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryCache:\n",
    "    \"\"\"\n",
    "    Un caché simple que almacena los resultados de búsquedas anteriores\n",
    "    para evitar procesamiento redundante.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size=5):\n",
    "        # El diccionario para almacenar los resultados: {consulta_str: resultados}\n",
    "        self.cache = {}\n",
    "        # Límite de entradas para mantener el caché gestionable (LRU simple)\n",
    "        self.max_size = max_size\n",
    "        print(f\"✅ Caché inicializado con tamaño máximo: {self.max_size}\")\n",
    "\n",
    "    def get_results(self, query):\n",
    "        \"\"\"\n",
    "        Retorna los resultados si la consulta existe en el caché.\n",
    "        Mueve la consulta al final (simulación LRU) para mantenerla fresca.\n",
    "        \"\"\"\n",
    "        if query in self.cache:\n",
    "            # Simulación simple de \"usado recientemente\": borra y reinserta\n",
    "            # para moverlo al \"final\" del orden de inserción (mantenerlo fresco).\n",
    "            results = self.cache.pop(query)\n",
    "            self.cache[query] = results\n",
    "            print(f\"➡️ CACHÉ HIT: Resultados devueltos para la consulta: '{query[:30]}...'\")\n",
    "            return results\n",
    "        \n",
    "        print(f\"❌ CACHÉ MISS: La consulta no está en el caché.\")\n",
    "        return None\n",
    "\n",
    "    def store_results(self, query, results):\n",
    "        \"\"\"\n",
    "        Almacena los resultados de una nueva consulta. Si el caché está lleno,\n",
    "        elimina la entrada más antigua (el primer elemento insertado, simple LRU).\n",
    "        \"\"\"\n",
    "        # 1. Chequeo de Límite (Simulación de \"Least Recently Used - LRU\")\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            # Elimina el primer elemento (el más antiguo/menos recientemente usado)\n",
    "            oldest_key = next(iter(self.cache))\n",
    "            self.cache.pop(oldest_key)\n",
    "            print(f\"🗑️ Caché lleno. Eliminada la entrada más antigua: '{oldest_key[:30]}...'\")\n",
    "\n",
    "        # 2. Almacenar el nuevo resultado\n",
    "        self.cache[query] = results\n",
    "        print(f\"➕ Resultado almacenado en caché para: '{query[:30]}...'\")\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Vacía completamente el caché.\"\"\"\n",
    "        self.cache = {}\n",
    "        print(\"Caché limpiado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd3bf0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Caché inicializado con tamaño máximo: 3\n",
      "\n",
      "--- Intento 1 (Consulta A) ---\n",
      "❌ CACHÉ MISS: La consulta no está en el caché.\n",
      "...Ejecutando búsqueda semántica, sparse y RRF (simulación de 2s)...\n",
      "➕ Resultado almacenado en caché para: '¿Qué información tienes sobre ...'\n",
      "\n",
      "--- Intento 2 (Consulta A - Repetida) ---\n",
      "➡️ CACHÉ HIT: Resultados devueltos para la consulta: '¿Qué información tienes sobre ...'\n",
      "\n",
      "--- Intento 3 (Consulta B) ---\n",
      "❌ CACHÉ MISS: La consulta no está en el caché.\n",
      "...Ejecutando búsqueda semántica, sparse y RRF (simulación de 2s)...\n",
      "➕ Resultado almacenado en caché para: '¿Cuál es el animal terrestre m...'\n",
      "\n",
      "--- Intento 4 (Consulta C - Límite de Caché) ---\n",
      "❌ CACHÉ MISS: La consulta no está en el caché.\n",
      "...Ejecutando búsqueda semántica, sparse y RRF (simulación de 2s)...\n",
      "➕ Resultado almacenado en caché para: 'Menciona los beneficios del bl...'\n",
      "\n",
      "--- Intento 5 (Consulta A - Eliminada) ---\n",
      "➡️ CACHÉ HIT: Resultados devueltos para la consulta: '¿Qué información tienes sobre ...'\n"
     ]
    }
   ],
   "source": [
    "query_cache = QueryCache(max_size=3)\n",
    "\n",
    "\n",
    "def buscar_hibrida_optimizada(query, cache_instance):\n",
    "    \"\"\"\n",
    "    Simula la función de búsqueda híbrida (lenta) que utiliza el caché.\n",
    "    \"\"\"\n",
    "    # 1. Revisar Caché\n",
    "    cached_results = cache_instance.get_results(query)\n",
    "    if cached_results is not None:\n",
    "        return cached_results\n",
    "\n",
    "    # 2. Si hay CACHÉ MISS (Simular la búsqueda lenta)\n",
    "    import time\n",
    "    print(\"...Ejecutando búsqueda semántica, sparse y RRF (simulación de 2s)...\")\n",
    "    time.sleep(2)  # Simula el tiempo que tomaría la búsqueda real y el re-ranking\n",
    "    \n",
    "    # Simular los resultados finales (lista de diccionarios)\n",
    "    simulated_results = [\n",
    "        {\"document\": f\"Chunk relevante para '{query}'...\", \"score\": 0.95},\n",
    "        {\"document\": f\"Chunk secundario para '{query}'...\", \"score\": 0.88},\n",
    "    ]\n",
    "\n",
    "    # 3. Almacenar resultados\n",
    "    cache_instance.store_results(query, simulated_results)\n",
    "    return simulated_results\n",
    "\n",
    "# --- Escenario 1: Nueva Consulta (CACHÉ MISS) ---\n",
    "consulta_A = \"¿Qué información tienes sobre el agujero negro en el centro de nuestra galaxia?\"\n",
    "print(\"\\n--- Intento 1 (Consulta A) ---\")\n",
    "resultados_A = buscar_hibrida_optimizada(consulta_A, query_cache)\n",
    "# print(resultados_A)\n",
    "\n",
    "# --- Escenario 2: Repetición Inmediata (CACHÉ HIT) ---\n",
    "print(\"\\n--- Intento 2 (Consulta A - Repetida) ---\")\n",
    "resultados_A_repetido = buscar_hibrida_optimizada(consulta_A, query_cache)\n",
    "# ¡Tiempo de espera evitado!\n",
    "\n",
    "# --- Escenario 3: Nueva Consulta (CACHÉ MISS) ---\n",
    "consulta_B = \"¿Cuál es el animal terrestre más rápido del mundo?\"\n",
    "print(\"\\n--- Intento 3 (Consulta B) ---\")\n",
    "resultados_B = buscar_hibrida_optimizada(consulta_B, query_cache)\n",
    "\n",
    "# --- Escenario 4: Nueva Consulta que llena el caché y elimina el más antiguo ---\n",
    "consulta_C = \"Menciona los beneficios del blockchain.\"\n",
    "print(\"\\n--- Intento 4 (Consulta C - Límite de Caché) ---\")\n",
    "resultados_C = buscar_hibrida_optimizada(consulta_C, query_cache)\n",
    "# La Consulta A fue eliminada porque era la más antigua y el caché solo permite 3 entradas.\n",
    "\n",
    "# --- Escenario 5: Volver a consultar A (Ahora será CACHÉ MISS) ---\n",
    "print(\"\\n--- Intento 5 (Consulta A - Eliminada) ---\")\n",
    "resultados_A_re_miss = buscar_hibrida_optimizada(consulta_A, query_cache)\n",
    "# ¡Vuelve a ejecutar la búsqueda lenta!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
