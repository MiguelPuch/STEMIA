{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bb2157",
   "metadata": {},
   "source": [
    "# Sistema RAG para auditoría de respuestas y fidelidad de información"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d58f0",
   "metadata": {},
   "source": [
    "Una empresa tiene un asistente RAG que responde consultas sobre productos y procedimientos técnicos.\n",
    "El equipo quiere evaluar y auditar la calidad de las respuestas generadas, no solo responder preguntas.\n",
    "El objetivo es que los alumnos construyan un pipeline que permita medir y optimizar fidelidad, relevancia y eficiencia del sistema RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df4bd1",
   "metadata": {},
   "source": [
    "## Objetivos de la práctica\n",
    "\n",
    "1. Cargar Dataset de consultas_auditorias.csv.\n",
    "\n",
    "2. Implementar un retriever para buscar documentos relevantes según la consulta.\n",
    "\n",
    "3. Aplicar prompt engineering: diseñar prompts que obliguen a citar la información del contexto y evitar alucinaciones.\n",
    "\n",
    "4. Implementar compresión de contexto, enviando solo la información necesaria al LLM.\n",
    "\n",
    "5. Crear métricas automáticas de evaluación:\n",
    "\n",
    "   *  Fidelidad (Faithfulness)\n",
    "\n",
    "   *  Relevancia de contexto (Context Relevance)\n",
    "\n",
    "   *  Relevancia de la respuesta (Answer Relevance)\n",
    "\n",
    "6. Analizar los resultados y sugerir mejoras en el retriever y generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ef6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297748e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be744f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ea67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1c7ab",
   "metadata": {},
   "source": [
    "## Desafíos de razonamiento\n",
    "\n",
    "1. Diseñar prompts que obliguen al LLM a usar únicamente el contenido recuperado.\n",
    "\n",
    "2. Implementar compresión de contexto, seleccionando solo fragmentos relevantes para cada consulta.\n",
    "\n",
    "3. Crear métricas automáticas que comparen la respuesta generada con la respuesta esperada.\n",
    "\n",
    "4. Analizar qué consultas generan respuestas poco fieles y proponer mejoras.\n",
    "\n",
    "5. Experimentar con distintos tamaños de contexto, número de documentos recuperados y caching para optimizar rendimiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
