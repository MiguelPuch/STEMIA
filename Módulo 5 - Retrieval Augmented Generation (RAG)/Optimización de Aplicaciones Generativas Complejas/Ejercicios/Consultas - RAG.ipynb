{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca32f0f",
   "metadata": {},
   "source": [
    "# Optimización de un sistema RAG para consultas frecuentes y contexto largo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342795ff",
   "metadata": {},
   "source": [
    "Una empresa tiene un sistema RAG interno que responde preguntas sobre documentación técnica de productos.\n",
    "Actualmente, el sistema es funcional, pero enfrenta problemas:\n",
    "\n",
    "* Algunas consultas frecuentes consumen demasiados recursos.\n",
    "\n",
    "* Las respuestas a veces son largas y contienen información irrelevante.\n",
    "\n",
    "* El costo de la API LLM es alto debido a inputs grandes.\n",
    "\n",
    "El objetivo de la práctica es optimizar el flujo del generator y del retriever, aplicando técnicas de Prompt Engineering, compresión de contexto, caching y medición de métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf326bd2",
   "metadata": {},
   "source": [
    "## Datos a utilizar\n",
    "\n",
    "Para realizar la práctica, se usarán dos CSVs de ejemplo:\n",
    "\n",
    "1. documentacion_auditoria.csv\n",
    "\n",
    "Contiene documentos relacionados con auditorías, normativa y riesgos financieros.\n",
    "\n",
    "Usar este dataset para consultas sobre auditoría, normativa, riesgos o control interno.\n",
    "\n",
    "\n",
    "2. documentacion_tecnica.csv\n",
    "\n",
    "Contiene documentos relacionados con sistemas RAG, arquitectura de software y optimización de procesos.\n",
    "\n",
    "Usar este dataset para consultas sobre implementación técnica, arquitectura, procesamiento de datos o RAG multimodal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee3ecbd",
   "metadata": {},
   "source": [
    "## Objetivos de la práctica\n",
    "\n",
    "1. Implementar Prompt Engineering avanzado:\n",
    "\n",
    "- Diseñar prompts que obliguen al LLM a citar el contenido recuperado.\n",
    "\n",
    "- Establecer roles, por ejemplo: “Eres un experto técnico que redacta informes precisos”.\n",
    "\n",
    "2. Aplicar compresión de contexto:\n",
    "\n",
    "- Reducir la cantidad de tokens enviados al LLM extrayendo solo la información relevante de los documentos recuperados.\n",
    "\n",
    "- Comparar eficiencia y costo antes y después de la compresión.\n",
    "\n",
    "3. Implementar caching de respuestas:\n",
    "\n",
    "- Guardar respuestas a consultas frecuentes y reutilizarlas sin invocar al LLM nuevamente.\n",
    "\n",
    "- Medir reducción de latencia y uso de recursos.\n",
    "\n",
    "4. Medir métricas de calidad:\n",
    "\n",
    "- Fidelidad (Faithfulness): verificar que la respuesta esté basada en los documentos recuperados.\n",
    "\n",
    "- Contexto relevante: asegurar que solo se use información necesaria.\n",
    "\n",
    "- Respuesta relevante: verificar que la consulta esté contestada correctamente.\n",
    "\n",
    "5. Experimentación iterativa (MLOps):\n",
    "\n",
    "- Probar distintos modelos de embeddings y LLM para evaluar trade-offs entre calidad y costo.\n",
    "\n",
    "- Ajustar número de documentos recuperados, re-ranking y tamaño de chunks.\n",
    "\n",
    "- Comparar dos versiones del sistema (A/B Testing) y reportar mejoras.\n",
    "\n",
    "6. Opcional avanzado – RAG Multimodal:\n",
    "\n",
    "- Indexar documentos con imágenes/diagramas y permitir consultas que incluyan referencias visuales.\n",
    "\n",
    "- Generar respuestas que incluyan la descripción del diagrama relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc4965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51b05df0",
   "metadata": {},
   "source": [
    "## Preguntas \n",
    "\n",
    "- Diseñar prompts claros que obliguen a citar fuentes y reduzcan alucinaciones.\n",
    "\n",
    "- Determinar qué partes del contexto son esenciales y aplicar compresión eficaz.\n",
    "\n",
    "- Implementar cache y decidir cuándo reutilizar respuestas.\n",
    "\n",
    "- Evaluar métricas de calidad para mejorar iterativamente el sistema.\n",
    "\n",
    "- Decidir parámetros de recuperación, chunking y re-ranking para optimizar latencia y costo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
